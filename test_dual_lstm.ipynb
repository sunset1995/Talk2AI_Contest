{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/nctu/Talk2AI_Contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.uf5db4499c0b7f893953f2b98fec37422.cache\n",
      "Loading model cost 1.488 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "from mini_batch_helper import extractor, MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference: https://github.com/dennybritz/chatbot-retrieval/blob/8b1be4c2e63631b1180b97ef927dc2c1f7fe9bea/udc_hparams.py\n",
    "exp_name = 'dual_lstm_8'\n",
    "with open('models/%s/model_parameters.json' %exp_name) as json_data:\n",
    "    params = json.load(json_data)\n",
    "    json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in  training data\n",
    "word2vec_fname = params['word2vec_model_name']\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = None\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, [], 0, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record = {}\n",
    "save_record_dir = 'models/%s/' %exp_name\n",
    "record['newest_model_dir'] = 'models/' + exp_name +'/newest/'\n",
    "record['best_model_dir'] = 'models/' + exp_name +'/best/'\n",
    "record['loss_train'] = []\n",
    "record['loss_valid'] = []\n",
    "record['accuracy_valid'] = []\n",
    "record['best_iter'] = 0\n",
    "record['sample_correct'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f53680fdcc0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f53675eeba8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input\n",
    "context = tf.placeholder(dtype=tf.int32, shape=(None, None), name='context')\n",
    "context_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='context_len')\n",
    "response = tf.placeholder(dtype=tf.int32, shape=(None, None), name='response')\n",
    "response_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='response_len')\n",
    "target = tf.placeholder(dtype=tf.int32, shape=(None, ), name='target')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "# Embedding\n",
    "init_embedding_W = tf.constant_initializer(embedding_matrix)\n",
    "embeddings_W = tf.get_variable('embeddings_W', shape=[embedding_matrix.shape[0], embedding_matrix.shape[1]], initializer=init_embedding_W)\n",
    "context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "\n",
    "if params['n_layers'] == 1:\n",
    "# shared LSTM encoder\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, \n",
    "                use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(context_len, depth=tf.shape(context)[1]), 1)\n",
    "    encoding_context = tf.squeeze(tf.matmul(mask, c_outputs), 1)   # c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(response_len, depth=tf.shape(response)[1]), 1)\n",
    "    encoding_response =  tf.squeeze(tf.matmul(mask, r_outputs), 1)  # r_states.h\n",
    "else:\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, use_peepholes=True, state_is_tuple=False, reuse=tf.get_variable_scope().reuse) \n",
    "                for _ in range(params['n_layers'])]\n",
    "    dropcells = [tf.contrib.rnn.DropoutWrapper(cell,input_keep_prob=keep_prob) for cell in cells]\n",
    "    multicell = tf.contrib.rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "    multicell = tf.contrib.rnn.DropoutWrapper(multicell, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(multicell, context_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(context_len, depth=tf.shape(context)[1]), 1)\n",
    "    encoding_context = tf.squeeze(tf.matmul(mask, c_outputs), 1)   # c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(multicell, response_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(response_len, depth=tf.shape(response)[1]), 1)\n",
    "    encoding_response =  tf.squeeze(tf.matmul(mask, r_outputs), 1)  # r_states.h\n",
    "\n",
    "# Ïƒ(cMr)\n",
    "M = tf.get_variable('M', shape=[params['rnn_dim'], params['rnn_dim']], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "# \"Predict\" a  response: c * M\n",
    "generated_response = tf.matmul(encoding_context, M)\n",
    "generated_response = tf.expand_dims(generated_response, 2)\n",
    "encoding_response = tf.expand_dims(encoding_response, 2)\n",
    "\n",
    "# Dot product between generated response and actual response\n",
    "logits = tf.matmul(generated_response, encoding_response, True)\n",
    "logits = tf.reshape(logits, [-1])\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "probs = tf.sigmoid(logits)\n",
    "correct_prediction = tf.logical_or( tf.logical_and(tf.equal(target,1), tf.greater_equal(probs,0.5)), tf.logical_and(tf.equal(target,0), tf.less(probs,0.5)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Calculate the binary cross-entropy loss\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.to_float(target)))\n",
    "loss = loss + params['l1_loss'] * tf.reduce_sum(tf.abs(M))\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(params['learning_rate'])\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_norm(grad, params['clip']), var) for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in sample and test\n",
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "sample_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "sample_y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in sample_x2]) == 0)\n",
    "sample_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in sample_x1]\n",
    "sample_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in sample_x2]\n",
    "\n",
    "\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)\n",
    "test_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in test_x1]\n",
    "test_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in test_x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_lst_2_id_lst(lst, pad_to_len=-1):\n",
    "    pad_word_id = word2id['<pad>']\n",
    "    pad_len = max(len(lst), 0)\n",
    "    id_list = [word2id[lst[i]] if i<len(lst) and lst[i] in word2id else pad_word_id for i in range(pad_len)]\n",
    "    pad_len = pad_to_len - len(id_list)\n",
    "    if pad_len > 0:\n",
    "        id_list.extend([pad_word_id] * pad_len)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_to_length = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in sample_x1])\n",
    "sample_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in sample_x2])\n",
    "test_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in test_x1])\n",
    "test_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in test_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_8/best/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, record['best_model_dir']+'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample correct    8\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "for q, rs in zip(sample_id1, sample_id2):\n",
    "    for r in rs:\n",
    "        now_score = sess.run(probs, {\n",
    "            context: [q],\n",
    "            response: [r],\n",
    "            keep_prob: params['keep_prob_valid'],\n",
    "            context_len:[len(q)-1],\n",
    "            response_len:[len(r)-1]})[0]\n",
    "        score.append(now_score)\n",
    "#             print(now_score, [id2word[idx] for idx in q], [id2word[idx] for idx in r])\n",
    "score = np.array(score).reshape(-1, 6)\n",
    "my_ans = np.argmax(score, axis=1)\n",
    "sample_correct = np.sum(my_ans == sample_y)\n",
    "print('sample correct %4d' % (sample_correct), flush=True)\n",
    "record['sample_correct'] = sample_correct.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49914375,  0.50045317,  0.50524336,  0.50411385,  0.50393343,\n",
       "         0.50720251],\n",
       "       [ 0.50686544,  0.51499879,  0.50074548,  0.49391925,  0.50516891,\n",
       "         0.50923228],\n",
       "       [ 0.50903654,  0.50764638,  0.50769573,  0.50530785,  0.50288779,\n",
       "         0.50101858],\n",
       "       [ 0.50604457,  0.50621617,  0.49536687,  0.50626075,  0.49702889,\n",
       "         0.50689912],\n",
       "       [ 0.50049418,  0.50183284,  0.49242011,  0.50053787,  0.49925581,\n",
       "         0.50967932],\n",
       "       [ 0.49790299,  0.51386374,  0.50519663,  0.50586748,  0.51938707,\n",
       "         0.50406784],\n",
       "       [ 0.55156147,  0.5075438 ,  0.48584616,  0.55778337,  0.51230252,\n",
       "         0.51879293],\n",
       "       [ 0.50732559,  0.49826074,  0.50446159,  0.5012539 ,  0.50704503,\n",
       "         0.50931817],\n",
       "       [ 0.50397664,  0.4952471 ,  0.49991098,  0.49277857,  0.48833892,\n",
       "         0.50313699],\n",
       "       [ 0.49768269,  0.4997144 ,  0.50146765,  0.48420268,  0.50164115,\n",
       "         0.50265551],\n",
       "       [ 0.50436151,  0.49945605,  0.50297558,  0.5019393 ,  0.50887924,\n",
       "         0.51526535],\n",
       "       [ 0.49424875,  0.49580851,  0.49938741,  0.49339783,  0.50293696,\n",
       "         0.49778405],\n",
       "       [ 0.49137843,  0.49428257,  0.51021928,  0.51480931,  0.50304294,\n",
       "         0.50917447],\n",
       "       [ 0.50835955,  0.49536183,  0.48539433,  0.52606148,  0.50266403,\n",
       "         0.48543072],\n",
       "       [ 0.50876158,  0.51429749,  0.50963962,  0.49941286,  0.53369415,\n",
       "         0.51335621],\n",
       "       [ 0.50705755,  0.49340203,  0.49413514,  0.50329131,  0.51489818,\n",
       "         0.48610044],\n",
       "       [ 0.50573415,  0.48900348,  0.51084793,  0.50244766,  0.50876153,\n",
       "         0.49917144],\n",
       "       [ 0.52306193,  0.50669944,  0.51918912,  0.52805352,  0.51716202,\n",
       "         0.51387215],\n",
       "       [ 0.50373715,  0.50157833,  0.50343126,  0.49775067,  0.50630307,\n",
       "         0.50355709],\n",
       "       [ 0.50653362,  0.50001252,  0.49867332,  0.51747382,  0.49875334,\n",
       "         0.51658607],\n",
       "       [ 0.49983901,  0.50313205,  0.49627537,  0.49898639,  0.49854073,\n",
       "         0.49596599],\n",
       "       [ 0.50292051,  0.49572214,  0.51438218,  0.51044208,  0.50244379,\n",
       "         0.49350002],\n",
       "       [ 0.49741909,  0.49342182,  0.50209713,  0.48041561,  0.4946889 ,\n",
       "         0.50291473],\n",
       "       [ 0.5268963 ,  0.5139454 ,  0.53507268,  0.51067895,  0.49070168,\n",
       "         0.51624817],\n",
       "       [ 0.49839312,  0.49455056,  0.4971005 ,  0.49469224,  0.49204686,\n",
       "         0.49931929],\n",
       "       [ 0.49990514,  0.46846008,  0.48328033,  0.49395776,  0.48612928,\n",
       "         0.50000471],\n",
       "       [ 0.49474597,  0.50819904,  0.51073688,  0.50396186,  0.48984179,\n",
       "         0.50961721],\n",
       "       [ 0.49529219,  0.50597513,  0.48924989,  0.49738589,  0.50249529,\n",
       "         0.49914834],\n",
       "       [ 0.48887637,  0.47441891,  0.4932825 ,  0.49093452,  0.46123144,\n",
       "         0.49284771],\n",
       "       [ 0.50417382,  0.5029301 ,  0.49843776,  0.49775523,  0.49646246,\n",
       "         0.50533748],\n",
       "       [ 0.51037943,  0.5199278 ,  0.52091247,  0.50262713,  0.50294203,\n",
       "         0.48477507],\n",
       "       [ 0.50155598,  0.49274969,  0.51495349,  0.50063914,  0.49997258,\n",
       "         0.50075388],\n",
       "       [ 0.50281435,  0.50382924,  0.5003376 ,  0.50318682,  0.52172822,\n",
       "         0.49532861],\n",
       "       [ 0.54999804,  0.53042775,  0.54178971,  0.5131712 ,  0.51823288,\n",
       "         0.52325821],\n",
       "       [ 0.48069122,  0.49886894,  0.50008512,  0.49866712,  0.50416511,\n",
       "         0.50354087],\n",
       "       [ 0.50282854,  0.49943575,  0.50345039,  0.50468099,  0.4917376 ,\n",
       "         0.49657503],\n",
       "       [ 0.47460097,  0.48479956,  0.48500609,  0.49375725,  0.48937705,\n",
       "         0.49380994],\n",
       "       [ 0.51587123,  0.49337664,  0.51377505,  0.5101428 ,  0.50229383,\n",
       "         0.51518828],\n",
       "       [ 0.5096553 ,  0.4971132 ,  0.51239258,  0.49354863,  0.50887483,\n",
       "         0.50859338],\n",
       "       [ 0.50334603,  0.50541276,  0.49192557,  0.48989499,  0.51245356,\n",
       "         0.50680041],\n",
       "       [ 0.49818912,  0.51115632,  0.51177436,  0.49721053,  0.50179982,\n",
       "         0.50299358],\n",
       "       [ 0.48662251,  0.50418222,  0.4707776 ,  0.49548835,  0.49655962,\n",
       "         0.50953209],\n",
       "       [ 0.51988727,  0.52452916,  0.51782769,  0.50015014,  0.48294413,\n",
       "         0.5041008 ],\n",
       "       [ 0.51994729,  0.50964302,  0.50423419,  0.49237543,  0.50256872,\n",
       "         0.50953877],\n",
       "       [ 0.49558964,  0.50853747,  0.49531364,  0.49918023,  0.53572744,\n",
       "         0.51445156],\n",
       "       [ 0.55699509,  0.50930703,  0.52452362,  0.5500797 ,  0.51209527,\n",
       "         0.51048881],\n",
       "       [ 0.49032667,  0.50263691,  0.50023001,  0.52281946,  0.49801379,\n",
       "         0.49555686],\n",
       "       [ 0.52593851,  0.50676316,  0.50706679,  0.50050926,  0.49737021,\n",
       "         0.49957442],\n",
       "       [ 0.52254903,  0.49768522,  0.53053433,  0.47306275,  0.5403828 ,\n",
       "         0.5287962 ],\n",
       "       [ 0.49959114,  0.52331221,  0.50017309,  0.5063132 ,  0.49999607,\n",
       "         0.50205243]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
