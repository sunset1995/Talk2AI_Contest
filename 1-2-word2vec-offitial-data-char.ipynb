{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus file of Offitial training data\n",
    "* Remove non-chinese-word\n",
    "* Use jieba cut\n",
    "* 輸出到 'datas/official_all_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fname = [\n",
    "    'datas/training_data/no_TC_下課花路米.txt',\n",
    "    'datas/training_data/no_TC_人生劇展.txt',\n",
    "    'datas/training_data/no_TC_公視藝文大道.txt',\n",
    "    'datas/training_data/no_TC_成語賽恩思.txt',\n",
    "    'datas/training_data/no_TC_我的這一班.txt',\n",
    "    'datas/training_data/no_TC_流言追追追.txt',\n",
    "    'datas/training_data/no_TC_聽聽看.txt',\n",
    "    'datas/training_data/no_TC_誰來晚餐.txt',\n",
    "]\n",
    "\n",
    "corpus = []\n",
    "for fname in corpus_fname:\n",
    "    with open(fname, 'r') as f:\n",
    "        corpus.extend([[ch for s in line.strip().split('\\t') for word in s.strip().split() for ch in word] for line in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 助教說是 train word2vec 不錯的參數:\n",
    "-./word2vec/trunk/word2vec -train corpus.txt -output my.cbow.200d.txt -size 200 -windows 5 sample 1e-4 -negative 10 -hs 0 -cbow 1 -iter 15 -threads 8 -min - count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 253.9769917440135\n",
      "vocab    size: 4934\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec model\n",
    "word2vec_model = word2vec.Word2Vec(corpus, size=200, window=5, workers=2, min_count=5, sample=1e-4, negative=10, iter=15)\n",
    "print('training time:', word2vec_model.total_train_time)\n",
    "print('vocab    size:', len(word2vec_model.wv.vocab))\n",
    "\n",
    "word2vec_model.save('models/word2vec_no_tc_offitial_char_200.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
