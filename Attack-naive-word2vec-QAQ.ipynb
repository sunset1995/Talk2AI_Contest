{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# naive word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/sunset/Talk2AI_Contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /var/folders/43/l4vp_w_x4wb11mmy_bb1jrkc0000gn/T/jieba.u857f67a870683287981bc6f5b9493ffc.cache\n",
      "Loading model cost 1.937 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in x2]) == 0)\n",
    "\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)\n",
    "with open('datas/AIFirst_test_answer.txt', 'r') as f:\n",
    "    f.readline()\n",
    "    test_y = np.array([int(line.strip().split(',')[-1]) for line in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'models/word2vec/dual-lstm-12-best',\n",
    "    'models/word2vec/dual-lstm-12-newest',\n",
    "    'models/word2vec/dual-lstm-13-best',\n",
    "    'models/word2vec/dual-lstm-13-newest',\n",
    "    'models/word2vec/dual-lstm-14-best',\n",
    "    'models/word2vec/dual-lstm-14-newest',\n",
    "    'models/word2vec/dual-lstm-15-best',\n",
    "    'models/word2vec/dual-lstm-15-newest',\n",
    "    'models/word2vec/dual-lstm-16-best',\n",
    "    'models/word2vec/dual-lstm-16-newest',\n",
    "    'models/word2vec/dual-lstm-17-best',\n",
    "    'models/word2vec/dual-lstm-17-newest',\n",
    "    'models/word2vec/dual-lstm-18-best',\n",
    "    'models/word2vec/dual-lstm-18-newest',\n",
    "    'models/word2vec/dual-lstm-22-best',\n",
    "    'models/word2vec/dual-lstm-22-newest',\n",
    "    'models/word2vec/dual-lstm-24-best',\n",
    "    'models/word2vec/dual-lstm-24-newest',\n",
    "    'models/word2vec/smn-1-best',\n",
    "    'models/word2vec/smn-1-newest',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65865"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = KeyedVectors.load('models/word2vec/dual-lstm-12-best')\n",
    "len(word_vectors.vocab)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('models/word2vec/dual-lstm-12-best.txt', binary=False)\n",
    "# len(word_vectors.vocab)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('models/word2vec/vec200_win40_iter15_mincnt1.bin', binary=True)\n",
    "# len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_cnt = 0\n",
    "for a, b in zip(x1, x2):\n",
    "    a_sentence = [word for s in a for word in jieba.cut(s) if word.strip() != '']\n",
    "    b_sentences = [[word for word in jieba.cut(s) if word.strip() != ''] for s in b]\n",
    "    \n",
    "    unk_cnt += len([w for w in a_sentence if w not in word_vectors.vocab])\n",
    "    unk_cnt += len([w for s in b_sentences for w in s if w not in word_vectors.vocab])\n",
    "unk_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive trial - centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unitvec(vec):\n",
    "    l = np.linalg.norm(vec)\n",
    "    return vec / l if l != 0 else vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def centroid(sentence):\n",
    "    vecs = [word_vectors.word_vec(word) for word in sentence if word in word_vectors.vocab]\n",
    "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [word for s in a for word in jieba.cut(s) if word.strip() != '']\n",
    "        b_sentences = [[word for word in jieba.cut(s) if word.strip() != ''] for s in b]\n",
    "\n",
    "        a_center = centroid(a_sentence)\n",
    "        b_centers = [centroid(s) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def attack_naive_centroid(x1, x2):\n",
    "    my_cos_ans = centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample centroid: 0.6600\n",
      "               test   centroid: 0.6800\n"
     ]
    }
   ],
   "source": [
    "cos_ans = attack_naive_centroid(x1, x2)\n",
    "test_cos_ans = attack_naive_centroid(test_x1, test_x2)\n",
    "\n",
    "print('%30s: %.4f' % ('sample centroid', np.sum(cos_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   centroid', np.sum(test_cos_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive trial - dis centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dis_centroid(ss, beta=0.77):\n",
    "    for s in ss:\n",
    "        assert(type(s) == list)\n",
    "    vecs = [[word_vectors.word_vec(word) for word in s if word in word_vectors.vocab] for s in ss]\n",
    "    vecs = [s for s in vecs if len(s) > 0]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    cens = list(reversed([np.mean(vs, axis=0) for vs in vecs]))\n",
    "    for cen in cens:\n",
    "        assert(np.sum(np.isnan(cen)) == 0)\n",
    "    w_sum = sum(beta**i for i in range(len(cens)))\n",
    "    return np.sum([cens[i] * (beta ** i / w_sum) for i in range(len(cens))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dis_centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [[word for word in jieba.cut(s) if word.strip()] for s in a]\n",
    "        b_sentences = [[word for word in jieba.cut(s) if word.strip()] for s in b]\n",
    "\n",
    "        a_center = dis_centroid(a_sentence)\n",
    "        b_centers = [dis_centroid([s]) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def attack_naive_dis_centroid(x1, x2):\n",
    "    my_cos_ans = dis_centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample centroid: 0.6000\n",
      "               test   centroid: 0.6960\n"
     ]
    }
   ],
   "source": [
    "dis_cos_ans = attack_naive_dis_centroid(x1, x2)\n",
    "test_dis_cos_ans = attack_naive_dis_centroid(test_x1, test_x2)\n",
    "\n",
    "print('%30s: %.4f' % ('sample centroid', np.sum(dis_cos_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   centroid', np.sum(test_dis_cos_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive trial - weighted centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def w_centroid(ss, beta=0.77):\n",
    "    for s in ss:\n",
    "        assert(type(s) == list)\n",
    "    vecs = [[word_vectors.word_vec(word) for word in s if word in word_vectors.vocab] for s in ss]\n",
    "    vecs = list(reversed([s for s in vecs if len(s) > 0]))\n",
    "    w_cen = np.zeros(word_vectors.vector_size)\n",
    "    if len(vecs) == 0:\n",
    "        return w_cen\n",
    "    w = np.array([beta**i for i in range(len(vecs)) for _ in range(len(vecs[i]))]).reshape(-1, 1)\n",
    "    cen = np.array([vec for s in vecs for vec in s])\n",
    "    return np.sum(w * cen, axis=0) / np.sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def w_centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [[word for word in jieba.cut(s) if word.strip()] for s in a]\n",
    "        b_sentences = [[word for word in jieba.cut(s) if word.strip()] for s in b]\n",
    "\n",
    "        a_center = w_centroid(a_sentence)\n",
    "        b_centers = [w_centroid([s]) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def attack_naive_w_centroid(x1, x2):\n",
    "    my_cos_ans = w_centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample centroid: 0.6800\n",
      "               test   centroid: 0.6920\n"
     ]
    }
   ],
   "source": [
    "w_cos_ans = attack_naive_w_centroid(x1, x2)\n",
    "test_w_cos_ans = attack_naive_w_centroid(test_x1, test_x2)\n",
    "\n",
    "print('%30s: %.4f' % ('sample centroid', np.sum(w_cos_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   centroid', np.sum(test_w_cos_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bagging naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sample bagging: 0.6800\n",
      "                test   bagging: 0.6900\n"
     ]
    }
   ],
   "source": [
    "mode_ans = stats.mode([cos_ans, dis_cos_ans, w_cos_ans]).mode[0]\n",
    "test_mode_ans = stats.mode([test_cos_ans, test_dis_cos_ans, test_w_cos_ans]).mode[0]\n",
    "\n",
    "print('%30s: %.4f' % ('sample bagging', np.sum(mode_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   bagging', np.sum(test_mode_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def blend_prob(x1, x2):\n",
    "    score_a = centroid_score(x1, x2)\n",
    "    score_b = dis_centroid_score(x1, x2)\n",
    "    score_c = w_centroid_score(x1, x2)\n",
    "    prob_a = np.exp(score_a) / np.sum(np.exp(score_a), axis=1).reshape(-1, 1)\n",
    "    prob_b = np.exp(score_b) / np.sum(np.exp(score_b), axis=1).reshape(-1, 1)\n",
    "    prob_c = np.exp(score_c) / np.sum(np.exp(score_c), axis=1).reshape(-1, 1)\n",
    "    return (prob_a + prob_b + prob_c) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample blending: 0.6800\n",
      "               test   blending: 0.6900\n"
     ]
    }
   ],
   "source": [
    "blend_prob_ans = np.argmax(blend_prob(x1, x2), axis=1)\n",
    "test_blend_prob_ans = np.argmax(blend_prob(test_x1, test_x2), axis=1)\n",
    "\n",
    "print('%30s: %.4f' % ('sample blending', np.sum(blend_prob_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   blending', np.sum(test_blend_prob_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load('models/word2vec/smn-1-newest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6800 0.6840 0.7000 0.6860 0.7000 0.6940 0.7000 0.6900 0.7000 0.6940'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_answers = [\n",
    "    attack_naive_centroid(x1, x2),\n",
    "    attack_naive_dis_centroid(x1, x2),\n",
    "    attack_naive_w_centroid(x1, x2),\n",
    "]\n",
    "test_answers = [\n",
    "    attack_naive_centroid(test_x1, test_x2),\n",
    "    attack_naive_dis_centroid(test_x1, test_x2),\n",
    "    attack_naive_w_centroid(test_x1, test_x2),\n",
    "]\n",
    "\n",
    "sample_answers.append(stats.mode(sample_answers[:3]).mode[0])\n",
    "test_answers.append(stats.mode(test_answers[:3]).mode[0])\n",
    "sample_answers.append(np.argmax(blend_prob(x1, x2), axis=1))\n",
    "test_answers.append(np.argmax(blend_prob(test_x1, test_x2), axis=1))\n",
    "\n",
    "' '.join(['%.4f %.4f' % (np.sum(s_ans == y) / len(y), np.sum(t_ans == test_y) / len(test_y))\n",
    "     for s_ans, t_ans in zip(sample_answers, test_answers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done models/word2vec/dual-lstm-12-best\n",
      "done models/word2vec/dual-lstm-12-newest\n",
      "done models/word2vec/dual-lstm-13-best\n",
      "done models/word2vec/dual-lstm-13-newest\n",
      "done models/word2vec/dual-lstm-14-best\n",
      "done models/word2vec/dual-lstm-14-newest\n",
      "done models/word2vec/dual-lstm-15-best\n",
      "done models/word2vec/dual-lstm-15-newest\n",
      "done models/word2vec/dual-lstm-16-best\n",
      "done models/word2vec/dual-lstm-16-newest\n",
      "done models/word2vec/dual-lstm-17-best\n",
      "done models/word2vec/dual-lstm-17-newest\n",
      "done models/word2vec/dual-lstm-18-best\n",
      "done models/word2vec/dual-lstm-18-newest\n",
      "done models/word2vec/dual-lstm-22-best\n",
      "done models/word2vec/dual-lstm-22-newest\n",
      "done models/word2vec/dual-lstm-24-best\n",
      "done models/word2vec/dual-lstm-24-newest\n",
      "done models/word2vec/smn-1-best\n",
      "done models/word2vec/smn-1-newest\n"
     ]
    }
   ],
   "source": [
    "test_answers = []\n",
    "sample_answers = []\n",
    "for mn in model_names:\n",
    "    word_vectors = KeyedVectors.load(mn)\n",
    "    sample_answers.extend([\n",
    "        attack_naive_centroid(x1, x2),\n",
    "        attack_naive_dis_centroid(x1, x2),\n",
    "        attack_naive_w_centroid(x1, x2),\n",
    "    ])\n",
    "    test_answers.extend([\n",
    "        attack_naive_centroid(test_x1, test_x2),\n",
    "        attack_naive_dis_centroid(test_x1, test_x2),\n",
    "        attack_naive_w_centroid(test_x1, test_x2),\n",
    "    ])\n",
    "    print('done', mn)\n",
    "sample_answers = np.array(sample_answers)\n",
    "test_answers = np.array(test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_answers[-12] != test_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, mn in enumerate(model_names):\n",
    "#     now_ans = stats.mode(test_answers[i*3:i*3+3]).mode[0]\n",
    "#     with open('answer/exp/naive-' + mn.split('/')[2] + '.txt', 'w') as f:\n",
    "#         f.write('id,ans\\n')\n",
    "#         f.write('\\n'.join(['%d,%d' % (idx+1, a) for idx, a in enumerate(now_ans)]))\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_all_mode = stats.mode(sample_answers).mode[0]\n",
    "test_all_mode = stats.mode(test_answers).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample all vote: 0.6800\n",
      "               test   all vote: 0.6980\n"
     ]
    }
   ],
   "source": [
    "print('%30s: %.4f' % ('sample all vote', np.sum(sample_all_mode == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   all vote', np.sum(test_all_mode == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60,  0,  0, 60, 60,  0, 60, 60, 54,  0,  0,  3, 60,  0,  0, 60, 60,\n",
       "        0,  0,  0,  0,  0,  0,  0, 60, 18,  0,  0,  0, 60,  0,  0,  0,  0,\n",
       "       60,  0,  0,  0, 16, 60, 60,  0,  0, 10,  0, 56, 16,  0, 19, 60])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_answers != y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60,  0,  0, 60, 60,  0, 26,  0, 60,  0,  0, 59,  0,  0, 60,  0, 24,\n",
       "        0, 60,  2, 60,  0, 60, 60,  0,  0,  0,  0,  0, 17, 60,  0,  0, 18,\n",
       "        0, 60, 60,  0,  0,  0, 36,  0, 15,  0, 22,  0,  6,  0, 60, 60, 32,\n",
       "       15,  0, 58,  0,  0,  0, 60,  0,  0, 58,  0,  0,  0,  0,  0, 60, 54,\n",
       "       60,  0,  0,  0,  0, 60, 30, 60, 60,  0,  0, 60, 42,  0,  0, 60,  0,\n",
       "        8,  0, 60, 17,  0,  0,  0, 54,  0,  0, 18,  6,  6,  0,  0, 60,  0,\n",
       "        0,  0, 60,  0,  0,  0,  0, 60,  0,  0,  0,  0,  0,  0, 60,  0, 60,\n",
       "        0,  0, 14, 54,  0, 60,  0,  0, 18,  0,  0,  0, 60,  0, 58,  0,  0,\n",
       "        0, 33,  4,  0,  0, 60, 60, 40,  0, 60, 60,  0,  0, 60,  6,  3,  0,\n",
       "        5,  0,  0, 53,  0,  0,  3,  0, 60,  0,  0, 60, 60,  0, 56, 60,  0,\n",
       "        0, 60, 60,  0, 60,  0,  0,  0, 50, 60,  0, 60,  2, 60, 48,  0,  6,\n",
       "        0,  0,  2, 60, 12,  0,  0, 60,  0, 60, 60,  0, 54, 60,  0, 60,  0,\n",
       "        0, 54,  0,  0,  0,  0,  0,  0,  0, 60,  0, 60,  0,  0,  0, 21, 60,\n",
       "        0, 25,  0, 34,  0,  0, 60,  0,  0,  0,  0,  0,  0, 36,  0,  0,  0,\n",
       "        0,  0,  5,  0,  0, 60,  0,  6,  0,  0,  0, 60,  0,  0, 60, 60, 60,\n",
       "       27, 60, 27, 16,  0,  0,  0, 60,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, 16, 59,  0,  0,  0,  0,  0, 56, 60,  0,  0, 28,  0, 48,  0, 42,\n",
       "        0,  0,  0, 60,  0,  0,  4,  0,  0,  0,  0,  0, 60,  0,  0, 60, 60,\n",
       "       60, 17, 60,  4,  0, 54, 60, 60,  0, 23, 54, 38, 38,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, 60,  0,  0,  0, 60, 60,  0,  0, 41, 60,  0,  0,  0,\n",
       "        0,  0,  0, 60, 54, 26, 60, 56, 60, 60, 19,  0,  0,  0, 60,  0,  0,\n",
       "        0,  0, 60,  0,  4,  9, 42, 54,  0,  0, 24,  0,  0,  0,  0, 60,  0,\n",
       "        0,  0,  0,  0, 60,  0,  0,  6,  0, 60,  0,  0,  0,  6,  0,  0, 52,\n",
       "       54,  0,  0, 60, 60, 60,  0,  0, 43,  0, 60,  0, 60, 36, 60,  0,  3,\n",
       "        0,  0, 60, 24,  0,  0, 60,  0,  0, 59,  0,  6,  0,  0,  0, 60, 33,\n",
       "       47, 18,  0,  0,  0,  0,  0,  0, 56,  0, 60,  0, 60, 40, 60,  0,  0,\n",
       "        0, 30,  0,  0,  0,  0,  0, 60,  0,  0,  0,  0, 10,  0,  0,  0, 60,\n",
       "        0, 60,  0,  0,  0,  0,  0, 29,  0, 60, 60,  0, 33, 60,  0,  0,  0,\n",
       "        0, 44,  0,  0,  0,  0, 27,  0, 54,  0, 60,  0, 60, 60, 60, 60,  9,\n",
       "       40,  0,  4,  0,  0, 44,  3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_answers != test_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done models/word2vec/dual-lstm-12-best\n",
      "done models/word2vec/dual-lstm-12-newest\n",
      "done models/word2vec/dual-lstm-13-best\n",
      "done models/word2vec/dual-lstm-13-newest\n",
      "done models/word2vec/dual-lstm-14-best\n",
      "done models/word2vec/dual-lstm-14-newest\n",
      "done models/word2vec/dual-lstm-15-best\n",
      "done models/word2vec/dual-lstm-15-newest\n",
      "done models/word2vec/dual-lstm-16-best\n",
      "done models/word2vec/dual-lstm-16-newest\n",
      "done models/word2vec/dual-lstm-17-best\n",
      "done models/word2vec/dual-lstm-17-newest\n",
      "done models/word2vec/dual-lstm-18-best\n",
      "done models/word2vec/dual-lstm-18-newest\n",
      "done models/word2vec/dual-lstm-22-best\n",
      "done models/word2vec/dual-lstm-22-newest\n",
      "done models/word2vec/dual-lstm-24-best\n",
      "done models/word2vec/dual-lstm-24-newest\n",
      "done models/word2vec/smn-1-best\n",
      "done models/word2vec/smn-1-newest\n"
     ]
    }
   ],
   "source": [
    "test_prob = []\n",
    "sample_prob = []\n",
    "for mn in model_names:\n",
    "    word_vectors = KeyedVectors.load(mn)\n",
    "    sample_prob.append(blend_prob(x1, x2))\n",
    "    test_prob.append(blend_prob(test_x1, test_x2))\n",
    "    print('done', mn)\n",
    "test_prob = np.array(test_prob)\n",
    "sample_prob = np.array(sample_prob)\n",
    "test_all_blend_ans = np.argmax(np.sum(test_prob, axis=0), axis=1)\n",
    "sample_all_blend_ans = np.argmax(np.sum(sample_prob, axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sample all vote: 0.6800\n",
      "               test   all vote: 0.6980\n"
     ]
    }
   ],
   "source": [
    "print('%30s: %.4f' % ('sample all vote', np.sum(sample_all_blend_ans == y) / len(y)))\n",
    "print('%30s: %.4f' % ('test   all vote', np.sum(test_all_blend_ans == test_y) / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Output answer on testing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# with open('answer/exp/exp_14_best_naive.txt', 'w') as f:\n",
    "#     f.write('id,ans\\n')\n",
    "#     for i, ans in enumerate(test_cos_ans):\n",
    "#         f.write('%d,%d\\n' % (i+1, ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
