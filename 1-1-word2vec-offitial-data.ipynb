{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus file of Offitial training data\n",
    "* Remove non-chinese-word\n",
    "* Use jieba cut\n",
    "* 輸出到 'datas/official_all_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/sunset/word_contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u849ecfdca27003d306f39ca004b82b5b.cache\n",
      "Loading model cost 1.174 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Set up tokenize dictionary\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在 'datas/training_data/subtitle_no_TC/我的這一班/聽稿版' 裡的 files 有包含說話者人名，先不要用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(460)_G000021604600005_010.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(461)_G000021604610005_002.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(464)_G000021604640005_008.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(472)_G000021604720005_024.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(465)_G000021604650005_010.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(462)_G000021604620005_004.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(466)_G000021604660005_012.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(467)_G000021604670005_014.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(471)_G000021604710009_022.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(470)_G000021604700006_020.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(473)_G000021604730006_026.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(468)_G000021604680005_016.txt\tno such file: datas/training_data/subtitle_no_TC/我的這一班/我的這一班(469)_G000021604690005_018.txt\t"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pdb \n",
    "import re\n",
    "\n",
    "import jieba\n",
    "\n",
    "not_chinese_word = u'[^\\u4e00-\\u9fff]'  # only keep Chinese word, remove \n",
    "\n",
    "def create_corpus(out_filename, file_mode='w'):\n",
    "    out_f = open(out_filename, file_mode)\n",
    "    \n",
    "    all_corpus_dirs = ['datas/training_data/subtitle_with_TC', 'datas/training_data/subtitle_no_TC']\n",
    "    for corpus_dir in all_corpus_dirs:\n",
    "        for _, dirs, corpus_files in os.walk(corpus_dir):\n",
    "            for dir in dirs:\n",
    "                for _,_,files in os.walk(corpus_dir+'/'+dir):\n",
    "                    for fn in files:\n",
    "                        if fn[0] == '.':\n",
    "                            continue\n",
    "                        try:\n",
    "                            f = open(corpus_dir+'/'+dir+'/'+fn, 'r')\n",
    "                            for line in f:\n",
    "                                line = re.sub(not_chinese_word, ' ', line)\n",
    "\n",
    "                                # word segmentation with jieba.\n",
    "                                words = list(jieba.cut(line))\n",
    "                                #words = [w for w in words if w != ' ']\n",
    "                                # write to file.\n",
    "                                out_f.write('%s\\n' % ' '.join(words))\n",
    "                        except:\n",
    "                            print('no such file: %s' %corpus_dir+'/'+dir+'/'+fn, end='\\t')\n",
    "                            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_corpus('datas/official_all_corpus.txt', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['躲', '推銷員', '和', '金蟬脫殼']\n",
      "====================================================================================================\n",
      "\n",
      "['有', '什麼', '關係', '呢']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Create sentences from preprocessed corpus file\n",
    "sentences = word2vec.LineSentence('datas/official_all_corpus.txt')\n",
    "\n",
    "# Show tokenize \"sentence\"\n",
    "for i, s in enumerate(sentences):\n",
    "    print()\n",
    "    print(s)\n",
    "    print('=' * 100)\n",
    "    if i >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 助教說是 train word2vec 不錯的參數:\n",
    "-./word2vec/trunk/word2vec -train corpus.txt -output my.cbow.200d.txt -size 200 -windows 5 sample 1e-4 -negative 10 -hs 0 -cbow 1 -iter 15 -threads 8 -min - count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389.8865989702754"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train word2vec model\n",
    "# word2vec_model = word2vec.Word2Vec(sentences, size=200, window=5, workers=4, min_count=5, sample=1e-4, negative=10, iter=15)\n",
    "# word2vec_model.total_train_time\n",
    "\n",
    "# word2vec_model.save('models/word2vec_all_offitial_200.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrain\n",
    "# word2vec_model = gensim.models.Word2Vec.load('models/word2vec_all_offitial_200.model.bin')\n",
    "# word2vec_model.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由現有 word2vec model 建立 dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec.load('models/word2vec_all_offitial_200.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朋友 ->  Vocab(count:18080, index:100, sample_int:1483476828)\n",
      "朋友 ->  100\n"
     ]
    }
   ],
   "source": [
    "vocab = word2vec_model.wv.vocab\n",
    "print('朋友 -> ', word2vec_model.wv.vocab['朋友'])\n",
    "print('朋友 -> ', word2vec_model.wv.vocab['朋友'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將公視文字資料轉成 word2vec model 對應的 vocab id\n",
    "* 存成 npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出 embedding \n",
    "* [輸出給 keras 用](http://ben.bolte.cc/blog/2016/gensim.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
