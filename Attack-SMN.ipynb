{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from /home/sunset/Talk2AI_Contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.ufb7b5ede4bbc311ed39003ae859d1289.cache\n",
      "Loading model cost 1.161 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "\n",
    "from mini_batch_helper import extractor, MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in  training data\n",
    "word2vec_fname = 'models/word2vec/fine-tuned-2.txt'\n",
    "corpus_fnames = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "    'datas/training_data/人生劇展.txt',\n",
    "    'datas/training_data/公視藝文大道.txt',\n",
    "    'datas/training_data/成語賽恩思.txt',\n",
    "    'datas/training_data/我的這一班.txt',\n",
    "    'datas/training_data/流言追追追.txt',\n",
    "    'datas/training_data/聽聽看.txt',\n",
    "    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "sample_rate_on_training_datas = 1.0  # 1.0\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = None\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, corpus_fnames, sample_rate_on_training_datas, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "batchSize=128\n",
    "wordLen=64\n",
    "\n",
    "filterSize=10\n",
    "strideSize=1\n",
    "\n",
    "fm1_num=25\n",
    "fm2_num=50\n",
    "\n",
    "use_gru=True #if false, use lstm\n",
    "use_dropout=False\n",
    "\n",
    "fm1Size=int(wordLen/(2*strideSize))\n",
    "fm2Size=int(wordLen/(2*strideSize)/(2*strideSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datas num: 5767705\n",
      "valid datas num: 14102\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "#rnd_idx = np.arange(len(corpus_id))\n",
    "#np.random.shuffle(rnd_idx)\n",
    "#corpus_id = corpus_id[rnd_idx[:len(corpus_id)]]\n",
    "valid_corpus_num = 10\n",
    "\n",
    "train_data_loader = MiniBatchCorpus(corpus_id[valid_corpus_num:], context_len=3, max_len=64)\n",
    "valid_data_loader = MiniBatchCorpus(corpus_id[:valid_corpus_num], context_len=3, max_len=64)\n",
    "print('train datas num:', train_data_loader.data_num, flush=True)\n",
    "print('valid datas num:', valid_data_loader.data_num, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in sample\n",
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "sample_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "sample_y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in sample_x2]) == 0)\n",
    "sample_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in sample_x1]\n",
    "sample_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in sample_x2]\n",
    "\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)\n",
    "test_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in test_x1]\n",
    "test_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in test_x2]\n",
    "with open('datas/AIFirst_test_answer.txt', 'r') as f:\n",
    "    f.readline()\n",
    "    test_y = np.array([int(line.strip().split(',')[-1]) for line in f])\n",
    "\n",
    "def word_lst_2_id_lst(lst, pad_to_len=-1):\n",
    "    pad_word_id = word2id['<pad>']\n",
    "    pad_len = max(len(lst), 0)\n",
    "    id_list = [word2id[lst[i]] if i<len(lst) and lst[i] in word2id else pad_word_id for i in range(pad_len)]\n",
    "    pad_len = pad_to_len - len(id_list)\n",
    "    if pad_len > 0:\n",
    "        id_list.extend([pad_word_id] * pad_len)\n",
    "    return id_list\n",
    "\n",
    "pad_to_length = wordLen\n",
    "\n",
    "sample_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in sample_x1])\n",
    "sample_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in sample_x2])\n",
    "test_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in test_x1])\n",
    "test_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in test_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_accuracy(next_x1, next_x2, _y, _keep_prob):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={context: next_x1, response: next_x2, keep_prob:_keep_prob})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob:_keep_prob})\n",
    "    return result\n",
    " \n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_uniform(shape,-1.0,1.0)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):\n",
    "    initial = tf.random_uniform(shape,-1.0,1.0)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, strideSize, strideSize, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Input\n",
    "context = tf.placeholder(dtype=tf.int32, shape=(None, None), name='context')\n",
    "context_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='context_len')\n",
    "response = tf.placeholder(dtype=tf.int32, shape=(None, None), name='response')\n",
    "response_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='response_len')\n",
    "target = tf.placeholder(dtype=tf.float32, shape=(None, None), name='target')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding\n",
    "\n",
    "init_embedding_W = tf.constant_initializer(embedding_matrix)\n",
    "embeddings_W = tf.get_variable('embeddings_W', shape=[embedding_matrix.shape[0], embedding_matrix.shape[1]], initializer=init_embedding_W)\n",
    "context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "# here should pass a gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn layer\n",
    "\n",
    "# gru cell\n",
    "if use_gru:\n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=200, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "# lstm cell\n",
    "else:\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=200, forget_bias=1.0, use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "if use_dropout:\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "context_gru = c_outputs\n",
    "r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "response_gru = r_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 matrix and M2 matrix\n",
    "\n",
    "# M1 word dot matrix\n",
    "word_dot_matrix=tf.matmul(context_embedded, response_embedded, False, True)\n",
    "m1_image=tf.reshape(word_dot_matrix, [-1, wordLen, wordLen, 1])\n",
    "m1_image=tf.divide(m1_image, tf.reduce_max(m1_image))\n",
    "\n",
    "# M2 segment dot matrix\n",
    "segment_dot_matrix=tf.matmul(context_gru, response_gru, False, True)\n",
    "m2_image=tf.reshape(segment_dot_matrix, [-1, wordLen, wordLen, 1])\n",
    "m2_image=tf.divide(m2_image, tf.reduce_max(m2_image))\n",
    "\n",
    "y_label=tf.cast(target, tf.float32)\n",
    "\n",
    "# M1 convolution\n",
    "W_conv1_m1 = weight_variable([filterSize,filterSize, 1, fm1_num])\n",
    "b_conv1_m1 = bias_variable([fm1_num])\n",
    "h_conv1_m1 = tf.nn.sigmoid(conv2d(m1_image, W_conv1_m1) + b_conv1_m1)\n",
    "h_pool1_m1 = max_pool_2x2(h_conv1_m1)\n",
    "\n",
    "W_conv2_m1 = weight_variable([filterSize,filterSize, fm1_num, fm2_num])\n",
    "b_conv2_m1 = bias_variable([fm2_num])\n",
    "h_conv2_m1 = tf.nn.sigmoid(conv2d(h_pool1_m1, W_conv2_m1) + b_conv2_m1)\n",
    "h_pool2_m1 = max_pool_2x2(h_conv2_m1)\n",
    "\n",
    "h_pool2_m1_flat = tf.reshape(h_pool2_m1, [-1, fm2Size*fm2Size*fm2_num])\n",
    "\n",
    "# M2 convolution\n",
    "W_conv1_m2 = weight_variable([filterSize,filterSize, 1, fm1_num])\n",
    "b_conv1_m2 = bias_variable([fm1_num])\n",
    "h_conv1_m2 = tf.nn.sigmoid(conv2d(m2_image, W_conv1_m2) + b_conv1_m2)\n",
    "h_pool1_m2 = max_pool_2x2(h_conv1_m2)\n",
    "\n",
    "W_conv2_m2 = weight_variable([filterSize,filterSize, fm1_num, fm2_num])\n",
    "b_conv2_m2 = bias_variable([fm2_num])\n",
    "h_conv2_m2 = tf.nn.sigmoid(conv2d(h_pool1_m2, W_conv2_m2) + b_conv2_m2)\n",
    "h_pool2_m2 = max_pool_2x2(h_conv2_m2)\n",
    "\n",
    "h_pool2_m2_flat = tf.reshape(h_pool2_m2, [-1, fm2Size*fm2Size*fm2_num])\n",
    "\n",
    "# Accumulate M1 and M2\n",
    "matching_accumulation = tf.add(h_pool2_m1_flat, h_pool2_m2_flat)\n",
    "\n",
    "W_fc1 = weight_variable([fm2Size*fm2Size*fm2_num, wordLen*wordLen])\n",
    "b_fc1 = bias_variable([wordLen*wordLen])\n",
    "h_fc1 = tf.nn.sigmoid(tf.matmul(matching_accumulation, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = weight_variable([wordLen*wordLen, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_mean(y_label * tf.log(prediction))\n",
    "train_step=tf.train.AdagradOptimizer(learning_rate=learning_rate,initial_accumulator_value=0.001).minimize(cross_entropy)\n",
    "\n",
    "#mean_square_error=tf.reduce_mean(tf.multiply(tf.subtract(y_label, prediction), tf.subtract(y_label, prediction)))\n",
    "#train_step=tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1).minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.46654 0.523438 24 /50  196 /500\n",
      "10 0.30681 0.679688 31 /50  246 /500\n",
      "20 0.300697 0.664062 26 /50  245 /500\n",
      "30 0.271612 0.703125 27 /50  240 /500\n",
      "40 0.326902 0.625 27 /50  249 /500\n",
      "50 0.456517 0.515625 28 /50  259 /500\n",
      "60 0.264976 0.710938 27 /50  256 /500\n",
      "70 0.330166 0.617188 27 /50  254 /500\n",
      "80 0.296531 0.632812 27 /50  252 /500\n",
      "90 0.267195 0.703125 27 /50  252 /500\n",
      "100 0.295663 0.710938 28 /50  255 /500\n",
      "110 0.296535 0.664062 30 /50  262 /500\n",
      "120 0.288191 0.695312 29 /50  266 /500\n",
      "130 0.304712 0.640625 31 /50  263 /500\n",
      "140 0.27289 0.703125 30 /50  264 /500\n",
      "150 0.286914 0.6875 27 /50  260 /500\n",
      "160 0.29494 0.6875 30 /50  257 /500\n",
      "170 0.275794 0.71875 31 /50  262 /500\n",
      "180 0.275541 0.695312 30 /50  267 /500\n",
      "190 0.263863 0.757812 31 /50  266 /500\n",
      "200 0.249903 0.789062 31 /50  272 /500\n",
      "210 0.256865 0.75 30 /50  263 /500\n",
      "220 0.261549 0.742188 31 /50  267 /500\n",
      "230 0.22177 0.757812 31 /50  265 /500\n",
      "240 0.288955 0.695312 30 /50  269 /500\n",
      "250 0.271509 0.710938 30 /50  269 /500\n",
      "260 0.249353 0.757812 33 /50  273 /500\n",
      "270 0.280388 0.703125 32 /50  275 /500\n",
      "280 0.267642 0.726562 33 /50  270 /500\n",
      "290 0.27297 0.757812 28 /50  270 /500\n",
      "300 0.241732 0.765625 34 /50  265 /500\n",
      "310 0.260137 0.820312 34 /50  269 /500\n",
      "320 0.277349 0.710938 33 /50  276 /500\n",
      "330 0.242567 0.765625 33 /50  272 /500\n",
      "340 0.232849 0.773438 32 /50  274 /500\n",
      "350 0.246904 0.773438 32 /50  270 /500\n",
      "360 0.251094 0.734375 32 /50  270 /500\n",
      "370 0.262908 0.734375 32 /50  274 /500\n",
      "380 0.265521 0.664062 31 /50  271 /500\n",
      "390 0.286992 0.695312 30 /50  269 /500\n",
      "400 0.27 0.734375 30 /50  265 /500\n",
      "410 0.247766 0.78125 29 /50  271 /500\n",
      "420 0.266058 0.757812 29 /50  268 /500\n",
      "430 0.255004 0.742188 30 /50  271 /500\n",
      "440 0.247533 0.773438 29 /50  270 /500\n",
      "450 0.267929 0.695312 31 /50  269 /500\n",
      "460 0.270774 0.695312 29 /50  270 /500\n",
      "470 0.238039 0.78125 27 /50  261 /500\n",
      "480 0.248453 0.726562 31 /50  269 /500\n",
      "490 0.254832 0.75 33 /50  264 /500\n",
      "500 0.29867 0.648438 32 /50  268 /500\n",
      "510 0.224895 0.765625 33 /50  266 /500\n",
      "520 0.234949 0.78125 32 /50  265 /500\n",
      "530 0.258893 0.742188 32 /50  265 /500\n",
      "540 0.256384 0.726562 32 /50  266 /500\n",
      "550 0.259509 0.742188 32 /50  266 /500\n",
      "560 0.272985 0.703125 33 /50  265 /500\n",
      "570 0.29446 0.703125 31 /50  266 /500\n",
      "580 0.248311 0.734375 32 /50  264 /500\n",
      "590 0.27637 0.710938 32 /50  265 /500\n",
      "600 0.266298 0.726562 32 /50  265 /500\n",
      "610 0.252202 0.734375 31 /50  265 /500\n",
      "620 0.262114 0.726562 31 /50  264 /500\n",
      "630 0.267401 0.726562 31 /50  265 /500\n",
      "640 0.267471 0.695312 31 /50  263 /500\n",
      "650 0.26325 0.734375 31 /50  262 /500\n",
      "660 0.267466 0.695312 31 /50  264 /500\n",
      "670 0.277432 0.710938 32 /50  263 /500\n",
      "680 0.25497 0.71875 32 /50  264 /500\n",
      "690 0.263422 0.710938 32 /50  262 /500\n",
      "700 0.241651 0.75 32 /50  262 /500\n",
      "710 0.245851 0.703125 32 /50  261 /500\n",
      "720 0.2622 0.726562 32 /50  262 /500\n",
      "730 0.294835 0.6875 32 /50  262 /500\n",
      "740 0.253219 0.75 32 /50  262 /500\n",
      "750 0.22232 0.796875 32 /50  262 /500\n",
      "760 0.230859 0.78125 32 /50  262 /500\n",
      "770 0.271577 0.773438 32 /50  262 /500\n",
      "780 0.284045 0.679688 32 /50  263 /500\n",
      "790 0.255193 0.734375 32 /50  263 /500\n",
      "800 0.253899 0.734375 32 /50  264 /500\n",
      "810 0.277312 0.6875 32 /50  261 /500\n",
      "820 0.220376 0.78125 32 /50  265 /500\n",
      "830 0.283009 0.71875 32 /50  264 /500\n",
      "840 0.273094 0.710938 32 /50  262 /500\n",
      "850 0.241095 0.765625 32 /50  263 /500\n",
      "860 0.277482 0.734375 32 /50  263 /500\n",
      "870 0.2613 0.734375 32 /50  265 /500\n",
      "880 0.239728 0.773438 32 /50  265 /500\n",
      "890 0.291786 0.664062 32 /50  266 /500\n",
      "900 0.262151 0.742188 31 /50  266 /500\n",
      "910 0.223446 0.742188 32 /50  266 /500\n",
      "920 0.249573 0.742188 32 /50  266 /500\n",
      "930 0.238578 0.765625 31 /50  265 /500\n",
      "940 0.240469 0.8125 31 /50  265 /500\n",
      "950 0.281943 0.757812 32 /50  266 /500\n",
      "960 0.231995 0.78125 32 /50  265 /500\n",
      "970 0.245606 0.726562 32 /50  265 /500\n",
      "980 0.230463 0.8125 32 /50  266 /500\n",
      "990 0.259083 0.765625 31 /50  266 /500\n",
      "1000 0.253937 0.75 31 /50  265 /500\n",
      "1010 0.268018 0.703125 31 /50  265 /500\n",
      "1020 0.313462 0.664062 31 /50  265 /500\n",
      "1030 0.279404 0.78125 31 /50  265 /500\n",
      "1040 0.281144 0.695312 31 /50  265 /500\n",
      "1050 0.21507 0.8125 31 /50  265 /500\n",
      "1060 0.302954 0.648438 31 /50  266 /500\n",
      "1070 0.268489 0.71875 31 /50  265 /500\n",
      "1080 0.246362 0.765625 31 /50  265 /500\n",
      "1090 0.278058 0.71875 31 /50  265 /500\n",
      "1100 0.24764 0.773438 31 /50  266 /500\n",
      "1110 0.217342 0.796875 31 /50  266 /500\n",
      "1120 0.277487 0.726562 31 /50  266 /500\n",
      "1130 0.255858 0.757812 31 /50  266 /500\n",
      "1140 0.329328 0.59375 31 /50  266 /500\n",
      "1150 0.266653 0.765625 31 /50  266 /500\n",
      "1160 0.238139 0.78125 31 /50  265 /500\n",
      "1170 0.246646 0.757812 31 /50  265 /500\n",
      "1180 0.222796 0.734375 31 /50  265 /500\n",
      "1190 0.294704 0.71875 31 /50  265 /500\n",
      "1200 0.233608 0.75 31 /50  265 /500\n",
      "1210 0.249997 0.726562 31 /50  265 /500\n",
      "1220 0.250041 0.765625 31 /50  265 /500\n",
      "1230 0.293024 0.679688 31 /50  265 /500\n",
      "1240 0.265385 0.734375 31 /50  265 /500\n",
      "1250 0.250083 0.765625 31 /50  265 /500\n",
      "1260 0.259965 0.765625 31 /50  265 /500\n",
      "1270 0.241663 0.78125 31 /50  265 /500\n",
      "1280 0.269268 0.695312 31 /50  265 /500\n",
      "1290 0.239123 0.765625 31 /50  265 /500\n",
      "1300 0.280362 0.695312 31 /50  265 /500\n",
      "1310 0.292397 0.695312 31 /50  265 /500\n",
      "1320 0.257028 0.726562 31 /50  265 /500\n",
      "1330 0.230634 0.78125 31 /50  265 /500\n",
      "1340 0.280036 0.710938 31 /50  265 /500\n",
      "1350 0.246937 0.757812 31 /50  265 /500\n",
      "1360 0.253804 0.765625 30 /50  265 /500\n",
      "1370 0.284632 0.75 30 /50  265 /500\n",
      "1380 0.294259 0.6875 30 /50  265 /500\n",
      "1390 0.218711 0.773438 30 /50  265 /500\n",
      "1400 0.252942 0.757812 30 /50  264 /500\n",
      "1410 0.257945 0.765625 30 /50  264 /500\n",
      "1420 0.279625 0.75 30 /50  266 /500\n",
      "1430 0.258003 0.71875 30 /50  266 /500\n",
      "1440 0.212864 0.8125 30 /50  265 /500\n",
      "1450 0.23437 0.789062 30 /50  265 /500\n",
      "1460 0.191412 0.835938 30 /50  264 /500\n",
      "1470 0.255973 0.765625 30 /50  266 /500\n",
      "1480 0.256974 0.757812 30 /50  265 /500\n",
      "1490 0.234564 0.773438 30 /50  265 /500\n",
      "1500 0.262075 0.773438 30 /50  265 /500\n",
      "1510 0.256689 0.734375 30 /50  265 /500\n",
      "1520 0.293929 0.671875 30 /50  265 /500\n",
      "1530 0.253679 0.75 30 /50  265 /500\n",
      "1540 0.233147 0.789062 30 /50  265 /500\n",
      "1550 0.266612 0.757812 30 /50  265 /500\n",
      "1560 0.229933 0.773438 30 /50  265 /500\n",
      "1570 0.261613 0.726562 30 /50  265 /500\n",
      "1580 0.275867 0.6875 30 /50  265 /500\n",
      "1590 0.239227 0.757812 30 /50  265 /500\n",
      "1600 0.266845 0.71875 30 /50  265 /500\n",
      "1610 0.264759 0.71875 30 /50  265 /500\n",
      "1620 0.239695 0.796875 30 /50  265 /500\n",
      "1630 0.261927 0.757812 30 /50  265 /500\n",
      "1640 0.234169 0.796875 30 /50  265 /500\n",
      "1650 0.266278 0.742188 30 /50  265 /500\n",
      "1660 0.273903 0.664062 30 /50  265 /500\n",
      "1670 0.28156 0.695312 30 /50  265 /500\n",
      "1680 0.283007 0.640625 30 /50  265 /500\n",
      "1690 0.256813 0.703125 30 /50  265 /500\n",
      "1700 0.2671 0.734375 30 /50  265 /500\n",
      "1710 0.249961 0.773438 30 /50  265 /500\n",
      "1720 0.244751 0.765625 30 /50  265 /500\n",
      "1730 0.26629 0.742188 30 /50  265 /500\n",
      "1740 0.263297 0.695312 30 /50  265 /500\n",
      "1750 0.261228 0.773438 30 /50  265 /500\n",
      "1760 0.225861 0.804688 30 /50  265 /500\n",
      "1770 0.269531 0.726562 30 /50  265 /500\n",
      "1780 0.306301 0.703125 30 /50  265 /500\n",
      "1790 0.234566 0.820312 30 /50  265 /500\n",
      "1800 0.247352 0.757812 30 /50  265 /500\n",
      "1810 0.27329 0.726562 30 /50  265 /500\n",
      "1820 0.253476 0.75 30 /50  265 /500\n",
      "1830 0.225276 0.78125 30 /50  265 /500\n",
      "1840 0.276425 0.726562 30 /50  265 /500\n",
      "1850 0.232791 0.773438 30 /50  265 /500\n",
      "1860 0.293136 0.742188 30 /50  265 /500\n",
      "1870 0.27437 0.734375 30 /50  265 /500\n",
      "1880 0.274488 0.75 30 /50  265 /500\n",
      "1890 0.264378 0.710938 30 /50  265 /500\n",
      "1900 0.241637 0.789062 30 /50  265 /500\n",
      "1910 0.320419 0.664062 30 /50  265 /500\n",
      "1920 0.254878 0.75 30 /50  265 /500\n",
      "1930 0.259903 0.726562 30 /50  265 /500\n",
      "1940 0.236522 0.78125 30 /50  265 /500\n",
      "1950 0.255708 0.78125 30 /50  265 /500\n",
      "1960 0.254465 0.773438 30 /50  265 /500\n",
      "1970 0.216845 0.789062 30 /50  265 /500\n",
      "1980 0.266153 0.75 30 /50  265 /500\n",
      "1990 0.295669 0.679688 30 /50  265 /500\n",
      "2000 0.246974 0.734375 30 /50  265 /500\n",
      "2010 0.288803 0.679688 30 /50  265 /500\n",
      "2020 0.245094 0.757812 30 /50  265 /500\n",
      "2030 0.279852 0.742188 30 /50  265 /500\n",
      "2040 0.302219 0.726562 30 /50  265 /500\n",
      "2050 0.260462 0.75 30 /50  265 /500\n",
      "2060 0.26212 0.734375 30 /50  265 /500\n",
      "2070 0.289687 0.71875 30 /50  265 /500\n",
      "2080 0.295108 0.664062 30 /50  265 /500\n",
      "2090 0.219194 0.75 30 /50  265 /500\n",
      "2100 0.234794 0.789062 30 /50  265 /500\n",
      "2110 0.287364 0.726562 30 /50  265 /500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120 0.255637 0.765625 30 /50  265 /500\n",
      "2130 0.259361 0.734375 30 /50  265 /500\n",
      "2140 0.218776 0.78125 30 /50  265 /500\n",
      "2150 0.263603 0.710938 30 /50  265 /500\n",
      "2160 0.236603 0.78125 30 /50  265 /500\n",
      "2170 0.293804 0.65625 30 /50  265 /500\n",
      "2180 0.234742 0.78125 30 /50  265 /500\n",
      "2190 0.248321 0.734375 30 /50  265 /500\n",
      "2200 0.217372 0.804688 30 /50  265 /500\n",
      "2210 0.26577 0.71875 30 /50  265 /500\n",
      "2220 0.255356 0.765625 30 /50  265 /500\n",
      "2230 0.234578 0.75 30 /50  265 /500\n",
      "2240 0.222525 0.789062 30 /50  265 /500\n",
      "2250 0.272716 0.75 30 /50  265 /500\n",
      "2260 0.248925 0.757812 30 /50  265 /500\n",
      "2270 0.241539 0.773438 30 /50  265 /500\n",
      "2280 0.272049 0.773438 30 /50  265 /500\n",
      "2290 0.283143 0.71875 30 /50  265 /500\n",
      "2300 0.248947 0.742188 30 /50  265 /500\n",
      "2310 0.240116 0.757812 30 /50  265 /500\n",
      "2320 0.270183 0.75 30 /50  265 /500\n",
      "2330 0.231859 0.765625 30 /50  265 /500\n",
      "2340 0.23327 0.757812 30 /50  265 /500\n",
      "2350 0.248678 0.75 30 /50  265 /500\n",
      "2360 0.25135 0.703125 30 /50  265 /500\n",
      "2370 0.26375 0.710938 30 /50  265 /500\n",
      "2380 0.22969 0.765625 30 /50  265 /500\n",
      "2390 0.232834 0.796875 30 /50  265 /500\n",
      "2400 0.239562 0.734375 30 /50  265 /500\n",
      "2410 0.225519 0.789062 30 /50  265 /500\n",
      "2420 0.26883 0.703125 30 /50  265 /500\n",
      "2430 0.276518 0.6875 30 /50  265 /500\n",
      "2440 0.276237 0.695312 30 /50  265 /500\n",
      "2450 0.251349 0.773438 30 /50  265 /500\n",
      "2460 0.234579 0.78125 30 /50  265 /500\n",
      "2470 0.248261 0.75 30 /50  265 /500\n",
      "2480 0.224965 0.75 30 /50  265 /500\n",
      "2490 0.24397 0.742188 30 /50  265 /500\n",
      "2500 0.277636 0.726562 30 /50  265 /500\n",
      "2510 0.248571 0.742188 30 /50  265 /500\n",
      "2520 0.218314 0.804688 30 /50  265 /500\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    lr=0.01\n",
    "    for it in range(10000):\n",
    "        if it%500==0 :\n",
    "            lr/=10;\n",
    "        next_x1, next_x2, next_y, x1_len, x2_len = train_data_loader.next_batch(batch_size=batchSize, pad_to_length=wordLen, pad_word=word2id['<pad>'], return_len=True)\n",
    "        _y=np.zeros((batchSize,2))\n",
    "        _y[np.arange(batchSize), next_y]=1\n",
    "        #_context_lstm=sess.run(context_lstm, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len, learning_rate:lr})\n",
    "        #print(_context_lstm)\n",
    "        #_response_emb=sess.run(response_embedded, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len, learning_rate:lr})\n",
    "        #print(_response_emb)\n",
    "        sess.run(train_step, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len, learning_rate:lr})\n",
    "        ce=sess.run(cross_entropy, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len})\n",
    "        if it%10==0:\n",
    "            #mse=sess.run(mean_square_error, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len})\n",
    "            ce=sess.run(cross_entropy, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob: 0.8, context_len: x1_len, response_len:x2_len})\n",
    "            acc=compute_accuracy(next_x1, next_x2, _y, 0.8)\n",
    "            #print(mse, acc)\n",
    "            sample_acc=0\n",
    "            for i in range(50):\n",
    "                _context=[]\n",
    "                _response=[]\n",
    "                _ans=np.zeros((6,2))\n",
    "                for j in range(6):\n",
    "                    _context.append(list(sample_id1[i]))\n",
    "                    _response.append(list(sample_id2[i][j]))\n",
    "                    if j==sample_y[i]:\n",
    "                        _ans[j][1]=1.0\n",
    "                    else:\n",
    "                        _ans[j][0]=1.0\n",
    "                _context=np.array(_context)\n",
    "                _response=np.array(_response)\n",
    "                pred=sess.run(prediction, feed_dict={context: _context, response: _response, target: _ans, keep_prob: 0.8, context_len: x1_len, response_len:x2_len})\n",
    "                #print(sample_y[i])\n",
    "                #print(pred)\n",
    "                guess=np.argmax(pred, axis=0)\n",
    "                #print(guess)\n",
    "                if guess[1]==sample_y[i]:\n",
    "                    sample_acc=sample_acc+1\n",
    "            test_acc=0\n",
    "            for i in range(500):\n",
    "                _context=[]\n",
    "                _response=[]\n",
    "                _ans=np.zeros((6,2))\n",
    "                for j in range(6):\n",
    "                    _context.append(list(test_id1[i]))\n",
    "                    _response.append(list(test_id2[i][j]))\n",
    "                    if j==test_y[i]:\n",
    "                        _ans[j][1]=1.0\n",
    "                    else:\n",
    "                        _ans[j][0]=1.0\n",
    "                _context=np.array(_context)\n",
    "                _response=np.array(_response)\n",
    "                pred=sess.run(prediction, feed_dict={context: _context, response: _response, target: _ans, keep_prob: 0.8, context_len: x1_len, response_len:x2_len})\n",
    "                #print(sample_y[i])\n",
    "                #print(pred)\n",
    "                guess=np.argmax(pred, axis=0)\n",
    "                #print(guess)\n",
    "                if guess[1]==test_y[i]:\n",
    "                    test_acc=test_acc+1\n",
    "            print(it, ce, acc, sample_acc, '/50 ', test_acc, '/500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
