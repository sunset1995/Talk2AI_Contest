{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batchizer\n",
    "train 資料可能很大，跑 Mini-batch 時應該儘量不增加記憶體使用量 (用 reference 的)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(word2vec_fname, corpus_fnames, extra_words=[], unknown_word=None):\n",
    "    assert(unknown_word is None or unknown_word in extra_words)\n",
    "    \n",
    "    # Read word2vec model\n",
    "    word2vec_model = word2vec.Word2Vec.load(word2vec_fname)\n",
    "    print('vocab    size:', len(word2vec_model.wv.vocab))\n",
    "    print('embeding size:', word2vec_model.layer1_size)\n",
    "\n",
    "\n",
    "    # Extract word2vec\n",
    "    word2id = {}\n",
    "    id2word = [None] * (len(word2vec_model.wv.vocab) + len(extra_words)) \n",
    "    embedding_matrix = np.zeros([len(word2vec_model.wv.vocab) + len(extra_words), word2vec_model.layer1_size])\n",
    "    word_p = np.zeros(len(word2vec_model.wv.vocab) + len(extra_words))\n",
    "    total_word = np.sum([v.count for v in word2vec_model.wv.vocab.values()])\n",
    "\n",
    "    for i, word in enumerate(extra_words):\n",
    "        word2id[word] = i + len(word2vec_model.wv.vocab)\n",
    "        id2word[i + len(word2vec_model.wv.vocab)] = word\n",
    "\n",
    "    for k, v in word2vec_model.wv.vocab.items():\n",
    "        word2id[k] = v.index\n",
    "        id2word[v.index] = k\n",
    "        word_p[v.index] = v.count / total_word\n",
    "        embedding_matrix[v.index] = word2vec_model.wv.word_vec(k)\n",
    "    \n",
    "    del(word2vec_model)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Extract corpus\n",
    "    corpus = []\n",
    "    for fname in corpus_fnames:\n",
    "        with open(fname, 'r') as f:\n",
    "            corpus.extend([[s.split() for s in line.strip().split('\\t')] for line in f])\n",
    "    \n",
    "    def s_2_sid(s):\n",
    "        ret = []\n",
    "        for word in s:\n",
    "            if word in word2id:\n",
    "                ret.append(word2id[word])\n",
    "            else:\n",
    "                ret.append(word2id[unknown_word] if unknown_word is not None else -1)\n",
    "        return ret\n",
    "    corpus_id = [[s_2_sid(s) for s in c] for c in corpus]\n",
    "\n",
    "\n",
    "    return word2id, id2word, word_p, embedding_matrix, corpus, corpus_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab    size: 65863\n",
      "embeding size: 200\n"
     ]
    }
   ],
   "source": [
    "word2vec_fname = 'models/word2vec_all_offitial_200.model.bin'\n",
    "corpus_fnames = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "#     'datas/training_data/人生劇展.txt',\n",
    "#     'datas/training_data/公視藝文大道.txt',\n",
    "#     'datas/training_data/成語賽恩思.txt',\n",
    "#     'datas/training_data/我的這一班.txt',\n",
    "#     'datas/training_data/流言追追追.txt',\n",
    "#     'datas/training_data/聽聽看.txt',\n",
    "    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = '<pad>'\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, corpus_fnames, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65863"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料 scenario：一個問題 n 個選項"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mini_batcher_one_q_multiple_r():\n",
    "    def __init__(self, x1, x2, y):\n",
    "        '''\n",
    "        Parameters\n",
    "            x1: np.array. Containing a list of questions\n",
    "            x2: np.array. Containing options for each corresponding x1(question)\n",
    "            y : np.array. Containing a list of int which is the answer for corresponding x1(question)\n",
    "        Note I:\n",
    "            x1, x2, y inside or outside the class are referencing to same memory.\n",
    "            So do all returning result by this class.\n",
    "            This class won't (hope) do any modification on x1, x2, y\n",
    "        Note II:\n",
    "            # of batch in a epoch for sigmoid = # of options\n",
    "            # of batch in a epoch for cross entropy = # of questions\n",
    "        '''\n",
    "        if type(x1) != np.ndarray or type(x2) != np.ndarray or type(y) != np.ndarray:\n",
    "            raise AssertionError('x1, x2, y should be np.ndarray')\n",
    "        if len(x1) != len(x2) or len(x1) != len(y):\n",
    "            raise AssertionError('len(x1), len(x2), len(y) should be the same')\n",
    "        for i in range(len(x2)):\n",
    "            if len(x2[i]) != len(x2[0]):\n",
    "                raise AssertionError('Each element of x2 should be the same length')\n",
    "        self._x1 = x1\n",
    "        self._x2 = x2\n",
    "        self._y = y\n",
    "        self._sigmoid_pointer = 0\n",
    "        self._sigmoid_idx_pool = np.array([(i, j) for i in range(len(x2)) for j in range(len(x2[i]))])\n",
    "        self._entropy_pointer = 0\n",
    "        self._entropy_idx_pool = np.arange(len(x1))\n",
    "        np.random.shuffle(self._sigmoid_idx_pool)\n",
    "        np.random.shuffle(self._entropy_idx_pool)\n",
    "\n",
    "\n",
    "    def next_batch_4_sigmoid(self, batch_size):\n",
    "        f = self._sigmoid_pointer\n",
    "        t = self._sigmoid_pointer + batch_size\n",
    "        if t > len(self._sigmoid_idx_pool):\n",
    "            f = 0\n",
    "            t = batch_size\n",
    "            np.random.shuffle(self._sigmoid_idx_pool)\n",
    "        self._sigmoid_pointer = t\n",
    "        idx = self._sigmoid_idx_pool[f:t]\n",
    "        idx_0 = idx[:, 0]\n",
    "        idx_1 = idx[:, 1]\n",
    "        return self._x1[idx_0], self._x2[idx_0, idx_1], np.array(self._y[idx_0]==idx_1, dtype=np.int8)\n",
    "\n",
    "\n",
    "    def next_batch_4_cross_entropy(self, batch_size):\n",
    "        f = self._entropy_pointer\n",
    "        t = self._entropy_pointer + batch_size\n",
    "        if t > len(self._entropy_idx_pool):\n",
    "            f = 0\n",
    "            t = batch_size\n",
    "            np.random.shuffle(self._entropy_idx_pool)\n",
    "        self._entropy_pointer = t\n",
    "        idx = self._entropy_idx_pool[f:t]\n",
    "        onehot = np.zeros((len(idx), len(x2[0])))\n",
    "        onehot[np.arange(len(idx)), self._y[idx]] = 1\n",
    "        return self._x1[idx], self._x2[idx], onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Datas for Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract sample test datas\n",
    "x1 = np.array(\n",
    "    [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    ")\n",
    "x2 = np.array(\n",
    "    [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    ")\n",
    "y = np.array(sample.answer.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x1[27])\n",
    "print(x2[27])\n",
    "print(y[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid scenario\n",
    "data_loader = mini_batcher_one_q_multiple_r(x1, x2, y)\n",
    "batch_size = 5\n",
    "batch_q, batch_r, batch_ans = data_loader.next_batch_4_sigmoid(batch_size)\n",
    "for i in range(batch_size):\n",
    "    print('  Q:', batch_q[i])\n",
    "    print('  R:', batch_r[i])\n",
    "    print('ans:', batch_ans[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy scenario\n",
    "data_loader = mini_batcher_one_q_multiple_r(x1, x2, y)\n",
    "batch_size = 5\n",
    "batch_q, batch_r, batch_ans = data_loader.next_batch_4_cross_entropy(batch_size)\n",
    "for i in range(batch_size):\n",
    "    print('  Q:', batch_q[i])\n",
    "    for j in range(len(batch_r[i])):\n",
    "        print('R%2d: %s' % (j, batch_r[i][j]))\n",
    "    print('ans:', batch_ans[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料 scenario：多個不同長度的文本，轉成一筆一筆的 (上句, 下句, 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MiniBatchCorpus():\n",
    "    def __init__(self, corpus, n_wrong=1):\n",
    "        '''\n",
    "        Parameters:\n",
    "            corpus : list of corpus (2D)\n",
    "            n_wrong: int. # of wrong answer to be generated for each question.\n",
    "        Note I:\n",
    "            This class will create a flatten (1D) version of corpus for convenient.\n",
    "            But still a reference to outside corpus, changing corpus outside will \n",
    "            changing corpus inside the class also.\n",
    "        '''\n",
    "        self._corpus = np.array([s for c in corpus for s in c])\n",
    "        self._pointer = 0\n",
    "        \n",
    "        border_idx = np.cumsum([len(c) for c in corpus]) - 1\n",
    "        que_idx = np.delete(np.arange(np.sum([len(c) for c in corpus])), border_idx)\n",
    "        ans_idx = que_idx + 1\n",
    "        \n",
    "        self._dt_pool = np.vstack([\n",
    "            np.stack([que_idx, ans_idx, np.ones(len(que_idx), dtype=np.int32)], axis=1),\n",
    "            *[\n",
    "                np.stack([que_idx, self.__get_wrong_idx(ans_idx), np.zeros(len(que_idx), dtype=np.int32)], axis=1)\n",
    "                for i in range(n_wrong)\n",
    "            ]\n",
    "        ])\n",
    "        np.random.shuffle(self._dt_pool)\n",
    "        \n",
    "        self.data_num = len(self._dt_pool)\n",
    "\n",
    "\n",
    "    def __get_wrong_idx(self, ans_idx):\n",
    "        '''\n",
    "        Generate a sequence which is a shuffle version of input ans.\n",
    "        Each output elements is different from input ans.\n",
    "        '''\n",
    "        assert(len(ans_idx) > 1)\n",
    "        idx = ans_idx.copy()\n",
    "        np.random.shuffle(idx)\n",
    "        for i in np.where(idx == ans_idx)[0]:\n",
    "            if idx[i] != ans_idx[i]:\n",
    "                continue\n",
    "            t = np.random.randint(len(ans_idx))\n",
    "            while t==i or idx[i]==ans_idx[t] or idx[t]==ans_idx[i]:\n",
    "                t = np.random.randint(len(ans_idx))\n",
    "            idx[i], idx[t] = idx[t], idx[i]\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        f = self._pointer\n",
    "        t = self._pointer + batch_size\n",
    "        if t > self.data_num:\n",
    "            f = 0\n",
    "            t = batch_size\n",
    "            np.random.shuffle(self._dt_pool)\n",
    "        self._pointer = t\n",
    "        dt = self._dt_pool[f:t]\n",
    "        x1 = self._corpus[dt[:, 0]]\n",
    "        x2 = self._corpus[dt[:, 1]]\n",
    "        y = dt[:, 2]\n",
    "        return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fname = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "#     'datas/training_data/人生劇展.txt',\n",
    "#     'datas/training_data/公視藝文大道.txt',\n",
    "#     'datas/training_data/成語賽恩思.txt',\n",
    "#     'datas/training_data/我的這一班.txt',\n",
    "#     'datas/training_data/流言追追追.txt',\n",
    "#     'datas/training_data/聽聽看.txt',\n",
    "    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "\n",
    "corpus = []\n",
    "for fname in corpus_fname:\n",
    "    with open(fname, 'r') as f:\n",
    "        corpus.extend([[s.split() for s in line.split('\\t')] for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = MiniBatchCorpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "x1, x2, y = data_loader.next_batch(batch_size)\n",
    "for i in range(batch_size):\n",
    "    print('x1:', x1[i])\n",
    "    print('x2:', x2[i])\n",
    "    print('y :', y[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
