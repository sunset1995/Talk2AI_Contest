{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from /home/sunset/Talk2AI_Contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.ufb7b5ede4bbc311ed39003ae859d1289.cache\n",
      "Loading model cost 1.157 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "from mini_batch_helper import extractor, MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in  training data\n",
    "word2vec_fname = 'models/word2vec/fine-tuned-2.txt'\n",
    "corpus_fnames = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "    'datas/training_data/人生劇展.txt',\n",
    "    'datas/training_data/公視藝文大道.txt',\n",
    "    'datas/training_data/成語賽恩思.txt',\n",
    "    'datas/training_data/我的這一班.txt',\n",
    "    'datas/training_data/流言追追追.txt',\n",
    "    'datas/training_data/聽聽看.txt',\n",
    "    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "sample_rate_on_training_datas = 1.0  # 1.0\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = None\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, corpus_fnames, sample_rate_on_training_datas, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datas num: 5767155\n",
      "valid datas num: 14652\n"
     ]
    }
   ],
   "source": [
    "valid_corpus_num = 10\n",
    "\n",
    "train_data_loader = MiniBatchCorpus(corpus_id[valid_corpus_num:], context_len=3, max_len=64)\n",
    "valid_data_loader = MiniBatchCorpus(corpus_id[:valid_corpus_num], context_len=3, max_len=64)\n",
    "print('train datas num:', train_data_loader.data_num, flush=True)\n",
    "print('valid datas num:', valid_data_loader.data_num, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'SMN_debug'\n",
    "# HyperParameters\n",
    "# Model Parameters\n",
    "hp = {}\n",
    "\n",
    "hp['word2vec_model_name'] = word2vec_fname\n",
    "hp['word2vec_vocab_size'] = embedding_matrix.shape[0]\n",
    "hp['word2vec_dim'] = embedding_matrix.shape[1]\n",
    "hp['rnn_dim'] = 256  # 200\n",
    "hp['forget_bias'] = 1.0 # 0.0\n",
    "\n",
    "hp['word_len'] = 64\n",
    "hp['filter_size'] = 3\n",
    "hp['stride_size'] = 1\n",
    "hp['fm1_num'] = 4  \n",
    "hp['fm2_num'] = 8\n",
    "hp['cell_type'] = 'gru'  # 'gru' or 'lstm'\n",
    "hp['keep_prob'] = 0.8  # 0.8 , 0.5 !\n",
    "# hp['fm1_size'] = int(hp['word_len']/(2*hp['stride_size']))  # unused ?? \n",
    "# hp['fm2_size'] = int(hp['word_len']/(2*hp['stride_size'])/(2*hp['stride_size']))\n",
    "\n",
    "# Training Parameters\n",
    "hp['learning_rate'] = 1e-3\n",
    "hp['decay_learning_rate'] = 0.8\n",
    "hp['decay_times_no_improve'] = 5\n",
    "hp['clip'] = 15\n",
    "hp['batch_size'] = 256\n",
    "hp['n_iterations'] = int(20 * train_data_loader.data_num / hp['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the hyperparameters as json\n",
    "save_hp_dir = 'models/%s/' %exp_name\n",
    "newest_model_dir = save_hp_dir + 'newest/'\n",
    "best_model_dir = save_hp_dir + 'best/'\n",
    "if not os.path.exists(save_hp_dir):\n",
    "    os.makedirs(save_hp_dir)\n",
    "with open(save_hp_dir+'model_parameters.json', 'w') as f:\n",
    "    json.dump(hp, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in sample\n",
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "sample_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "sample_y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in sample_x2]) == 0)\n",
    "sample_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in sample_x1]\n",
    "sample_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in sample_x2]\n",
    "\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)\n",
    "test_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in test_x1]\n",
    "test_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in test_x2]\n",
    "with open('datas/AIFirst_test_answer.txt', 'r') as f:\n",
    "    f.readline()\n",
    "    test_y = np.array([int(line.strip().split(',')[-1]) for line in f])\n",
    "\n",
    "def word_lst_2_id_lst(lst, pad_to_len=-1):\n",
    "    pad_word_id = word2id['<pad>']\n",
    "    pad_len = max(len(lst), 0)\n",
    "    id_list = [word2id[lst[i]] if i<len(lst) and lst[i] in word2id else pad_word_id for i in range(pad_len)]\n",
    "    pad_len = pad_to_len - len(id_list)\n",
    "    if pad_len > 0:\n",
    "        id_list.extend([pad_word_id] * pad_len)\n",
    "    return id_list\n",
    "\n",
    "pad_to_length = hp['word_len']\n",
    "\n",
    "sample_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in sample_x1])\n",
    "sample_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in sample_x2])\n",
    "test_id1 = np.array([word_lst_2_id_lst(s, pad_to_length) for s in test_x1])\n",
    "test_id2 = np.array([[word_lst_2_id_lst(r, pad_to_length) for r in rs] for rs in test_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_accuracy(next_x1, next_x2, _y, _keep_prob):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={context: next_x1, response: next_x2, keep_prob:_keep_prob})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={context: next_x1, response: next_x2, target: _y, keep_prob:_keep_prob})\n",
    "    return result\n",
    " \n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_uniform(shape,-1.0,1.0)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):\n",
    "    initial = tf.random_uniform(shape,-1.0,1.0)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, hp['stride_size'], hp['stride_size'], 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Input\n",
    "context = tf.placeholder(dtype=tf.int32, shape=(None, None), name='context')\n",
    "context_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='context_len')\n",
    "response = tf.placeholder(dtype=tf.int32, shape=(None, None), name='response')\n",
    "response_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='response_len')\n",
    "target = tf.placeholder(dtype=tf.int32, shape=(None,), name='target')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding\n",
    "init_embedding_W = tf.constant_initializer(embedding_matrix)\n",
    "embeddings_W = tf.get_variable('embeddings_W', shape=[embedding_matrix.shape[0], embedding_matrix.shape[1]], initializer=init_embedding_W)\n",
    "context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "# here should pass a gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn layer\n",
    "assert(hp['cell_type'] == 'gru' or hp['cell_type'] == 'lstm')\n",
    "if hp['cell_type'] == 'gru':\n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=hp['rnn_dim'], reuse=tf.get_variable_scope().reuse)\n",
    "elif hp['cell_type'] == 'lstm':\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=hp['rnn_dim'], forget_bias=hp['forget_bias'], \n",
    "                                   use_peepholes=True, state_is_tuple=True, \n",
    "                                   reuse=tf.get_variable_scope().reuse)\n",
    "cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "context_rnn = c_outputs\n",
    "r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "response_rnn = r_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 matrix and M2 matrix\n",
    "\n",
    "# M1 word dot matrix\n",
    "word_dot_matrix = tf.matmul(context_embedded, response_embedded, False, True)\n",
    "m1_image = tf.reshape(word_dot_matrix, [-1, hp['word_len'], hp['word_len'], 1])\n",
    "m1_image = tf.divide(m1_image, 1e-9 + tf.reshape(tf.reduce_max(m1_image, axis=[1, 2]), [-1, 1, 1, 1]))\n",
    "\n",
    "# M2 segment dot matrix\n",
    "segment_dot_matrix = tf.matmul(context_rnn, response_rnn, False, True)\n",
    "m2_image = tf.reshape(segment_dot_matrix, [-1, hp['word_len'], hp['word_len'], 1])\n",
    "m2_image = tf.divide(m2_image, 1e-9 + tf.reshape(tf.reduce_max(m2_image, axis=[1, 2]), [-1, 1, 1, 1]))\n",
    "\n",
    "y_label=tf.cast(target, tf.float32)\n",
    "# M1 convolution\n",
    "W_conv1_m1 = weight_variable([hp['filter_size'], hp['filter_size'], 1, hp['fm1_num']])\n",
    "b_conv1_m1 = bias_variable([hp['fm1_num']])\n",
    "h_conv1_m1 = tf.nn.relu(conv2d(m1_image, W_conv1_m1) + b_conv1_m1)\n",
    "h_pool1_m1 = max_pool_2x2(h_conv1_m1)\n",
    "\n",
    "W_conv2_m1 = weight_variable([hp['filter_size'], hp['filter_size'], hp['fm1_num'], hp['fm2_num']])\n",
    "b_conv2_m1 = bias_variable([hp['fm2_num']])\n",
    "h_conv2_m1 = tf.nn.relu(conv2d(h_pool1_m1, W_conv2_m1) + b_conv2_m1)\n",
    "h_pool2_m1 = max_pool_2x2(h_conv2_m1)\n",
    "\n",
    "h_pool2_m1_flat = tf.contrib.layers.flatten(h_pool2_m1)\n",
    "# tf.reshape(, [-1, hp['fm2_size']*hp['fm2_size']*hp['fm2_num']])  # ??\n",
    "\n",
    "# M2 convolution\n",
    "W_conv1_m2 = weight_variable([hp['filter_size'], hp['filter_size'], 1, hp['fm1_num']])\n",
    "b_conv1_m2 = bias_variable([hp['fm1_num']])\n",
    "h_conv1_m2 = tf.nn.relu(conv2d(m2_image, W_conv1_m2) + b_conv1_m2)\n",
    "h_pool1_m2 = max_pool_2x2(h_conv1_m2)\n",
    "\n",
    "W_conv2_m2 = weight_variable([hp['filter_size'], hp['filter_size'], hp['fm1_num'], hp['fm2_num']])\n",
    "b_conv2_m2 = bias_variable([hp['fm2_num']])\n",
    "h_conv2_m2 = tf.nn.relu(conv2d(h_pool1_m2, W_conv2_m2) + b_conv2_m2)\n",
    "h_pool2_m2 = max_pool_2x2(h_conv2_m2)\n",
    "\n",
    "h_pool2_m2_flat = tf.contrib.layers.flatten(h_pool2_m2)\n",
    "# tf.reshape(h_pool2_m2, [-1, hp['fm2_size']*hp['fm2_size']*hp['fm2_num']])\n",
    "\n",
    "# Accumulate M1 and M2\n",
    "matching_accumulation = tf.add(h_pool2_m1_flat, h_pool2_m2_flat)\n",
    "\n",
    "W_fc1 = weight_variable([int(matching_accumulation.shape[1]), hp['word_len']*hp['word_len']])\n",
    "b_fc1 = bias_variable([hp['word_len']*hp['word_len']])\n",
    "h_fc1 = tf.nn.sigmoid(tf.matmul(matching_accumulation, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = weight_variable([hp['word_len']*hp['word_len'], 1])\n",
    "b_fc2 = bias_variable([1])\n",
    "logits = tf.reshape(tf.matmul(h_fc1, W_fc2) + b_fc2, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "probs = tf.sigmoid(logits)\n",
    "correct_prediction = tf.logical_or( tf.logical_and(tf.equal(target,1), tf.greater_equal(probs,0.5)), tf.logical_and(tf.equal(target,0), tf.less(probs,0.5)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Optimize\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y_label)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def problem_acc(sess, q, rs, ans):\n",
    "    p_prob = sess.run(probs, feed_dict={\n",
    "            context: np.repeat(q, 6, axis=0).reshape(-1, hp['word_len']),\n",
    "            response: rs.reshape(-1, hp['word_len']),\n",
    "            keep_prob: 1.0, context_len: x1_len, response_len: x2_len})\n",
    "    return np.sum(np.argmax(p_prob.reshape(-1, 6), axis=1) == ans) / len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_loss_accuracy(sess):\n",
    "    valid_loss = 0\n",
    "    valid_accuracy = 0\n",
    "    n_iter = int(valid_data_loader.data_num/hp['batch_size'])\n",
    "    for it in range(n_iter):\n",
    "        next_x1, next_x2, next_y, x1_len, x2_len = train_data_loader.next_batch(\n",
    "            batch_size=hp['batch_size'], pad_to_length=hp['word_len'], pad_word=word2id['<pad>'], return_len=True)\n",
    "        batch_loss, batch_acc = sess.run([cross_entropy, accuracy], feed_dict={\n",
    "            context: next_x1, response: next_x2, target: next_y,\n",
    "            keep_prob: hp['keep_prob'], context_len: x1_len, response_len:x2_len, learning_rate:lr})\n",
    "        batch_loss = np.mean(batch_loss)\n",
    "        valid_accuracy += batch_acc\n",
    "        valid_loss += batch_loss\n",
    "    valid_loss /= n_iter\n",
    "    valid_accuracy /= n_iter\n",
    "    print('Valid loss = %.5f, accuracy = %.5f' % (valid_loss, valid_accuracy), flush=True)\n",
    "    print('Sample accuracy = %.5f' % problem_acc(sess, sample_id1, sample_id2, sample_y))\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations    1:\tTrain loss = 14.98685, accuracy = 0.50781 / elapsed time 1\n",
      "Valid loss = 3.60053, accuracy = 0.56517\n",
      "Best model save in 0 iteration\n",
      "Iterations    2:\tTrain loss = 3.70348, accuracy = 0.57422 / elapsed time 26\n",
      "Valid loss = 3.68741, accuracy = 0.67756\n",
      "Iterations    3:\tTrain loss = 3.55256, accuracy = 0.67578 / elapsed time 42\n",
      "Valid loss = 6.35648, accuracy = 0.61890\n",
      "Iterations    4:\tTrain loss = 5.33244, accuracy = 0.68359 / elapsed time 57\n",
      "Valid loss = 6.78857, accuracy = 0.61047\n",
      "Iterations    5:\tTrain loss = 7.05041, accuracy = 0.61328 / elapsed time 73\n",
      "Valid loss = 5.47354, accuracy = 0.63151\n",
      "Iterations    6:\tTrain loss = 5.60493, accuracy = 0.64453 / elapsed time 89\n",
      "Valid loss = 3.61976, accuracy = 0.67544\n",
      "Learning rate decay to 0.000800\n",
      "INFO:tensorflow:Restoring parameters from models/SMN_debug/best/model.ckpt\n",
      "Iterations    7:\tTrain loss = 3.43864, accuracy = 0.57812 / elapsed time 104\n",
      "Valid loss = 2.65899, accuracy = 0.71567\n",
      "Best model save in 6 iteration\n",
      "Iterations    8:\tTrain loss = 2.53412, accuracy = 0.75000 / elapsed time 123\n",
      "Valid loss = 5.13765, accuracy = 0.64069\n",
      "Iterations    9:\tTrain loss = 4.81180, accuracy = 0.64062 / elapsed time 139\n",
      "Valid loss = 5.47182, accuracy = 0.64570\n",
      "Iterations   10:\tTrain loss = 6.53838, accuracy = 0.57812 / elapsed time 155\n",
      "Valid loss = 4.83828, accuracy = 0.64563\n",
      "Iterations   11:\tTrain loss = 6.11145, accuracy = 0.57812 / elapsed time 170\n",
      "Valid loss = 3.47095, accuracy = 0.67791\n",
      "Iterations   12:\tTrain loss = 3.65314, accuracy = 0.67578 / elapsed time 186\n",
      "Valid loss = 2.25844, accuracy = 0.71587\n",
      "Best model save in 11 iteration\n",
      "Iterations   13:\tTrain loss = 2.21832, accuracy = 0.73047 / elapsed time 205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-31c844923955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Save the model if has smaller loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcurrent_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_loss_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_valid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbest_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_valid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5a02357d1eaa>\u001b[0m in \u001b[0;36mget_valid_loss_accuracy\u001b[0;34m(sess)\u001b[0m\n\u001b[1;32m      8\u001b[0m         batch_loss, batch_acc = sess.run([cross_entropy, accuracy], feed_dict={\n\u001b[1;32m      9\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             keep_prob: hp['keep_prob'], context_len: x1_len, response_len:x2_len, learning_rate:lr})\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalid_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = hp['learning_rate']\n",
    "decay_times_no_improve = hp['decay_times_no_improve']\n",
    "best_valid_loss = 1e9\n",
    "for it in range(hp['n_iterations']):\n",
    "    print('Iterations %4d:\\t' % (it+1) , end='')\n",
    "    next_x1, next_x2, next_y, x1_len, x2_len = train_data_loader.next_batch(\n",
    "        batch_size=hp['batch_size'], pad_to_length=hp['word_len'], pad_word=word2id['<pad>'], return_len=True)\n",
    "    batch_loss, batch_acc, _ = sess.run([cross_entropy, accuracy, train_step], feed_dict={\n",
    "        context: next_x1, response: next_x2, target: next_y,\n",
    "        keep_prob: hp['keep_prob'], context_len: x1_len, response_len: x2_len, learning_rate: lr})\n",
    "    batch_loss = np.mean(batch_loss)\n",
    "    print('Train loss = %.5f, accuracy = %.5f / elapsed time %.f' % (batch_loss, batch_acc, time.time() - start_time), flush=True)\n",
    "    if it % 1000 == 0:\n",
    "        # Save the model if has smaller loss\n",
    "        current_valid_loss = get_valid_loss_accuracy(sess)\n",
    "        if current_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = current_valid_loss\n",
    "            if not os.path.exists(best_model_dir):\n",
    "                os.makedirs(best_model_dir)\n",
    "            save_path = saver.save(sess, best_model_dir+'model.ckpt')\n",
    "            print('Best model save in %d iteration' % (it+1), flush=True)\n",
    "\n",
    "        # Decay the learning rate if no improve for 3 times\n",
    "        if hp['decay_learning_rate'] < 1:\n",
    "            if current_valid_loss > best_valid_loss:\n",
    "                times_no_improve += 1\n",
    "            else:\n",
    "                times_no_improve = 0\n",
    "                decay_times_no_improve = max(hp['decay_times_no_improve'], decay_times_no_improve-1)\n",
    "            if times_no_improve >= decay_times_no_improve:\n",
    "                # Decay learning rate\n",
    "                times_no_improve = 0\n",
    "                decay_times_no_improve *= 2\n",
    "                lr *= hp['decay_learning_rate']\n",
    "                print('Learning rate decay to %f' % lr, flush=True)\n",
    "                # Restrore to the best model\n",
    "                saver.restore(sess, best_model_dir+'model.ckpt')\n",
    "                # Stop if lr is too small\n",
    "                if lr < 1e-9:\n",
    "                    print('Current_learning_rate is smaller than 1e-9. Stop.', flush=True)\n",
    "                    break\n",
    "    if it % 100 == 0:\n",
    "        if not os.path.exists(newest_model_dir):\n",
    "            os.makedirs(newest_model_dir)\n",
    "        save_path = saver.save(sess, newest_model_dir+'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
