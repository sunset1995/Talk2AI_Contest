{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Self define module\n",
    "from mini_batch_helper import extractor\n",
    "from mini_batch_helper import MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading corpus and forming dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2vec_fname = 'models/word2vec/vec512_win15_iter15_mincnt5.bin'\n",
    "corpus_fnames = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "    'datas/training_data/人生劇展.txt',\n",
    "    'datas/training_data/公視藝文大道.txt',\n",
    "    'datas/training_data/成語賽恩思.txt',\n",
    "    'datas/training_data/我的這一班.txt',\n",
    "    'datas/training_data/流言追追追.txt',\n",
    "    'datas/training_data/聽聽看.txt',\n",
    "    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "sample_rate_on_training_datas = 1\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = None\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, corpus_fnames, sample_rate_on_training_datas, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            voc_size: 65865\n",
      "            emb_size: 512\n",
      "         pad_word_id: 65864\n",
      "         max_seq_len: 76\n",
      "train datas num: 2859792\n",
      "valid datas num: 16526\n"
     ]
    }
   ],
   "source": [
    "voc_size = embedding_matrix.shape[0]\n",
    "emb_size = embedding_matrix.shape[1]\n",
    "pad_word_id = word2id['<pad>']\n",
    "max_seq_len = np.max([len(s) for cp in corpus_id for s in cp])\n",
    "\n",
    "print('%20s: %d' % ('voc_size', voc_size))\n",
    "print('%20s: %d' % ('emb_size', emb_size))\n",
    "print('%20s: %d' % ('pad_word_id', pad_word_id))\n",
    "print('%20s: %d' % ('max_seq_len', max_seq_len))\n",
    "\n",
    "# Data split\n",
    "rnd_idx = np.arange(len(corpus_id))\n",
    "np.random.shuffle(rnd_idx)\n",
    "corpus_id = corpus_id[rnd_idx[:len(corpus_id)//2]]\n",
    "valid_corpus_num = 10\n",
    "\n",
    "train_data_loader = MiniBatchCorpus(corpus_id[valid_corpus_num:])\n",
    "valid_data_loader = MiniBatchCorpus(corpus_id[:valid_corpus_num])\n",
    "print('train datas num:', train_data_loader.data_num)\n",
    "print('valid datas num:', valid_data_loader.data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Word embedding model\n",
    "embeddings_W = tf.Variable(embedding_matrix)\n",
    "\n",
    "# Input\n",
    "x1 = tf.placeholder(tf.int32, [None, None])\n",
    "x2 = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "lr = tf.placeholder(tf.float64)\n",
    "\n",
    "# Sentence embedding\n",
    "x1_mask = tf.to_double(tf.not_equal(x1, pad_word_id))\n",
    "x2_mask = tf.to_double(tf.not_equal(x2, pad_word_id))\n",
    "x1_len = tf.reduce_sum(x1_mask, axis=1)\n",
    "x2_len = tf.reduce_sum(x2_mask, axis=1)\n",
    "x1_embedded = tf.gather(embeddings_W, x1) * tf.reshape(x1_mask, [-1, tf.shape(x1)[1], 1])\n",
    "x2_embedded = tf.gather(embeddings_W, x2) * tf.reshape(x2_mask, [-1, tf.shape(x2)[1], 1])\n",
    "x1_center = tf.reduce_sum(x1_embedded, axis=1) / tf.reshape(tf.to_double(x1_len)+1e-6, [-1, 1])\n",
    "x2_center = tf.reduce_sum(x2_embedded, axis=1) / tf.reshape(tf.to_double(x2_len)+1e-6, [-1, 1])\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([emb_size, emb_size], stddev=0.01, dtype=tf.float64))\n",
    "tf_score = tf.reduce_sum((x2_center * (x1_center @ W)), axis=1)\n",
    "\n",
    "tf_prob = tf.sigmoid(tf_score)\n",
    "tf_guess = tf.cast(tf.greater_equal(tf_prob, 0.5), tf.int32)\n",
    "tf_correct = tf.reduce_sum(tf.cast(tf.equal(y, tf_guess), tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Export embedding layer for naive method to use\n",
    "# saver = tf.train.Saver()\n",
    "# sess = tf.Session()\n",
    "# saver.restore(sess, './models/Attack-sentence-embedding-best/final')\n",
    "# now_emb_w = sess.run(embeddings_W)\n",
    "# with open('./models/word2vec/fine-tuned.txt', 'w') as f:\n",
    "#     assert(len(id2word) == now_emb_w.shape[0])\n",
    "#     f.write('%d %d\\n' % (voc_size, emb_size))\n",
    "#     for word, vec in zip(id2word, now_emb_w):\n",
    "#         f.write('%s %s\\n' % (word, ' '.join([str(f) for f in vec])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----------------------------------\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reg = tf.nn.l2_loss(W) * 0\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float64), logits=tf_score))\n",
    "cost_reg = cost + reg\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "gvs = optimizer.compute_gradients(cost_reg)\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 20), var) for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_valid_loss():\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    valid_batch = 2048\n",
    "    num = [0, 0]\n",
    "    correct = [0, 0]\n",
    "    batch_num = valid_data_loader.data_num // valid_batch\n",
    "    for i in range(batch_num):\n",
    "        b_x1, b_x2, b_y = valid_data_loader.next_batch(valid_batch, max_seq_len, pad_word_id)\n",
    "        now_loss, now_correct, now_guess = sess.run([cost, tf_correct, tf_guess], {x1: b_x1, x2: b_x2, y: b_y})\n",
    "        assert(now_correct == np.sum(now_guess == b_y))\n",
    "        valid_loss += now_loss / batch_num\n",
    "        valid_acc += now_correct / (batch_num * valid_batch)\n",
    "        num[0] += np.sum(b_y == 0)\n",
    "        num[1] += np.sum(b_y == 1)\n",
    "        correct[0] += np.sum((b_y == 0) & (now_guess == b_y))\n",
    "        correct[1] += np.sum((b_y == 1) & (now_guess == b_y))\n",
    "    recall_0 = correct[0] / num[0] if num[0] else 0\n",
    "    recall_1 = correct[1] / num[1] if num[1] else 0\n",
    "    return valid_loss, valid_acc, recall_0, recall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0\n",
      "train batch loss 0.956859 / valid loss 0.938181 / valid acc 0.584900 / recall_0 0.462712 / recall_1 0.707118 / elapsed time 42\n",
      "model saved (best)\n",
      "train batch loss 0.980248 / valid loss 0.936255 / valid acc 0.584106 / recall_0 0.597656 / recall_1 0.570557 / elapsed time 90\n",
      "Decay learing rate ->   0.000990\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2228b0ba991e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_batch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnow_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_valid_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         print('train batch loss %8f / valid loss %8f / valid acc %8f / recall_0 %8f / recall_1 %8f / elapsed time %.f' % (\n\u001b[1;32m     24\u001b[0m             train_batch_loss, valid_loss, valid_acc, recall_0, recall_1, time.time()-start_time), flush=True)\n",
      "\u001b[0;32m<ipython-input-7-326212d06a0f>\u001b[0m in \u001b[0;36meval_valid_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mb_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_word_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnow_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_guess\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_correct\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_guess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnow_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 256\n",
    "epoch_num = 40\n",
    "log_interval = 1\n",
    "save_interval = 10000\n",
    "\n",
    "last_epoch = None\n",
    "train_batch_loss = 0\n",
    "start_time = time.time()\n",
    "best_acc = None\n",
    "for i_batch in range(epoch_num * train_data_loader.data_num // batch_size):\n",
    "    epoch = i_batch // (train_data_loader.data_num // batch_size)\n",
    "    if last_epoch is None or last_epoch != epoch:\n",
    "        last_epoch = epoch\n",
    "        print('Start epoch %d' % (epoch))\n",
    "\n",
    "    epoch = i_batch // (train_data_loader.data_num // batch_size)\n",
    "    b_x1, b_x2, b_y = train_data_loader.next_batch(batch_size, max_seq_len, pad_word_id)\n",
    "    _, now_loss = sess.run([train_step, cost], {x1: b_x1, x2: b_x2, y: b_y, lr: learning_rate})\n",
    "    train_batch_loss += now_loss / log_interval\n",
    "    if (i_batch+1) % log_interval == 0:\n",
    "        valid_loss, valid_acc, recall_0, recall_1 = eval_valid_loss()\n",
    "        print('train batch loss %8f / valid loss %8f / valid acc %8f / recall_0 %8f / recall_1 %8f / elapsed time %.f' % (\n",
    "            train_batch_loss, valid_loss, valid_acc, recall_0, recall_1, time.time()-start_time), flush=True)\n",
    "        train_batch_loss = 0\n",
    "        if best_acc is None or best_acc < valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            print('model saved (best)', flush=True)\n",
    "            saver.save(sess, 'models/Attack-sentence-embedding-512/best')\n",
    "        else:\n",
    "            learning_rate /= 1.01\n",
    "            print('Decay learing rate -> %10f' % (learning_rate))\n",
    "    if save_interval is not None and (i_batch+1) % save_interval == 0:\n",
    "        saver.save(sess, 'models/Attack-sentence-embedding-512/latest')\n",
    "        print('model saved (latest)', flush=True)\n",
    "\n",
    "saver.save(sess, 'models/Attack-sentence-embedding-512/final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "------------------------------\n",
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eval_max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "\n",
    "sample_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "sample_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "sample_y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in sample_x2]) == 0)\n",
    "\n",
    "sample_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in sample_x1]\n",
    "sample_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in sample_x2]\n",
    "\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)\n",
    "\n",
    "test_x1 = [[word for word in jieba.cut(' '.join(s)) if word != ' '] for s in test_x1]\n",
    "test_x2 = [[[word for word in jieba.cut(r) if word != ' '] for r in rs] for rs in test_x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_lst_2_id_lst(lst):\n",
    "    return [word2id[lst[i]] if i<len(lst) and lst[i] in word2id else pad_word_id for i in range(eval_max_seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_id1 = np.array([word_lst_2_id_lst(s) for s in sample_x1])\n",
    "sample_id2 = np.array([[word_lst_2_id_lst(r) for r in rs] for rs in sample_x2])\n",
    "test_id1 = np.array([word_lst_2_id_lst(s) for s in test_x1])\n",
    "test_id2 = np.array([[word_lst_2_id_lst(r) for r in rs] for rs in test_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, 'models/Attack-sentence-embedding-previous/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate sample answer\n",
    "probs = sess.run(tf_prob, {\n",
    "    x1: np.repeat(sample_id1, 6, axis=0),\n",
    "    x2: sample_id2.reshape(-1, eval_max_seq_len),\n",
    "})\n",
    "probs = probs.reshape(-1, 6)\n",
    "probs_ans = np.argmax(probs, axis=1)\n",
    "np.sum(probs_ans == sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate test answer\n",
    "probs = sess.run(tf_prob, {\n",
    "    x1: np.repeat(test_id1, 6, axis=0),\n",
    "    x2: test_id2.reshape(-1, eval_max_seq_len),\n",
    "})\n",
    "probs = probs.reshape(-1, 6)\n",
    "my_test_ans = np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# with open('answer/attack-sentence-embedding.txt', 'w') as f:\n",
    "#     f.write('id,ans\\n')\n",
    "#     f.write('\\n'.join(['%d,%d' % (i+1, a) for i, a in enumerate(my_test_ans)]))\n",
    "#     f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
