{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import & Init matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Self define module\n",
    "from mini_batch_helper import MiniBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract sample test datas\n",
    "x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "\n",
    "# Tokenize\n",
    "x1 = [list(jieba.cut(' '.join(_))) for _ in x1]\n",
    "x2 = [[list(jieba.cut(s)) for s in _] for _ in x2]\n",
    "y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in x2]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPT Gossiping QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Extract PPT Gossiping QA datas\n",
    "# with open('datas/raw/Gossiping-QA-Dataset.txt', 'r') as fi:\n",
    "#     gossip_lines = [line.strip().split('\\t') for line in fi]\n",
    "# gossip_x1 = [list(jieba.cut(line[0])) for line in gossip_lines if len(line) == 2]\n",
    "# gossip_x2 = [list(jieba.cut(line[1])) for line in gossip_lines if len(line) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del(sample)\n",
    "# del(gossip_lines)\n",
    "# time.sleep(1)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract first principle component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec_model = word2vec.Word2Vec.load('models/word2vec_250.model.bin')\n",
    "# total_word_cnt = np.sum([_.count for _ in word2vec_model.wv.vocab.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def weighted_centroid(sentence, a=0.0001):\n",
    "#     _ = [a / (a + word2vec_model.wv.vocab[word].count / total_word_cnt) * word2vec_model.wv.word_vec(word)\n",
    "#             for word in sentence if word in word2vec_model.wv.vocab]\n",
    "#     return np.mean(_, axis=0) if len(_) > 0 else np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vs = np.concatenate([\n",
    "#     np.array([weighted_centroid(s) for s in x1]),\n",
    "#     np.array([weighted_centroid(s) for opts in x2 for s in opts]),\n",
    "#     np.array([weighted_centroid(s) for s in test_x1]),\n",
    "#     np.array([weighted_centroid(s) for opts in test_x2 for s in opts]),\n",
    "#     np.array([weighted_centroid(s) for s in gossip_x1]),\n",
    "#     np.array([weighted_centroid(s) for s in gossip_x2]),\n",
    "# ])\n",
    "# assert(len(vs) == len(x1) + len(x2) * len(x2[0]) + len(test_x1) + len(test_x2) * len(test_x2[0]) + len(gossip_x1) + len(gossip_x2))\n",
    "# vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Release unused memory comsumed model\n",
    "# del(word2vec_model)\n",
    "# time.sleep(1)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Compute first principle component\n",
    "# eig_vals, eig_vecs = np.linalg.eig(np.corrcoef(vs.T))\n",
    "# eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "# eig_pairs.sort(reverse=True)\n",
    "# u = eig_pairs[0][1].copy()\n",
    "\n",
    "# # Release unused memory\n",
    "# del(eig_vals)\n",
    "# del(eig_vecs)\n",
    "# del(eig_pairs)\n",
    "# time.sleep(1)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common component removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(vs)):\n",
    "#     vs[i] = vs[i] - u * (vs[i] @ u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# _ = np.cumsum([len(x1), len(x2) * len(x2[0]), len(test_x1), len(test_x2) * len(test_x2[0]), len(gossip_x1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# _ = np.cumsum([len(x1), len(x2) * len(x2[0]), len(test_x1), len(test_x2) * len(test_x2[0]), len(gossip_x1)])\n",
    "# sample_vs1, sample_vs2, test_vs1, test_vs2, gossip_vs1, gossip_vs2 = np.split(vs, _)\n",
    "# del(vs)\n",
    "# time.sleep(1)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('datas/sentence_embedding/first_principle_component', u)\n",
    "# np.save('datas/sentence_embedding/sample_vs1_250', sample_vs1)\n",
    "# np.save('datas/sentence_embedding/sample_vs2_250', sample_vs2)\n",
    "# np.save('datas/sentence_embedding/test_vs1_250', test_vs1)\n",
    "# np.save('datas/sentence_embedding/test_vs2_250', test_vs2)\n",
    "# np.save('datas/sentence_embedding/gossip_vs1_250', gossip_vs1)\n",
    "# np.save('datas/sentence_embedding/gossip_vs2_250', gossip_vs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wrong_idx(n):\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    for i in np.where(idx == np.arange(n))[0]:\n",
    "        if idx[i] != i:\n",
    "            continue\n",
    "        t = np.random.randint(n)\n",
    "        while t==i or t==idx[i]:\n",
    "            t = np.random.randint(n)\n",
    "        idx[i], idx[t] = idx[t], idx[i]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read embedding feature\n",
    "test_x1 = np.load('datas/sentence_embedding/sample_vs1.npy')\n",
    "test_x2 = np.load('datas/sentence_embedding/sample_vs2.npy')\n",
    "train_x1 = np.load('datas/sentence_embedding/gossip_vs1.npy')\n",
    "train_x2 = np.load('datas/sentence_embedding/gossip_vs2.npy')\n",
    "\n",
    "# Processed\n",
    "test_x1 = test_x1[np.repeat(np.arange(len(test_x1)), 6)]\n",
    "train_x2 = np.stack([\n",
    "    train_x2,\n",
    "    train_x2[get_wrong_idx(train_x2.shape[0])],\n",
    "    train_x2[get_wrong_idx(train_x2.shape[0])],\n",
    "    train_x2[get_wrong_idx(train_x2.shape[0])],\n",
    "], axis=1)\n",
    "\n",
    "test_x1.shape, test_x2.shape, train_x1.shape, train_x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Model\n",
    "vec_sz = train_x1.shape[-1]\n",
    "tf_x1 = tf.placeholder(tf.float32, [None, vec_sz])\n",
    "tf_x2 = tf.placeholder(tf.float32, [None, vec_sz])\n",
    "tf_y = tf.placeholder(tf.float32, [None])\n",
    "tf_W = tf.Variable(tf.truncated_normal([vec_sz, vec_sz]))\n",
    "tf_y_ = tf.reduce_sum(tf_x1 * (tf_x2 @ tf_W), axis=1)\n",
    "\n",
    "# tf Objective\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=tf_y_) + 5e-3 * tf.nn.l2_loss(tf_W)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# tf Session & Saver\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train loop hyperparameters\n",
    "batch_size = 2777\n",
    "batch_num = np.prod(train_x2.shape[:-1]) // batch_size\n",
    "epoch_num = 30\n",
    "data_loader = MiniBatch(train_x1, train_x2, np.zeros(train_x2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_acc_lst = []\n",
    "test_acc_lst = []\n",
    "for ith_epoch in range(epoch_num):\n",
    "    train_correct = 0\n",
    "    for ith_batch in range(batch_num):\n",
    "        b_x1, b_x2, b_y = data_loader.next_batch_4_sigmoid(batch_size)\n",
    "        _, b_y_ = sess.run([optimizer, tf_y_], {tf_x1: b_x1, tf_x2: b_x2, tf_y: b_y})\n",
    "        train_correct += np.sum((b_y == 0) * (b_y_ < 0) + (b_y == 1) * (b_y_ > 0))\n",
    "    train_acc_lst.append(train_correct / (batch_num * batch_size))\n",
    "    \n",
    "    # Evalutae accuracy after epoch\n",
    "    test_y_ = sess.run(tf_y_, {tf_x1: test_x1, tf_x2: test_x2})\n",
    "    test_correct = np.sum(np.argmax(test_y_.reshape(-1, 6), axis=1) == y)\n",
    "    test_acc_lst.append(test_correct / (len(test_y_) / 6))\n",
    "    \n",
    "    print('epoch %3d: train acc %.4f / test acc %.4f / elapsed time %10.f' % (\n",
    "        ith_epoch, train_acc_lst[-1], test_acc_lst[-1], time.time() - start_time\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot train cost in each epoch\n",
    "fig, ax = plt.subplots(1, figsize=(15, 6))\n",
    "ax.set_title('loss')\n",
    "ax.set_xlim(0, len(train_acc_lst))\n",
    "ax.scatter(x=list(range(len(train_acc_lst))), y=train_acc_lst, marker='.', color='blue')\n",
    "ax.set_title('accuracy')\n",
    "ax.set_xlim(0, len(test_acc_lst))\n",
    "ax.scatter(x=list(range(len(test_acc_lst))), y=test_acc_lst, marker='.', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
