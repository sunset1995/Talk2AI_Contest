{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from /home/sunset/word_contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u849ecfdca27003d306f39ca004b82b5b.cache\n",
      "Loading model cost 1.157 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import & Init matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract sample test datas\n",
    "x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "\n",
    "# Tokenize\n",
    "x1 = [list(jieba.cut(' '.join(_))) for _ in x1]\n",
    "x2 = [[list(jieba.cut(s)) for s in _] for _ in x2]\n",
    "y = sample.answer.values\n",
    "assert(np.sum([len(_)!=6 for _ in x2]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPT Gossiping QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract PPT Gossiping QA datas\n",
    "with open('datas/raw/Gossiping-QA-Dataset.txt', 'r') as fi:\n",
    "    gossip_lines = [line.strip().split('\\t') for line in fi]\n",
    "gossip_x1 = [list(jieba.cut(line[0])) for line in gossip_lines if len(line) == 2]\n",
    "gossip_x2 = [list(jieba.cut(line[1])) for line in gossip_lines if len(line) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(sample)\n",
    "del(gossip_lines)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract first principle component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.Word2Vec.load('models/word2vec_250.model.bin')\n",
    "total_word_cnt = np.sum([_.count for _ in word2vec_model.wv.vocab.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_centroid(sentence, a=0.0001):\n",
    "    _ = [a / (a + word2vec_model.wv.vocab[word].count / total_word_cnt) * word2vec_model.wv.word_vec(word)\n",
    "            for word in sentence if word in word2vec_model.wv.vocab]\n",
    "    return np.mean(_, axis=0) if len(_) > 0 else np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs = np.concatenate([\n",
    "    np.array([weighted_centroid(s) for s in x1]),\n",
    "    np.array([weighted_centroid(s) for opts in x2 for s in opts]),\n",
    "    np.array([weighted_centroid(s) for s in gossip_x1]),\n",
    "    np.array([weighted_centroid(s) for s in gossip_x2])\n",
    "])\n",
    "assert(len(vs) == len(x1) + len(x2) * len(x2[0]) + len(gossip_x1) + len(gossip_x2))\n",
    "vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Release unused memory comsumed model\n",
    "del(word2vec_model)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute first principle component\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.corrcoef(vs.T))\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(reverse=True)\n",
    "u = eig_pairs[0][1].copy()\n",
    "\n",
    "# Release unused memory\n",
    "del(eig_vals)\n",
    "del(eig_vecs)\n",
    "del(eig_pairs)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common component removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(vs)):\n",
    "    vs[i] = vs[i] - u * (vs[i] @ u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = np.cumsum([len(x1), len(x2) * len(x2[0]), len(gossip_x1)])\n",
    "sample_vs1, sample_vs2, gossip_vs1, gossip_vs2 = np.split(vs, _)\n",
    "del(vs)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('datas/sentence_embedding/sample_vs1', sample_vs1)\n",
    "# np.save('datas/sentence_embedding/sample_vs2', sample_vs2)\n",
    "# np.save('datas/sentence_embedding/gossip_vs1', gossip_vs1)\n",
    "# np.save('datas/sentence_embedding/gossip_vs2', gossip_vs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_vs1 = np.load('datas/sentence_embedding/sample_vs1.npy')\n",
    "sample_vs2 = np.load('datas/sentence_embedding/sample_vs2.npy')\n",
    "gossip_vs1 = np.load('datas/sentence_embedding/gossip_vs1.npy')\n",
    "gossip_vs2 = np.load('datas/sentence_embedding/gossip_vs2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Scheme\n",
    "### Train using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define mini-batch data loader\n",
    "class mini_batcher():\n",
    "    def __init__(self, x1, x2, y, batch_size):\n",
    "        self._x1 = np.array(x1)\n",
    "        self._x2 = np.array(x2)\n",
    "        self._y = np.array(y)\n",
    "        self._batch_size = batch_size\n",
    "        self._datas_num = len(x1)\n",
    "        self._pointer = 0\n",
    "        assert(self._batch_size <= self._datas_num)\n",
    "        self._idx = np.arange(len(x1))\n",
    "        np.random.shuffle(self._idx)\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        f = self._pointer\n",
    "        t = self._pointer + self._batch_size\n",
    "        if t > self._datas_num:\n",
    "            f = 0\n",
    "            t = self._batch_size\n",
    "            np.random.shuffle(self._idx)\n",
    "        self._pointer = t\n",
    "        idx = self._idx[f:t]\n",
    "        return self._x1[idx], self._x2[idx], self._y[idx]\n",
    "    \n",
    "    def next_batch_4_sigmoid(self, shuffle_batch=True):\n",
    "        b_x1, b_x2, b_y = self._next_batch()\n",
    "\n",
    "        # Expand to (question, one_option, 0/1)\n",
    "        dt = np.array([\n",
    "            (b_x1[i], b_x2[i][j], float(b_y[i] == j))\n",
    "            for i in range(len(b_x2)) for j in range(len(b_x2[i]))\n",
    "        ])\n",
    "        b_idx = np.arange(len(dt))\n",
    "        if shuffle_batch:\n",
    "            np.random.shuffle(b_idx)\n",
    "        b_x1 = np.array([_[0] for _ in dt[b_idx]])\n",
    "        b_x2 = np.array([_[1] for _ in dt[b_idx]])\n",
    "        b_y = np.array([_[2] for _ in dt[b_idx]])\n",
    "        \n",
    "        return b_x1, b_x2, b_y\n",
    "    \n",
    "    def all_4_sigmoid_evaluation(self):\n",
    "        dt = np.array([\n",
    "            (self._x1[i], self._x2[i][j], float(self._y[i] == j))\n",
    "            for i in range(len(self._x2)) for j in range(len(self._x2[i]))\n",
    "        ])\n",
    "        all_x1 = np.array([_[0] for _ in dt])\n",
    "        all_x2 = np.array([_[1] for _ in dt])\n",
    "        all_y = np.array([_[2] for _ in dt])\n",
    "        return all_x1, all_x2, all_y\n",
    "\n",
    "    def next_batch_4_cross_entropy(self):\n",
    "        raise NotImplementedError('Not yet OwOb')\n",
    "        \n",
    "    def all_4_cross_entropy_evaluation(self):\n",
    "        raise NotImplementedError('Not yet OwOb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Model\n",
    "vec_sz = f1.shape[-1]\n",
    "tf_x1 = tf.placeholder(tf.float32, [None, vec_sz])\n",
    "tf_x2 = tf.placeholder(tf.float32, [None, vec_sz])\n",
    "tf_y = tf.placeholder(tf.float32, [None])\n",
    "tf_W = tf.Variable(tf.truncated_normal([vec_sz, vec_sz]))\n",
    "tf_y_ = tf.reduce_sum(tf_x1 * (tf_x2 @ tf_W), axis=1)\n",
    "\n",
    "# tf Objective\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=tf_y_) + 1e-3 * tf.nn.l2_loss(tf_W)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "# tf Session & Saver\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train loop hyperparameters\n",
    "batch_size = 10\n",
    "batch_num = len(y) // batch_size\n",
    "epoch_num = 500\n",
    "data_loader = mini_batcher(f1, f2, y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_cost_lst = []\n",
    "train_acc_lst = []\n",
    "for ith_epoch in range(epoch_num):\n",
    "    epoch_cost = 0\n",
    "    for ith_batch in range(batch_num):\n",
    "        b_x1, b_x2, b_y = data_loader.next_batch_4_sigmoid()\n",
    "        _, b_cost = sess.run([optimizer, cost], {tf_x1: b_x1, tf_x2: b_x2, tf_y: b_y})\n",
    "        b_cost = np.mean(b_cost)\n",
    "        epoch_cost += b_cost / batch_num\n",
    "    train_cost_lst.append(epoch_cost)\n",
    "    \n",
    "    # Evalutae accuracy after epoch\n",
    "    all_x1, all_x2, all_y = data_loader.all_4_sigmoid_evaluation()\n",
    "    all_y_ = sess.run(tf_y_, {tf_x1: all_x1, tf_x2: all_x2})\n",
    "    correct = np.sum(np.argmax(all_y.reshape(-1, 6), axis=1) == np.argmax(all_y_.reshape(-1, 6), axis=1))\n",
    "    train_acc_lst.append(correct / all_y.reshape(-1, 6).shape[0])\n",
    "    \n",
    "    print('epoch cost: %10f / elapsed time %10.2f' % (epoch_cost, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot train cost in each epoch\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 6))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].set_xlim(0, len(train_cost_lst))\n",
    "ax[0].scatter(x=list(range(len(train_cost_lst))), y=train_cost_lst, marker='.', color='blue')\n",
    "ax[1].set_title('accuracy')\n",
    "ax[1].set_xlim(0, len(train_cost_lst))\n",
    "ax[1].scatter(x=list(range(len(train_acc_lst))), y=train_acc_lst, marker='.', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gogo_fold(train_f1, train_f2, train_y, test_f1, test_f2, test_y, verbose=1):\n",
    "    train_loader = mini_batcher(train_f1, train_f2, train_y, batch_size)\n",
    "    test_loader = mini_batcher(test_f1, test_f2, test_y, batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def evaluate(data_loader):\n",
    "        # Evalutae train accuracy\n",
    "        all_x1, all_x2, all_y = data_loader.all_4_sigmoid_evaluation()\n",
    "        all_y_ = sess.run(tf_y_, {tf_x1: all_x1, tf_x2: all_x2})\n",
    "        correct = np.sum(np.argmax(all_y.reshape(-1, 6), axis=1) == np.argmax(all_y_.reshape(-1, 6), axis=1))\n",
    "        return correct / all_y.reshape(-1, 6).shape[0]\n",
    "    \n",
    "    train_acc_lst = []\n",
    "    test_acc_lst = []\n",
    "    for ith_epoch in range(epoch_num):\n",
    "        epoch_cost = 0\n",
    "        for ith_batch in range(batch_num):\n",
    "            b_x1, b_x2, b_y = train_loader.next_batch_4_sigmoid()\n",
    "            sess.run(optimizer, {tf_x1: b_x1, tf_x2: b_x2, tf_y: b_y})\n",
    "        \n",
    "        train_acc = evaluate(train_loader)\n",
    "        test_acc = evaluate(test_loader)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        test_acc_lst.append(test_acc)\n",
    "        if verbose:\n",
    "            print('epoch %3d: train acc %10f / test acc %10f' % (ith_epoch, train_acc, test_acc))\n",
    "    return train_acc_lst, test_acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_idx, test_idx in cv.split(np.arange(len(y))):\n",
    "    train_lst, test_lst = gogo_fold(\n",
    "        f1[train_idx], f2[train_idx], y[train_idx], f1[test_idx], f2[test_idx], y[test_idx], verbose=False\n",
    "    )\n",
    "    print('train acc %10f / test acc %10f' % (train_lst[-1], test_lst[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
