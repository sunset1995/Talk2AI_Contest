{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual LSTM with official sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def release_variable(data_to_release):\n",
    "    del(data_to_release)\n",
    "    time.sleep(1)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "* [keras 使用 word2vec 當 embedding 的教學](http://ben.bolte.cc/blog/2016/gensim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word2vec_path = 'models/word2vec_all_offitial_200.model.bin'\n",
    "\n",
    "from gensim.models import word2vec\n",
    "word2vec_model = word2vec.Word2Vec.load(load_word2vec_path)\n",
    "word2vec_dim = word2vec_model.vector_size \n",
    "word2vec_vocab_size = len(word2vec_model.wv.vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將 word2vec_model 的 vocab 存成 dictionary 'word2id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab\n",
    "# Sequences have various lengths, so let index '0' serve as padding  -> all index+1\n",
    "word2id = dict([(k, v.index+1) for k, v in word2vec_model.wv.vocab.items()])\n",
    "print('given word string, find index in word2vec_model vocab... 然後 -> ', word2id['然後'])\n",
    "id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "print('given index, find word string in word2vec_model vocab... 95 -> ', id2word[95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release unused memory comsumed model\n",
    "import time\n",
    "import gc\n",
    "del(word2vec_model)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in official sample data ( for validation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample data for validation\n",
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "\n",
    "# Extract sample test datas\n",
    "x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "y = sample.answer.values\n",
    "\n",
    "# Tokenize\n",
    "x1 = np.array([list(jieba.cut(' '.join(_))) for _ in x1])\n",
    "x2 = np.array([[list(jieba.cut(s)) for s in _] for _ in x2])\n",
    "assert(np.sum([len(_)!=6 for _ in x2]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in test data ( for output answer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Offitial testing data\n",
    "test_datas = pd.read_csv('datas/AIFirstProblem.txt')\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "assert(np.sum([len(_)!=6 for _ in test_x2]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string list x1, x2 to np array of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Find the length of longest sequence, we shall pad all sentences to this length\n",
    "max_seq_len = 0\n",
    "for x in x1:\n",
    "    max_seq_len = max(max_seq_len, len(x))\n",
    "    \n",
    "for xs in x2:\n",
    "    for x in xs:   \n",
    "        max_seq_len = max(max_seq_len, len(x))\n",
    "        \n",
    "print('The longest sequnce in training data has %d words' %max_seq_len)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "new_x1 = []\n",
    "for sentence in x1:\n",
    "    tmp_sentence = []\n",
    "    # Converd word to index\n",
    "    for word in sentence:\n",
    "        if word in word2id:\n",
    "            tmp_sentence.append(word2id[word])\n",
    "        # else:\n",
    "            # print('Cannot find %s in vocab: ' %word)\n",
    "    \n",
    "    # Padding all sequences to same length\n",
    "    len_to_pad = max_seq_len - len(tmp_sentence)\n",
    "    tmp_sentence.extend([0] * len_to_pad)\n",
    "    new_x1.append(tmp_sentence)\n",
    "    \n",
    "x1 = np.array(new_x1)\n",
    "print(x1.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "new_x2 = []\n",
    "for options in x2:\n",
    "    for sentence in options:\n",
    "        tmp_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2id:\n",
    "                tmp_sentence.append(word2id[word])\n",
    "\n",
    "        # Padding all sequences to same length\n",
    "        len_to_pad = max_seq_len - len(tmp_sentence)\n",
    "        tmp_sentence.extend([0] * len_to_pad)\n",
    "        new_x2.append(tmp_sentence)\n",
    "    \n",
    "x2 = np.array(new_x2)\n",
    "print(x2.shape)\n",
    "assert(x2.shape[-1] == max_seq_len)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to format: ( context, reponse, 0/1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Repeate x1 \n",
    "# -> (x1[0], x1[0], x1[0], x1[0], x1[0], x1[0],  x1[1], ...)\n",
    "num_responses = 6\n",
    "x1 = np.repeat(x1, num_responses, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Original 'y' means which response is correct\n",
    "# y = sample.answer.values\n",
    "# Now convert y to indicate wherther one (context, respoonse) is corrct, 0/1\n",
    "new_y = []\n",
    "for answer in y:\n",
    "    new_y.extend([0]*answer)\n",
    "    new_y.append(1)\n",
    "    new_y.extend([0]*(num_responses-answer-1))\n",
    "y = np.reshape(np.array(new_y), (-1, 1))\n",
    "print(y.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in  training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data corpus\n",
    "from mini_batch_helper import MiniBatchCorpus\n",
    "corpus_fname = [\n",
    "    'datas/training_data/下課花路米.txt',\n",
    "##     'datas/training_data/人生劇展.txt',\n",
    "#     'datas/training_data/公視藝文大道.txt',\n",
    "##     'datas/training_data/成語賽恩思.txt',\n",
    "##    'datas/training_data/我的這一班.txt',\n",
    "#     'datas/training_data/流言追追追.txt',\n",
    "#     'datas/training_data/聽聽看.txt',\n",
    "#    'datas/training_data/誰來晚餐.txt',\n",
    "]\n",
    "corpus = []\n",
    "for fname in corpus_fname:\n",
    "    with open(fname, 'r') as f:\n",
    "        corpus.extend([[s.split() for s in line.split('\\t')] for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a MiniBatchCorpus to get training data as batch\n",
    "from mini_batch_helper import MiniBatchCorpus\n",
    "data_loader = MiniBatchCorpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_len 66\n",
      "mean_seq_len 4.628876693328018\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "mean_seq_len = 0\n",
    "num_sentences = 0\n",
    "for episode in corpus:\n",
    "    for sentence in episode:\n",
    "        max_seq_len = max(max_seq_len, len(sentence))\n",
    "        mean_seq_len += len(sentence)\n",
    "        num_sentences += 1\n",
    "mean_seq_len /= num_sentences\n",
    "print('max_seq_len', max_seq_len)\n",
    "print('mean_seq_len', mean_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['這裡', '有個', '洞', '這邊', '有個', '洞', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['西天', '說', '三次', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ list(['這裡', '有個', '洞', '這邊', '有個', '洞', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      " list(['西天', '說', '三次', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "(2,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3aeab2aed6bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "x1, x2, y = data_loader.next_batch(batch_size=2, pad_to_length=max_seq_len, pad_wor)\n",
    "print(x1)\n",
    "print(x1.shape)\n",
    "print(x1[0].shape)\n",
    "print(x1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "release_variable(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ( tf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# reference: https://github.com/dennybritz/chatbot-retrieval/blob/8b1be4c2e63631b1180b97ef927dc2c1f7fe9bea/udc_hparams.py\n",
    "# Model Parameters\n",
    "params = {}\n",
    "params['word2vec_path'] = load_word2vec_path + '.wv.syn0.npy'\n",
    "params['word2vec_vocab_size'] = word2vec_vocab_size\n",
    "params['word2vec_dim'] = word2vec_dim\n",
    "params['rnn_dim'] = 256\n",
    "\n",
    "# Training Parameters\n",
    "params['learning_rate'] = 0.001\n",
    "params['batch_size'] = 128\n",
    "params['eval_batch_size'] = 16\n",
    "params['n_iterations'] = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Embedding 後可以考慮加一層 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input\n",
    "#context = tf.placeholder(dtype=tf.int64, shape=(None, max_seq_len), name='context')\n",
    "#response = tf.placeholder(dtype=tf.int64, shape=(None, max_seq_len), name='response')\n",
    "context = tf.placeholder(dtype=tf.int64, shape=(None, None), name='context')\n",
    "response = tf.placeholder(dtype=tf.int64, shape=(None, None), name='response')\n",
    "target = tf.placeholder(dtype=tf.int64, shape=(None, 1), name='target')\n",
    "\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # Embedding\n",
    "    init_embedding_W = np.load(open(params['word2vec_path'], 'rb'))\n",
    "    embedding_input_dim = init_embedding_W.shape[0]  # vocab size\n",
    "    embedding_output_dim = init_embedding_W.shape[1]  # embedding output dim\n",
    "    init_embedding_W = tf.constant_initializer(init_embedding_W)\n",
    "    embeddings_W = tf.get_variable('embeddings_W', shape=[embedding_input_dim, embedding_output_dim], initializer=init_embedding_W)\n",
    "    context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "    response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "\n",
    "    # shared LSTM encoder\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, \n",
    "                use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "    encoding_context = c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "    encoding_response = r_states.h\n",
    "\n",
    "    # σ(cMr)\n",
    "    M = tf.get_variable('M', shape=[params['rnn_dim'], params['rnn_dim']], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    # \"Predict\" a  response: c * M\n",
    "    generated_response = tf.matmul(encoding_context, M)\n",
    "    generated_response = tf.expand_dims(generated_response, 2)\n",
    "    encoding_response = tf.expand_dims(encoding_response, 2)\n",
    "\n",
    "    # Dot product between generated response and actual response\n",
    "    logits = tf.matmul(generated_response, encoding_response, True)\n",
    "    logits = tf.squeeze(logits, [2])\n",
    "\n",
    "    # Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "    probs = tf.sigmoid(logits)\n",
    "\n",
    "    # Calculate the binary cross-entropy loss\n",
    "    loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.to_float(target)), name='mean_loss_of_batch')\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "import os\n",
    "\n",
    "exp_name = 'dual_lstm_0'\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # sess.run(embeddings_W.initializer)\n",
    "    # sess.run(M.initializer)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Restore model\n",
    "    # saver.restore(sess, 'models/%s/%s.ckpt' % (exp_name, params['n_iterations'])\n",
    "    # print(\"Model restored.\")\n",
    "\n",
    "    for it in range(params['n_iterations']):\n",
    "        print('Iterations %4d:\\t' %(it+1) , end=\"\")\n",
    "        # Train next batch\n",
    "        next_x1, next_x2, next_y = data_loader.next_batch(batch_size=params['batch_size'], pad_to_length=max_seq_len)\n",
    "        sess.run(train_step, feed_dict={context: next_x1, response: next_x2, target: next_y})\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists('models/'+exp_name):\n",
    "        os.makedirs('models/'+exp_name)\n",
    "    save_path = saver.save(sess, 'models/%s/%s.ckpt' % (exp_name, params['n_iterations']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
