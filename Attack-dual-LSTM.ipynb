{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual LSTM with official sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from /home/sunset/word_contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u849ecfdca27003d306f39ca004b82b5b.cache\n",
      "Loading model cost 1.169 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from mini_batch_helper import MiniBatch\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "* [keras 使用 word2vec 當 embedding 的教學](http://ben.bolte.cc/blog/2016/gensim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec model function to output as keras embedding layer\n",
    "from gensim.models import word2vec\n",
    "word2vec_model = word2vec.Word2Vec.load('models/word2vec_250.model.bin')\n",
    "# vocab = word2vec_model.wv.vocab\n",
    "# help(word2vec_model.wv.get_embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將 word2vec_model 的 vocab 存成 dictionary 'word2id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given word string, find index in word2vec_model vocab... 然後 ->  95\n"
     ]
    }
   ],
   "source": [
    "# Save the vocab\n",
    "# Sequences have various lengths, so let index '0' serve as padding  -> all index+1\n",
    "word2id = dict([(k, v.index+1) for k, v in word2vec_model.wv.vocab.items()])\n",
    "print('given word string, find index in word2vec_model vocab... 然後 -> ', word2id['然後'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release unused memory comsumed model\n",
    "import time\n",
    "import gc\n",
    "del(word2vec_model)\n",
    "time.sleep(1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given index, find word string in word2vec_model vocab... 95 ->  然後\n"
     ]
    }
   ],
   "source": [
    "id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "print('given index, find word string in word2vec_model vocab... 95 -> ', id2word[95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in official sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>options</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A:你這麼快就知道了</td>\n",
       "      <td>B:全家就是你家\\tB:付出不是浪費時間\\tB:願意為社會付出的人太少了\\tB:我都是一個人...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A:每一支冰塊都不同 水質不同 硬度不同</td>\n",
       "      <td>A:紋路不同\\tA:也是你唯一發洩情緒的辦法吧\\tA:還記不記得那次你在我家巷口\\tA:像拼...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A:這樣沖這麼一大塊網子 要多久時間</td>\n",
       "      <td>B:有更舒適的環境\\tB:感覺是一隻很貼心的貓咪\\tB:我相信一定可以成功的啦\\tB:可以講...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A:兒子啊 都幾點了 你還不睡</td>\n",
       "      <td>B:沒關係啦 我來幫你\\tB:我在背書啦 就快背好了\\tB:造成誤會 不是很可惜嗎\\tB:女...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A:孩子還在念書的時候 看到其他小朋友都有父母 而自己沒有媽媽</td>\n",
       "      <td>A:那孩子就是突然\\tA:就是說\\tA:禮拜一到學校\\tA:媽媽還不會開車\\tA:就會問我媽...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>A:那牠吐絲這三天三夜\\tA:你們不就都要一直在這邊守護著牠囉</td>\n",
       "      <td>B:涂先生\\tB:那這些蠶寶寶在這邊吐絲\\tB:要吐多久啊\\tB:溫度三十多度的話\\tB:對...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>A:奇怪了 濂僑跑到哪兒去了呢 濂僑 你怎麼啦</td>\n",
       "      <td>B:我可能吃壞肚子了 肚子好痛喔\\tB:另外一種比法是 一星期有七天\\tB:為什麼不可以 同...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A:是什麼讓玉屏的改變這麼大啊</td>\n",
       "      <td>B:當然是紀老師的魔法囉\\tB:都交給這位大力士吧\\tB:粗重的交給我就對了\\tB:我是八年...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A:曉書啊 你手上拿的 該不是最新的遊戲機吧</td>\n",
       "      <td>B:你不要小看這個扯鈴\\t B:互相幫忙一下嘛\\tB:一個週末出去走走 沒有關係的啦\\tB:...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A:失戀的確會讓人家很難過</td>\n",
       "      <td>A:所以你要縮短認真難過的時間\\tA:他走了十三個年頭\\tA:獲選之後當然是很開心囉\\tA:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>A:漁網破了 魚跑了 漁民的臉就綠了</td>\n",
       "      <td>B:桂花做成的花茶 在台灣的名氣可是響叮噹喔\\tB:這個洗漁網的工作 真的是比較枯燥乏味一點...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>A:可以穿襪子了\\t情況怎麼樣</td>\n",
       "      <td>B:使出擒拿手抓住球棒 反手制服毒販在地上\\tB:這個小朋友習慣蹺課去打網咖 一旦打到沒有錢...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>A:下下禮拜就要段考了 你們都不用看書的嗎</td>\n",
       "      <td>B:要懂得把工作平均分配給大家\\tB:我要報告紀老師\\tB:越早讀書就會越早忘記\\tB:我知...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>A:網路上那些流言說如果說你把蝦加一些尿液會讓蝦活得比較久根本騙人的</td>\n",
       "      <td>A:這個時期的小孩子\\tA:你看晶瑩剔透的\\tA:最新鮮的保存方式就是把牠急速冷凍起來\\tA...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>A:可是為什麼會選擇到 歐洲的布拉格那個城市 去學習戲劇呢</td>\n",
       "      <td>B:最早的人類先民們最早期的武士帶著青銅武器和騎術\\tB:客家米食以前是過年過節才吃的\\tB...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>A:叫你幫忙做個事你心不甘情不願的 乾脆不要做算了</td>\n",
       "      <td>B:不去理解孩子心裡想法的父親\\tB:我不想再丟臉一次了\\tB:因為不管是大人或小孩都喜歡聽...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>A:校長 你很喜歡雞蛋花嗎</td>\n",
       "      <td>B:我有吃過叫花雞\\tB:我最喜歡雞蛋花了 從前老家院子裡種了好多\\tB:我早上看你的雞蛋花...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>A:您平常都是幾點開始工作的呢 是早上呢 還是晚上工作呢</td>\n",
       "      <td>B:都是早上差不多五 六點鐘\\tB:我很好奇的是 這門課程內容是什麼\\tB:其實家長不用擔心...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>A:這沖不掉啊 怎麼辦</td>\n",
       "      <td>B:怎麼會沖不掉呢\\tB:那我們出發吧\\tB:你也笑得太誇張了吧\\tB:大家都好熱情\\tB:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>A:不過我覺得這首歌的旋律很特別 我很喜歡</td>\n",
       "      <td>B:今年看能不能抓多一點\\tB:獅是百獸之王\\tB:整個村莊裡當人媳婦的也只有她而已\\tB:...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>A:我是這一片桑園的主人\\t怎麼稱呼你</td>\n",
       "      <td>B:對\\tB:那我就出發囉\\tB:我姓涂\\tB:桑葉好漂亮哦\\tB:好難得看到那麼大片的桑葉...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>A:你在找什麼東西 需不需要我幫忙\\t我好像忘記帶膠水了 奇怪 我剛剛明明有拿下來的</td>\n",
       "      <td>B:那封匿名信確實是我寫的\\tB:我想結果還是一樣的啦\\tB:我們一定會獲勝的\\tB:大家要...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>A:那明天要什麼時候來學\\t明天喔 明天差不多早晨四點鐘</td>\n",
       "      <td>B:我們常看到的麻雀\\tB:你不要亂動好不好\\tB:這裡不是教育部嗎\\tB:我在製作一個很簡...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>A:則緯對你這麼好 為什麼要拒絕他\\tB:因為我只想 跟則緯繼續當好同學就好了</td>\n",
       "      <td>A:可是班上的同學都不喜歡我\\tA:可是我看則緯想當的 不只是好同學吧\\tA:我自己的男朋友...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>A:怎麼沒有通知我呢\\tB:怎麼通知 找得到嗎</td>\n",
       "      <td>B:老師要先走了\\tB:下一次再開同學會\\tB:我整整一年都在找你 好累\\tB:原來鍾沅失蹤...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>A:你還有閒情逸致聽收音機啊\\tB:我在聽今天星座運勢分析 已經快講到我了</td>\n",
       "      <td>A:物超所值耶\\tA:只要她一高興 你要借她的皇冠戴戴 絕對沒有問題\\tA:這種不科學的東西...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>A:小P 你跟我一起回家去好不好\\tB:妳在開玩笑的吧</td>\n",
       "      <td>A:我是從台灣來的\\tA:麻煩幫我還給老爹\\tA:晚上記得傳照片給我啊\\tA:我跟你沒開玩笑...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>A:只是連我們成年人談起戀愛\\tA:都很難做得到好聚好散</td>\n",
       "      <td>A:更何況你們只是青少年\\tA:這個時候又要和大家說再見了\\tA:要記得收看我和曉書主持的聽...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>A:下次如果不棄嫌 你如果要唱歌之前 可以來我們家吃飯\\tB:下次應該不是媽媽煮的吧</td>\n",
       "      <td>A:我說我的女兒給你了 請你接受她的脾氣\\tA:從這件事情呢 我也希望你可以得到一個教訓\\t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>A:據說開挖這段路的隧道\\tA:工程艱險到不斷的延期才完工</td>\n",
       "      <td>A:是東線鐵路拓寬的時候\\tA:我們單車騎士騎乘的水泥路\\tA:叫做掃叭隧道\\tA:當然我也...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>A:經過這一次的調查 其實我算是有一種體驗了\\tB:什麼體驗</td>\n",
       "      <td>A:現在我們打算一步一步慢慢來\\tA:大概很難運用在日常生活當中吧\\tA:會不會也是一場騙局...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>A:等那些官兵們過來 我們的村子早就掛了 B:不怕他 跟他拼了</td>\n",
       "      <td>A:我是先看她的工作效率好不好\\tA:你們幹嘛告我狀\\tA:大嬸啊 我們村子都是善男信女 老...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>A:你這隻襪子怎麼那麼髒 你到底有沒有洗\\tB:反正褲管遮住了也看不到 沒差啦</td>\n",
       "      <td>A:你怎麼那麼髒啊\\tA:我支持他\\tA:我又要補習\\tA:我要去籃球隊練球囉\\tA:你找我...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>A:你一直打它是在幹什麼\\tB:現在是要取肉\\tA:怎麼取</td>\n",
       "      <td>B:這個猴子的眼睛一刻上去 整個那隻潑猴的精神\\tB:就是先把這條魚 它是要打扁才能翻過來\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>A:不然我們來比賽啊\\tB:好啊 比什麼</td>\n",
       "      <td>A:我們來找土星的資料看誰找到比較多\\tA:讓我去啦\\tA:好啊 要比就比啊\\tA:我是不會...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>A:借我看一下嘛\\tB:不要啦我房間很亂</td>\n",
       "      <td>A:我們有問題要問你\\tA:可是老師現在有事情耶\\tA:我們下次再問好了\\tA:老師老師\\t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>A:這是班遊地點的幾個提案 請大家投票表決\\tB:這些地點都好無聊 誰想去啊</td>\n",
       "      <td>A:你們的想法都不錯啦\\tA:你還是找別人啦\\tA:但是這些地點都是你提議的耶\\tA:他的一...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>A:我的乖孫女呢\\tB:她在房間裡讀書 大概沒聽到您進來的聲音</td>\n",
       "      <td>A:膝蓋有沒有感覺舒服一點\\tA:去叫她一下\\tA:你笑得甜蜜蜜\\tA:可是我就覺得奶奶很可...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>A:那你知道為什麼會有攀岩場嗎 B:為什麼</td>\n",
       "      <td>A:我睡得好飽\\tA:他們常常被我罵\\tA:因為那邊的同學體育都非常好 很會爬樹\\tA:看你...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>A:家隆 你昨晚睡得還好嗎\\tB:不錯 你呢</td>\n",
       "      <td>A:我可不可以一起搭船\\tA:我也想試試看\\tA:美景當前 我們應該要把握當下\\tA:這個地...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>A:你們兩個會不會想讀同一所高中\\tB:當然囉 我和舒琪早就說好了 我們會一起讀高中和大學\\...</td>\n",
       "      <td>A:帶我們好好去探索一番\\tA:千晏這是特別做給你吃的點心\\tA:真好 這麼早就立定志向了\\...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>A:豆油伯 嘉義是嘉義的意思嗎\\tB:對啊 我想到有一句童謠 跟嘉義跟火車都有關係\\tA:那...</td>\n",
       "      <td>B:可是輝哥 現在可能很難辦到喔\\tB:好 你們仔細聽\\tB:可是現在大部分的鹽田好像都荒廢...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>A:老師 你不要那麼勉為其難的臉啦\\tB:好啦 我不為難你了\\tA:那我可不可以請問你一個問...</td>\n",
       "      <td>B:把它覆蓋上面把它吸乾來\\tB:像我這個就沒有保護的作用啦\\tB:好的年畫就要像這個 這個...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>A:神算公主 你這次啊麻煩大了\\tB:關我什麼事啊\\tA:你不知道嗎 她為了減肥塑身啊 已經...</td>\n",
       "      <td>B:幫我算一下我今天的考運是怎樣\\tB:我又沒有叫她不要吃飯\\tB:要算命先找我報名\\tB:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>A:你發什麼瘋啊你\\tB:你才發瘋呢\\tB:我看你有妄想症</td>\n",
       "      <td>A:是球員的生命\\tA:要想打好籃球\\tA:就得喜歡籃球\\tA:好好照顧它\\tA:要想在體育...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>A:豆油 我長得這個樣子啊\\tB:沒有啦 我是畫你的神情 你的神情這樣很像啊\\tA:我就說你...</td>\n",
       "      <td>B:那你們三個人就留下來做我的模特兒\\tB:姿勢要擺漂亮一點\\tB:每年四到九月是芒果成熟的...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>A:亞婷 你在刷馬嗎\\tB:對啊\\tA:為什麼要這樣刷馬呢</td>\n",
       "      <td>B:踩住馬蹬 然後嘿咻就上去了\\tB:因為牠這樣會比較舒服\\tB:這樣就行了\\tB:不過我現...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>A:吃超過一斤就要算錢\\tB:什麼 你們不能邊吃邊做嗎 邊做邊吃\\tA:不行 吃超過一斤就要算錢</td>\n",
       "      <td>B:是喔 要算錢就對了\\tB:你這個是感冒的症狀\\tB:你是哪裡不舒服啊\\tB:我覺得很像飛...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>A:你很髒 怎麼會有人想要跟你做朋友\\tB:姊 你說這什麼話 我同學都認為我是個愛乾淨的人\\...</td>\n",
       "      <td>B:她在體育方面有很高的成就 還當過立法委員呢\\tB:很多事都嘛只是看外表 我只要外表看起來...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>A:我們去游泳\\tB:我又沒有帶泳衣\\tA:我的借你啊</td>\n",
       "      <td>B:我們去跑步\\tB:你會不會覺得你長得像奧莉薇荷西啊\\tB:放學後早點回家\\tB:你看起來...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           dialogue  \\\n",
       "0    0                                         A:你這麼快就知道了   \n",
       "1    1                               A:每一支冰塊都不同 水質不同 硬度不同   \n",
       "2    2                                 A:這樣沖這麼一大塊網子 要多久時間   \n",
       "3    3                                    A:兒子啊 都幾點了 你還不睡   \n",
       "4    4                    A:孩子還在念書的時候 看到其他小朋友都有父母 而自己沒有媽媽   \n",
       "5    5                    A:那牠吐絲這三天三夜\\tA:你們不就都要一直在這邊守護著牠囉   \n",
       "6    6                            A:奇怪了 濂僑跑到哪兒去了呢 濂僑 你怎麼啦   \n",
       "7    7                                    A:是什麼讓玉屏的改變這麼大啊   \n",
       "8    8                             A:曉書啊 你手上拿的 該不是最新的遊戲機吧   \n",
       "9    9                                      A:失戀的確會讓人家很難過   \n",
       "10  10                                 A:漁網破了 魚跑了 漁民的臉就綠了   \n",
       "11  11                                    A:可以穿襪子了\\t情況怎麼樣   \n",
       "12  12                              A:下下禮拜就要段考了 你們都不用看書的嗎   \n",
       "13  13                 A:網路上那些流言說如果說你把蝦加一些尿液會讓蝦活得比較久根本騙人的   \n",
       "14  14                      A:可是為什麼會選擇到 歐洲的布拉格那個城市 去學習戲劇呢   \n",
       "15  15                          A:叫你幫忙做個事你心不甘情不願的 乾脆不要做算了   \n",
       "16  16                                      A:校長 你很喜歡雞蛋花嗎   \n",
       "17  17                       A:您平常都是幾點開始工作的呢 是早上呢 還是晚上工作呢   \n",
       "18  18                                        A:這沖不掉啊 怎麼辦   \n",
       "19  19                              A:不過我覺得這首歌的旋律很特別 我很喜歡   \n",
       "20  20                                A:我是這一片桑園的主人\\t怎麼稱呼你   \n",
       "21  21         A:你在找什麼東西 需不需要我幫忙\\t我好像忘記帶膠水了 奇怪 我剛剛明明有拿下來的   \n",
       "22  22                       A:那明天要什麼時候來學\\t明天喔 明天差不多早晨四點鐘   \n",
       "23  23            A:則緯對你這麼好 為什麼要拒絕他\\tB:因為我只想 跟則緯繼續當好同學就好了   \n",
       "24  24                            A:怎麼沒有通知我呢\\tB:怎麼通知 找得到嗎   \n",
       "25  25              A:你還有閒情逸致聽收音機啊\\tB:我在聽今天星座運勢分析 已經快講到我了   \n",
       "26  26                        A:小P 你跟我一起回家去好不好\\tB:妳在開玩笑的吧   \n",
       "27  27                       A:只是連我們成年人談起戀愛\\tA:都很難做得到好聚好散   \n",
       "28  28         A:下次如果不棄嫌 你如果要唱歌之前 可以來我們家吃飯\\tB:下次應該不是媽媽煮的吧   \n",
       "29  29                      A:據說開挖這段路的隧道\\tA:工程艱險到不斷的延期才完工   \n",
       "30  30                     A:經過這一次的調查 其實我算是有一種體驗了\\tB:什麼體驗   \n",
       "31  31                    A:等那些官兵們過來 我們的村子早就掛了 B:不怕他 跟他拼了   \n",
       "32  32            A:你這隻襪子怎麼那麼髒 你到底有沒有洗\\tB:反正褲管遮住了也看不到 沒差啦   \n",
       "33  33                      A:你一直打它是在幹什麼\\tB:現在是要取肉\\tA:怎麼取   \n",
       "34  34                               A:不然我們來比賽啊\\tB:好啊 比什麼   \n",
       "35  35                               A:借我看一下嘛\\tB:不要啦我房間很亂   \n",
       "36  36             A:這是班遊地點的幾個提案 請大家投票表決\\tB:這些地點都好無聊 誰想去啊   \n",
       "37  37                    A:我的乖孫女呢\\tB:她在房間裡讀書 大概沒聽到您進來的聲音   \n",
       "38  38                              A:那你知道為什麼會有攀岩場嗎 B:為什麼   \n",
       "39  39                             A:家隆 你昨晚睡得還好嗎\\tB:不錯 你呢   \n",
       "40  40  A:你們兩個會不會想讀同一所高中\\tB:當然囉 我和舒琪早就說好了 我們會一起讀高中和大學\\...   \n",
       "41  41  A:豆油伯 嘉義是嘉義的意思嗎\\tB:對啊 我想到有一句童謠 跟嘉義跟火車都有關係\\tA:那...   \n",
       "42  42  A:老師 你不要那麼勉為其難的臉啦\\tB:好啦 我不為難你了\\tA:那我可不可以請問你一個問...   \n",
       "43  43  A:神算公主 你這次啊麻煩大了\\tB:關我什麼事啊\\tA:你不知道嗎 她為了減肥塑身啊 已經...   \n",
       "44  44                      A:你發什麼瘋啊你\\tB:你才發瘋呢\\tB:我看你有妄想症   \n",
       "45  45  A:豆油 我長得這個樣子啊\\tB:沒有啦 我是畫你的神情 你的神情這樣很像啊\\tA:我就說你...   \n",
       "46  46                      A:亞婷 你在刷馬嗎\\tB:對啊\\tA:為什麼要這樣刷馬呢   \n",
       "47  47   A:吃超過一斤就要算錢\\tB:什麼 你們不能邊吃邊做嗎 邊做邊吃\\tA:不行 吃超過一斤就要算錢   \n",
       "48  48  A:你很髒 怎麼會有人想要跟你做朋友\\tB:姊 你說這什麼話 我同學都認為我是個愛乾淨的人\\...   \n",
       "49  49                        A:我們去游泳\\tB:我又沒有帶泳衣\\tA:我的借你啊   \n",
       "\n",
       "                                              options  answer  \n",
       "0   B:全家就是你家\\tB:付出不是浪費時間\\tB:願意為社會付出的人太少了\\tB:我都是一個人...       4  \n",
       "1   A:紋路不同\\tA:也是你唯一發洩情緒的辦法吧\\tA:還記不記得那次你在我家巷口\\tA:像拼...       0  \n",
       "2   B:有更舒適的環境\\tB:感覺是一隻很貼心的貓咪\\tB:我相信一定可以成功的啦\\tB:可以講...       5  \n",
       "3   B:沒關係啦 我來幫你\\tB:我在背書啦 就快背好了\\tB:造成誤會 不是很可惜嗎\\tB:女...       1  \n",
       "4   A:那孩子就是突然\\tA:就是說\\tA:禮拜一到學校\\tA:媽媽還不會開車\\tA:就會問我媽...       4  \n",
       "5   B:涂先生\\tB:那這些蠶寶寶在這邊吐絲\\tB:要吐多久啊\\tB:溫度三十多度的話\\tB:對...       4  \n",
       "6   B:我可能吃壞肚子了 肚子好痛喔\\tB:另外一種比法是 一星期有七天\\tB:為什麼不可以 同...       0  \n",
       "7   B:當然是紀老師的魔法囉\\tB:都交給這位大力士吧\\tB:粗重的交給我就對了\\tB:我是八年...       0  \n",
       "8   B:你不要小看這個扯鈴\\t B:互相幫忙一下嘛\\tB:一個週末出去走走 沒有關係的啦\\tB:...       3  \n",
       "9   A:所以你要縮短認真難過的時間\\tA:他走了十三個年頭\\tA:獲選之後當然是很開心囉\\tA:...       0  \n",
       "10  B:桂花做成的花茶 在台灣的名氣可是響叮噹喔\\tB:這個洗漁網的工作 真的是比較枯燥乏味一點...       1  \n",
       "11  B:使出擒拿手抓住球棒 反手制服毒販在地上\\tB:這個小朋友習慣蹺課去打網咖 一旦打到沒有錢...       2  \n",
       "12  B:要懂得把工作平均分配給大家\\tB:我要報告紀老師\\tB:越早讀書就會越早忘記\\tB:我知...       2  \n",
       "13  A:這個時期的小孩子\\tA:你看晶瑩剔透的\\tA:最新鮮的保存方式就是把牠急速冷凍起來\\tA...       2  \n",
       "14  B:最早的人類先民們最早期的武士帶著青銅武器和騎術\\tB:客家米食以前是過年過節才吃的\\tB...       2  \n",
       "15  B:不去理解孩子心裡想法的父親\\tB:我不想再丟臉一次了\\tB:因為不管是大人或小孩都喜歡聽...       5  \n",
       "16  B:我有吃過叫花雞\\tB:我最喜歡雞蛋花了 從前老家院子裡種了好多\\tB:我早上看你的雞蛋花...       1  \n",
       "17  B:都是早上差不多五 六點鐘\\tB:我很好奇的是 這門課程內容是什麼\\tB:其實家長不用擔心...       0  \n",
       "18  B:怎麼會沖不掉呢\\tB:那我們出發吧\\tB:你也笑得太誇張了吧\\tB:大家都好熱情\\tB:...       0  \n",
       "19  B:今年看能不能抓多一點\\tB:獅是百獸之王\\tB:整個村莊裡當人媳婦的也只有她而已\\tB:...       3  \n",
       "20  B:對\\tB:那我就出發囉\\tB:我姓涂\\tB:桑葉好漂亮哦\\tB:好難得看到那麼大片的桑葉...       2  \n",
       "21  B:那封匿名信確實是我寫的\\tB:我想結果還是一樣的啦\\tB:我們一定會獲勝的\\tB:大家要...       4  \n",
       "22  B:我們常看到的麻雀\\tB:你不要亂動好不好\\tB:這裡不是教育部嗎\\tB:我在製作一個很簡...       5  \n",
       "23  A:可是班上的同學都不喜歡我\\tA:可是我看則緯想當的 不只是好同學吧\\tA:我自己的男朋友...       1  \n",
       "24  B:老師要先走了\\tB:下一次再開同學會\\tB:我整整一年都在找你 好累\\tB:原來鍾沅失蹤...       2  \n",
       "25  A:物超所值耶\\tA:只要她一高興 你要借她的皇冠戴戴 絕對沒有問題\\tA:這種不科學的東西...       2  \n",
       "26  A:我是從台灣來的\\tA:麻煩幫我還給老爹\\tA:晚上記得傳照片給我啊\\tA:我跟你沒開玩笑...       3  \n",
       "27  A:更何況你們只是青少年\\tA:這個時候又要和大家說再見了\\tA:要記得收看我和曉書主持的聽...       0  \n",
       "28  A:我說我的女兒給你了 請你接受她的脾氣\\tA:從這件事情呢 我也希望你可以得到一個教訓\\t...       4  \n",
       "29  A:是東線鐵路拓寬的時候\\tA:我們單車騎士騎乘的水泥路\\tA:叫做掃叭隧道\\tA:當然我也...       4  \n",
       "30  A:現在我們打算一步一步慢慢來\\tA:大概很難運用在日常生活當中吧\\tA:會不會也是一場騙局...       5  \n",
       "31  A:我是先看她的工作效率好不好\\tA:你們幹嘛告我狀\\tA:大嬸啊 我們村子都是善男信女 老...       2  \n",
       "32  A:你怎麼那麼髒啊\\tA:我支持他\\tA:我又要補習\\tA:我要去籃球隊練球囉\\tA:你找我...       0  \n",
       "33  B:這個猴子的眼睛一刻上去 整個那隻潑猴的精神\\tB:就是先把這條魚 它是要打扁才能翻過來\\...       1  \n",
       "34  A:我們來找土星的資料看誰找到比較多\\tA:讓我去啦\\tA:好啊 要比就比啊\\tA:我是不會...       0  \n",
       "35  A:我們有問題要問你\\tA:可是老師現在有事情耶\\tA:我們下次再問好了\\tA:老師老師\\t...       4  \n",
       "36  A:你們的想法都不錯啦\\tA:你還是找別人啦\\tA:但是這些地點都是你提議的耶\\tA:他的一...       2  \n",
       "37  A:膝蓋有沒有感覺舒服一點\\tA:去叫她一下\\tA:你笑得甜蜜蜜\\tA:可是我就覺得奶奶很可...       1  \n",
       "38  A:我睡得好飽\\tA:他們常常被我罵\\tA:因為那邊的同學體育都非常好 很會爬樹\\tA:看你...       2  \n",
       "39  A:我可不可以一起搭船\\tA:我也想試試看\\tA:美景當前 我們應該要把握當下\\tA:這個地...       4  \n",
       "40  A:帶我們好好去探索一番\\tA:千晏這是特別做給你吃的點心\\tA:真好 這麼早就立定志向了\\...       2  \n",
       "41  B:可是輝哥 現在可能很難辦到喔\\tB:好 你們仔細聽\\tB:可是現在大部分的鹽田好像都荒廢...       1  \n",
       "42  B:把它覆蓋上面把它吸乾來\\tB:像我這個就沒有保護的作用啦\\tB:好的年畫就要像這個 這個...       2  \n",
       "43  B:幫我算一下我今天的考運是怎樣\\tB:我又沒有叫她不要吃飯\\tB:要算命先找我報名\\tB:...       1  \n",
       "44  A:是球員的生命\\tA:要想打好籃球\\tA:就得喜歡籃球\\tA:好好照顧它\\tA:要想在體育...       5  \n",
       "45  B:那你們三個人就留下來做我的模特兒\\tB:姿勢要擺漂亮一點\\tB:每年四到九月是芒果成熟的...       3  \n",
       "46  B:踩住馬蹬 然後嘿咻就上去了\\tB:因為牠這樣會比較舒服\\tB:這樣就行了\\tB:不過我現...       1  \n",
       "47  B:是喔 要算錢就對了\\tB:你這個是感冒的症狀\\tB:你是哪裡不舒服啊\\tB:我覺得很像飛...       0  \n",
       "48  B:她在體育方面有很高的成就 還當過立法委員呢\\tB:很多事都嘛只是看外表 我只要外表看起來...       1  \n",
       "49  B:我們去跑步\\tB:你會不會覺得你長得像奧莉薇荷西啊\\tB:放學後早點回家\\tB:你看起來...       5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract sample test datas\n",
    "x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "# y = sample.answer.values\n",
    "\n",
    "# Tokenize\n",
    "x1 = np.array([list(jieba.cut(' '.join(_))) for _ in x1])\n",
    "x2 = np.array([[list(jieba.cut(s)) for s in _] for _ in x2])\n",
    "assert(np.sum([len(_)!=6 for _ in x2]) == 0)\n",
    "\n",
    "# Create MiniBatch class\n",
    "# data_loader = MiniBatch(x1, x2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string list x1, x2 to np array of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequnce in training data has 44 words\n"
     ]
    }
   ],
   "source": [
    "# Find the length of longest sequence, we shall pad all sentences to this length\n",
    "max_seq_len = 0\n",
    "for x in x1:\n",
    "    max_seq_len = max(max_seq_len, len(x))\n",
    "    \n",
    "for xs in x2:\n",
    "    for x in xs:\n",
    "        \n",
    "        max_seq_len = max(max_seq_len, len(x))\n",
    "        \n",
    "print('The longest sequnce in training data has %d words' %max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 44)\n"
     ]
    }
   ],
   "source": [
    "new_x1 = []\n",
    "for sentence in x1:\n",
    "    tmp_sentence = []\n",
    "    # Converd word to index\n",
    "    for word in sentence:\n",
    "        if word in word2id:\n",
    "            tmp_sentence.append(word2id[word])\n",
    "        # else:\n",
    "            # print('Cannot find %s in vocab: ' %word)\n",
    "    \n",
    "    # Padding all sequences to same length\n",
    "    len_to_pad = max_seq_len - len(tmp_sentence)\n",
    "    tmp_sentence.extend([0] * len_to_pad)\n",
    "    new_x1.append(tmp_sentence)\n",
    "    \n",
    "x1 = np.array(new_x1)\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 44)\n"
     ]
    }
   ],
   "source": [
    "new_x2 = []\n",
    "for sample in x2:\n",
    "    for sentence in sample:\n",
    "        tmp_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2id:\n",
    "                tmp_sentence.append(word2id[word])\n",
    "\n",
    "        # Padding all sequences to same length\n",
    "        len_to_pad = max_seq_len - len(tmp_sentence)\n",
    "        tmp_sentence.extend([0] * len_to_pad)\n",
    "        new_x2.append(tmp_sentence)\n",
    "    \n",
    "x2 = np.array(new_x2)\n",
    "print(x2.shape)\n",
    "assert(x2.shape[-1] == max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to format: ( context, reponse, 0/1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeate x1 \n",
    "# -> (x1[0], x1[0], x1[0], x1[0], x1[0], x1[0],  x1[1], ...)\n",
    "num_responses = 6\n",
    "x1 = np.repeat(x1, num_responses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-926f27184b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Original 'y' means which response is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Now convert y to indicate wherther one (context, respoonse) is corrct, 0/1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'answer'"
     ]
    }
   ],
   "source": [
    "# Original 'y' means which response is correct\n",
    "y = sample.answer.values\n",
    "# Now convert y to indicate wherther one (context, respoonse) is corrct, 0/1\n",
    "new_y = []\n",
    "for answer in y:\n",
    "    new_y.extend([0]*answer)\n",
    "    new_y.append(1)\n",
    "    new_y.extend([0]*(num_responses-answer-1))\n",
    "y = np.array(new_y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 44)\n",
      "(300, 44)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f4513023789c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(y.shape)\n",
    "print(type(x2[0][0]))\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ( tf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# reference: https://github.com/dennybritz/chatbot-retrieval/blob/8b1be4c2e63631b1180b97ef927dc2c1f7fe9bea/udc_hparams.py\n",
    "# Model Parameters\n",
    "params = {}\n",
    "params['word2_path'] = 'models/word2vec_250.model.bin.wv.syn0.npy'\n",
    "params['word_vec_dim'] = 250\n",
    "params['rnn_dim'] = 256\n",
    "\n",
    "# Training Parameters\n",
    "params['learning_rate'] = 0.001\n",
    "params['batch_size'] = 10 #128\n",
    "params['eval_batch_size'] = 6  #16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dual_lstm_model(params, context, response, y):\n",
    "    # Embedding\n",
    "    # with tf.device('/cpu:0'):\n",
    "    embeddings_W = tf.Variable(np.load(open(params['word2_path'], 'rb')))\n",
    "    context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "    response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "    print('context_embedded', context_embedded.get_shape())\n",
    "    \n",
    "    # shared LSTM encoder\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, \n",
    "                use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    \n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "    encoding_context = c_states.h\n",
    "    \n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "    #tensorflow.contrib.rnn.static_rnn(cell, x_sequence, initial_state=init_state, dtype=tf.float32)\n",
    "    encoding_response = r_states.h\n",
    "    # rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
    "    #    cell, \n",
    "    #    tf.concat(0, [context_embedded, response_embedded]), \n",
    "    #    sequence_length=tf.concat(0, [max_seq_len, max_seq_len]),\n",
    "    #    dtype=tf.float32)\n",
    "    # encoding_context, encoding_response = tf.split(0, 2, rnn_states.h)\n",
    "    \n",
    "    # σ(cMr)\n",
    "    M = tf.get_variable(\"M\", shape=[params['rnn_dim'], params['rnn_dim']], initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    # \"Predict\" a  response: c * M\n",
    "    generated_response = tf.matmul(encoding_context, M)\n",
    "    generated_response = tf.expand_dims(generated_response, 2)\n",
    "    encoding_utterance = tf.expand_dims(encoding_utterance, 2)\n",
    "\n",
    "    # Dot product between generated response and actual response\n",
    "    logits = tf.batch_matmul(generated_response, encoding_utterance, True)\n",
    "    logits = tf.squeeze(logits, [2])\n",
    "\n",
    "    # Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "    probs = tf.sigmoid(logits)\n",
    "\n",
    "    # Calculate the binary cross-entropy loss\n",
    "    loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(y)), name=\"mean_loss_of_batch\")\n",
    "    return probs, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_embedded (?, 44, 250)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'encoding_utterance' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-36fa50d2b5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#response = tf.placeholder('int32', [None, n_time_steps, params.word_vec_dim])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ad2e84c6354d>\u001b[0m in \u001b[0;36mdual_lstm_model\u001b[0;34m(params, context, response, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mgenerated_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgenerated_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mencoding_utterance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_utterance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Dot product between generated response and actual response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'encoding_utterance' referenced before assignment"
     ]
    }
   ],
   "source": [
    "context = tf.placeholder('int64', [None, max_seq_len], name='context_id')\n",
    "response = tf.placeholder('int64', [None, 1], name='response_id')\n",
    "y = tf.placeholder('int64', name='y')\n",
    "#response = tf.placeholder('int32', [None, n_time_steps, params.word_vec_dim])\n",
    "\n",
    "predict, loss = dual_lstm_model(params, context, response, y)\n",
    "train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session as sess:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ( keras )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "* 坑： ```Input(shape=```... 此處 shape 不包含 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Keras implementation of Dual LSTM \n",
    "# Inputs\n",
    "from keras.engine import Input\n",
    "\n",
    "timesteps = x1.shape[1]\n",
    "input_context = Input(shape=(timesteps, ), dtype='float32', name='input_context')\n",
    "input_responses = Input(shape=(timesteps, ), dtype='float32', name='input_response')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer: initialized with keras word2vec weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from gensim.models import word2vec\n",
    "\n",
    "embedding_weights = np.load(open('models/word2vec_250.model.bin.wv.syn0.npy', 'rb'))\n",
    "word2vec_dim = embedding_weights.shape[0]\n",
    "timesteps = x1.shape[1]\n",
    "embedding_layer = keras.layers.Embedding(input_dim=embedding_weights.shape[0], \n",
    "                                         output_dim=embedding_weights.shape[1], \n",
    "                                         mask_zero=True, # '0' serve as padding\n",
    "                                         weights=[embedding_weights],\n",
    "                                         input_length=timesteps,\n",
    "                                         trainable=False,  # cost too much memory\n",
    "                                        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Release unused memory comsumed model\n",
    "import time\n",
    "import gc\n",
    "del(embedding_weights, word2vec_model)\n",
    "time.sleep(1)\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Embedding\n",
    "import tensorflow as tf\n",
    "with tf.device('/cpu:0'):\n",
    "    embedded_contex = embedding_layer(input_context)\n",
    "    embedded_responses = embedding_layer(input_responses)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! Embedding 後可以考慮加一層 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shared LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# input of LSTM (batch, timesteps=44, word_vectors=250)\n",
    "lstm_dim = 256  # hyperparameter\n",
    "lstm_layer = LSTM(lstm_dim)\n",
    "lstm_context = lstm_layer(embedded_contex)\n",
    "lstm_responses = lstm_layer(embedded_responses)\n",
    "print('lstm_context:',lstm_context.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\sigma$(c * M * r +b)\n",
    "\n",
    "* Keras Model only accept keras tensor as output, so lets create our own keras  layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    #def __init__(self, kernel_initializer='truncated_normal',  bias_initializer='zeros', **kwargs):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.output_dim = 1\n",
    "        self.kernel_initializer = 'truncated_normal'#kernel_initializer\n",
    "        self.bias_initializer = 'zeros'#bias_initializer\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.M = self.add_weight(name='M', \n",
    "                                      shape=(lstm_dim, lstm_dim),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      trainable=True)\n",
    "        self.b = self.add_weight(name='b', shape=(1,), initializer=self.bias_initializer, trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs):\n",
    "        c = inputs[0]\n",
    "        r = inputs[1]\n",
    "        # c*M: (batch, 256) * (256, 256) -> (batch, 256)\n",
    "        out = K.dot(c, self.M)\n",
    "        \n",
    "        # c_M*r (batch, 1, 256) * (batch, 1, 256)  -> (batch, 1) \n",
    "        out = K.expand_dims(c, axis=1)\n",
    "        r = K.expand_dims(r, axis=-1)\n",
    "        out = K.batch_dot(out, r)\n",
    "        out = K.squeeze(out, axis=2)\n",
    "\n",
    "        # c_M_r + b\n",
    "        out = out + self.b\n",
    "\n",
    "        # sigmoid\n",
    "        return K.sigmoid(out)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # return (-1, self.output_dim)\n",
    "        # return (input_shape[0], self.output_dim)\n",
    "        return (input_shape[0][0], self.output_dim)\n",
    "\n",
    "\n",
    "out = MyLayer(kernel_initializer='truncated_normal')([lstm_context, lstm_responses])\n",
    "K.is_keras_tensor(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "#lstm_context lstm_responses\n",
    "\n",
    "#M = K.random_normal_variable(shape=(lstm_dim, lstm_dim), mean=0.0, scale=1.0)\n",
    "def mul(c, M):\n",
    "    return K.dot(c, M)\n",
    "\n",
    "#cM = Lambda(lambda x: mul(x, M), output_shape =(256,))(lstm_context)\n",
    "#K.is_keras_tensor(cM)\n",
    "\n",
    "cMr = Lambda(lambda x: mul(x, lstm_responses), output_shape =(1,))(lstm_context)\n",
    "K.is_keras_tensor(cMr)\n",
    "out = cMr\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.layers.merge import Dot\n",
    "out = Dot(axes=1)([lstm_context, lstm_responses])\n",
    "\n",
    "from keras.models import Model\n",
    "model = Model(inputs=[input_context, input_responses], outputs=out)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit([x1, x2], y, epochs=10, batch_size=30)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
