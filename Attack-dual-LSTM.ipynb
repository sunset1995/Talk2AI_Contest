{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/nctu/Talk2AI_Contest/datas/dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.uf5db4499c0b7f893953f2b98fec37422.cache\n",
      "Loading model cost 1.276 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('datas/dict/dict.txt.big')\n",
    "jieba.load_userdict('datas/dict/edu_dict.txt')\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "from mini_batch_helper import extractor, MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in  training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in  training data\n",
    "word2vec_fname = 'models/word2vec_no_tc_offitial_200.model.bin'\n",
    "corpus_fnames = [\n",
    "    'datas/training_data/no_TC_下課花路米.txt',\n",
    "    'datas/training_data/no_TC_人生劇展.txt',\n",
    "    'datas/training_data/no_TC_公視藝文大道.txt',\n",
    "    'datas/training_data/no_TC_成語賽恩思.txt',\n",
    "    'datas/training_data/no_TC_我的這一班.txt',\n",
    "    'datas/training_data/no_TC_流言追追追.txt',\n",
    "    'datas/training_data/no_TC_聽聽看.txt',\n",
    "    'datas/training_data/no_TC_誰來晚餐.txt',\n",
    "]\n",
    "sample_rate_on_training_datas = 1.0  # 1.0\n",
    "extra_words = ['<pad>']\n",
    "unknown_word = None\n",
    "\n",
    "word2id, id2word, word_p, embedding_matrix, corpus, corpus_id = extractor(word2vec_fname, corpus_fnames, sample_rate_on_training_datas, extra_words, unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datas num: 680836\n",
      "valid datas num: 18236\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "rnd_idx = np.arange(len(corpus_id))\n",
    "np.random.shuffle(rnd_idx)\n",
    "corpus_id = corpus_id[rnd_idx[:len(corpus_id)//2]]\n",
    "valid_corpus_num = 10\n",
    "\n",
    "train_data_loader = MiniBatchCorpus(corpus_id[valid_corpus_num:])\n",
    "valid_data_loader = MiniBatchCorpus(corpus_id[:valid_corpus_num])\n",
    "print('train datas num:', train_data_loader.data_num, flush=True)\n",
    "print('valid datas num:', valid_data_loader.data_num, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    349960.000000\n",
       "mean          4.591745\n",
       "std           2.178855\n",
       "min           0.000000\n",
       "25%           3.000000\n",
       "50%           5.000000\n",
       "75%           6.000000\n",
       "max          15.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Series([len(sentence) for episode in corpus_id for sentence in episode]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([len(sentence) for episode in corpus_id for sentence in episode])\n",
    "max_seq_len\n",
    "\n",
    "del(corpus)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ( tf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference: https://github.com/dennybritz/chatbot-retrieval/blob/8b1be4c2e63631b1180b97ef927dc2c1f7fe9bea/udc_hparams.py\n",
    "exp_name = 'dual_lstm_9'\n",
    "# Model Parameters\n",
    "params = {}\n",
    "save_params_dir = 'models/%s/' %exp_name\n",
    "params['word2vec_model_name'] = word2vec_fname\n",
    "params['word2vec_vocab_size'] = embedding_matrix.shape[0]\n",
    "params['word2vec_dim'] = embedding_matrix.shape[1]\n",
    "params['rnn_dim'] = 512  # 256, 384, 512\n",
    "params['n_layers'] = 1\n",
    "\n",
    "# Training Parameters\n",
    "params['learning_rate'] = 1e-3\n",
    "params['keep_prob_train'] = 0.8\n",
    "params['keep_prob_valid'] = 1.0\n",
    "params['l1_loss'] = 1e-6 # regularize M\n",
    "params['clip'] = 1  # 1e-2\n",
    "params['batch_size'] = 512\n",
    "params['eval_batch_size'] = 16\n",
    "params['n_iterations'] = int(20 * train_data_loader.data_num / params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(save_params_dir):\n",
    "    os.makedirs(save_params_dir)\n",
    "with open(save_params_dir+'model_parameters.json', 'w') as f:\n",
    "    json.dump(params, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record = {}\n",
    "save_record_dir = 'models/%s/' %exp_name\n",
    "record['newest_model_dir'] = 'models/' + exp_name +'/newest/'\n",
    "record['best_model_dir'] = 'models/' + exp_name +'/best/'\n",
    "record['loss_train'] = []\n",
    "record['loss_valid'] = []\n",
    "record['accuracy_valid'] = []\n",
    "record['best_iter'] = 0\n",
    "record['sample_correct'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Embedding 後可以考慮加一層 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7c87654b38>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7ca6b8d518>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input\n",
    "context = tf.placeholder(dtype=tf.int32, shape=(None, None), name='context')\n",
    "context_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='context_len')\n",
    "response = tf.placeholder(dtype=tf.int32, shape=(None, None), name='response')\n",
    "response_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='response_len')\n",
    "target = tf.placeholder(dtype=tf.int32, shape=(None, ), name='target')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "# Embedding\n",
    "init_embedding_W = tf.constant_initializer(embedding_matrix)\n",
    "embeddings_W = tf.get_variable('embeddings_W', shape=[embedding_matrix.shape[0], embedding_matrix.shape[1]], initializer=init_embedding_W)\n",
    "context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "\n",
    "if params['n_layers'] == 1:\n",
    "# shared LSTM encoder\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, \n",
    "                use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(context_len, depth=tf.shape(context)[1]), 1)\n",
    "    encoding_context = tf.squeeze(tf.matmul(mask, c_outputs), 1)   # c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(response_len, depth=tf.shape(response)[1]), 1)\n",
    "    encoding_response =  tf.squeeze(tf.matmul(mask, r_outputs), 1)  # r_states.h\n",
    "else:\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=2.0, use_peepholes=True, state_is_tuple=False, reuse=tf.get_variable_scope().reuse) \n",
    "                for _ in range(params['n_layers'])]\n",
    "    dropcells = [tf.contrib.rnn.DropoutWrapper(cell,input_keep_prob=keep_prob) for cell in cells]\n",
    "    multicell = tf.contrib.rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "    multicell = tf.contrib.rnn.DropoutWrapper(multicell, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(multicell, context_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(context_len, depth=tf.shape(context)[1]), 1)\n",
    "    encoding_context = tf.squeeze(tf.matmul(mask, c_outputs), 1)   # c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(multicell, response_embedded, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(tf.one_hot(response_len, depth=tf.shape(response)[1]), 1)\n",
    "    encoding_response =  tf.squeeze(tf.matmul(mask, r_outputs), 1)  # r_states.h\n",
    "\n",
    "# σ(cMr)\n",
    "M = tf.get_variable('M', shape=[params['rnn_dim'], params['rnn_dim']], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "# \"Predict\" a  response: c * M\n",
    "generated_response = tf.matmul(encoding_context, M)\n",
    "generated_response = tf.expand_dims(generated_response, 2)\n",
    "encoding_response = tf.expand_dims(encoding_response, 2)\n",
    "\n",
    "# Dot product between generated response and actual response\n",
    "logits = tf.matmul(generated_response, encoding_response, True)\n",
    "logits = tf.reshape(logits, [-1])\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "probs = tf.sigmoid(logits)\n",
    "correct_prediction = tf.logical_or( tf.logical_and(tf.equal(target,1), tf.greater_equal(probs,0.5)), tf.logical_and(tf.equal(target,0), tf.less(probs,0.5)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Calculate the binary cross-entropy loss\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.to_float(target)))\n",
    "loss = loss + params['l1_loss'] * tf.reduce_sum(tf.abs(M))\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(params['learning_rate'])\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_norm(grad, params['clip']), var) for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_valid_loss_accuracy(sess):\n",
    "    valid_loss = 0\n",
    "    valid_accuracy = 0\n",
    "    n_iter = int(valid_data_loader.data_num/params['batch_size'])\n",
    "    for iter in range(n_iter):\n",
    "        next_x1, next_x2, next_y, x1_len, x2_len = valid_data_loader.next_batch(batch_size=params['batch_size'], pad_to_length=max_seq_len, return_len=True)\n",
    "        new_accuracy, new_loss = sess.run([accuracy, loss], \n",
    "                                    feed_dict={context: next_x1, response: next_x2, target: next_y, \n",
    "                                    keep_prob: params['keep_prob_train'], context_len: x1_len, response_len:x2_len}) \n",
    "        valid_accuracy += new_accuracy\n",
    "        valid_loss += new_loss\n",
    "    valid_loss /= n_iter\n",
    "    valid_accuracy /= n_iter\n",
    "    print('Valid loss = %.5f, accuracy = %.5f' % (valid_loss, valid_accuracy), flush=True)\n",
    "    record['loss_valid'].append( valid_loss.tolist() )\n",
    "    record['accuracy_valid'].append( valid_accuracy.tolist() )\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations    1:\tloss of batch = 0.69415 / elapsed time 8\n",
      "Valid loss = 0.69373, accuracy = 0.49642\n",
      "Best model save in 0 iteration\n",
      "Iterations    2:\tloss of batch = 0.69429 / elapsed time 12\n",
      "Iterations    3:\tloss of batch = 0.69280 / elapsed time 13\n",
      "Iterations    4:\tloss of batch = 0.69393 / elapsed time 16\n",
      "Iterations    5:\tloss of batch = 0.69310 / elapsed time 16\n",
      "Iterations    6:\tloss of batch = 0.69286 / elapsed time 20\n",
      "Iterations    7:\tloss of batch = 0.69355 / elapsed time 20\n",
      "Iterations    8:\tloss of batch = 0.69375 / elapsed time 24\n",
      "Iterations    9:\tloss of batch = 0.69392 / elapsed time 27\n",
      "Iterations   10:\tloss of batch = 0.69281 / elapsed time 27\n",
      "Iterations   11:\tloss of batch = 0.69387 / elapsed time 30\n",
      "Valid loss = 0.69278, accuracy = 0.53285\n",
      "Iterations   12:\tloss of batch = 0.69245 / elapsed time 33\n",
      "Iterations   13:\tloss of batch = 0.69160 / elapsed time 36\n",
      "Iterations   14:\tloss of batch = 0.69285 / elapsed time 37\n",
      "Iterations   15:\tloss of batch = 0.69345 / elapsed time 39\n",
      "Iterations   16:\tloss of batch = 0.69113 / elapsed time 40\n",
      "Iterations   17:\tloss of batch = 0.69240 / elapsed time 43\n",
      "Iterations   18:\tloss of batch = 0.69227 / elapsed time 45\n",
      "Iterations   19:\tloss of batch = 0.69129 / elapsed time 48\n",
      "Iterations   20:\tloss of batch = 0.69060 / elapsed time 51\n",
      "Iterations   21:\tloss of batch = 0.69189 / elapsed time 54\n",
      "Valid loss = 0.69182, accuracy = 0.56090\n",
      "Iterations   22:\tloss of batch = 0.69178 / elapsed time 57\n",
      "Iterations   23:\tloss of batch = 0.69068 / elapsed time 60\n",
      "Iterations   24:\tloss of batch = 0.69046 / elapsed time 61\n",
      "Iterations   25:\tloss of batch = 0.68860 / elapsed time 64\n",
      "Iterations   26:\tloss of batch = 0.68898 / elapsed time 65\n",
      "Iterations   27:\tloss of batch = 0.69212 / elapsed time 67\n",
      "Iterations   28:\tloss of batch = 0.68809 / elapsed time 70\n",
      "Iterations   29:\tloss of batch = 0.69377 / elapsed time 71\n",
      "Iterations   30:\tloss of batch = 0.68979 / elapsed time 73\n",
      "Iterations   31:\tloss of batch = 0.68947 / elapsed time 74\n",
      "Valid loss = 0.69074, accuracy = 0.56652\n",
      "Iterations   32:\tloss of batch = 0.69337 / elapsed time 78\n",
      "Iterations   33:\tloss of batch = 0.69164 / elapsed time 80\n",
      "Iterations   34:\tloss of batch = 0.68890 / elapsed time 81\n",
      "Iterations   35:\tloss of batch = 0.68886 / elapsed time 84\n",
      "Iterations   36:\tloss of batch = 0.69052 / elapsed time 85\n",
      "Iterations   37:\tloss of batch = 0.68962 / elapsed time 88\n",
      "Iterations   38:\tloss of batch = 0.69038 / elapsed time 89\n",
      "Iterations   39:\tloss of batch = 0.68379 / elapsed time 91\n",
      "Iterations   40:\tloss of batch = 0.68651 / elapsed time 92\n",
      "Iterations   41:\tloss of batch = 0.68792 / elapsed time 95\n",
      "Valid loss = 0.68867, accuracy = 0.57829\n",
      "Iterations   42:\tloss of batch = 0.68670 / elapsed time 99\n",
      "Iterations   43:\tloss of batch = 0.68883 / elapsed time 101\n",
      "Iterations   44:\tloss of batch = 0.68111 / elapsed time 102\n",
      "Iterations   45:\tloss of batch = 0.69097 / elapsed time 104\n",
      "Iterations   46:\tloss of batch = 0.68157 / elapsed time 105\n",
      "Iterations   47:\tloss of batch = 0.68325 / elapsed time 108\n",
      "Iterations   48:\tloss of batch = 0.68857 / elapsed time 111\n",
      "Iterations   49:\tloss of batch = 0.68614 / elapsed time 112\n",
      "Iterations   50:\tloss of batch = 0.68769 / elapsed time 115\n",
      "Iterations   51:\tloss of batch = 0.68750 / elapsed time 116\n",
      "Valid loss = 0.68545, accuracy = 0.59083\n",
      "Iterations   52:\tloss of batch = 0.67660 / elapsed time 119\n",
      "Iterations   53:\tloss of batch = 0.67534 / elapsed time 122\n",
      "Iterations   54:\tloss of batch = 0.68564 / elapsed time 127\n",
      "Iterations   55:\tloss of batch = 0.67838 / elapsed time 128\n",
      "Iterations   56:\tloss of batch = 0.67135 / elapsed time 130\n",
      "Iterations   57:\tloss of batch = 0.68503 / elapsed time 131\n",
      "Iterations   58:\tloss of batch = 0.67363 / elapsed time 134\n",
      "Iterations   59:\tloss of batch = 0.67720 / elapsed time 138\n",
      "Iterations   60:\tloss of batch = 0.67275 / elapsed time 138\n",
      "Iterations   61:\tloss of batch = 0.67158 / elapsed time 140\n",
      "Valid loss = 0.67956, accuracy = 0.59573\n",
      "Iterations   62:\tloss of batch = 0.66786 / elapsed time 144\n",
      "Iterations   63:\tloss of batch = 0.67986 / elapsed time 147\n",
      "Iterations   64:\tloss of batch = 0.67376 / elapsed time 147\n",
      "Iterations   65:\tloss of batch = 0.67186 / elapsed time 151\n",
      "Iterations   66:\tloss of batch = 0.66816 / elapsed time 154\n",
      "Iterations   67:\tloss of batch = 0.66355 / elapsed time 154\n",
      "Iterations   68:\tloss of batch = 0.68750 / elapsed time 157\n",
      "Iterations   69:\tloss of batch = 0.67176 / elapsed time 158\n",
      "Iterations   70:\tloss of batch = 0.68549 / elapsed time 161\n",
      "Iterations   71:\tloss of batch = 0.66319 / elapsed time 164\n",
      "Valid loss = 0.67698, accuracy = 0.59810\n",
      "Iterations   72:\tloss of batch = 0.66237 / elapsed time 167\n",
      "Iterations   73:\tloss of batch = 0.66549 / elapsed time 170\n",
      "Iterations   74:\tloss of batch = 0.67533 / elapsed time 171\n",
      "Iterations   75:\tloss of batch = 0.66484 / elapsed time 173\n",
      "Iterations   76:\tloss of batch = 0.69804 / elapsed time 175\n",
      "Iterations   77:\tloss of batch = 0.66225 / elapsed time 177\n",
      "Iterations   78:\tloss of batch = 0.65943 / elapsed time 180\n",
      "Iterations   79:\tloss of batch = 0.64679 / elapsed time 180\n",
      "Iterations   80:\tloss of batch = 0.68240 / elapsed time 183\n",
      "Iterations   81:\tloss of batch = 0.67732 / elapsed time 184\n",
      "Valid loss = 0.67263, accuracy = 0.60437\n",
      "Iterations   82:\tloss of batch = 0.66521 / elapsed time 187\n",
      "Iterations   83:\tloss of batch = 0.68169 / elapsed time 190\n",
      "Iterations   84:\tloss of batch = 0.67059 / elapsed time 191\n",
      "Iterations   85:\tloss of batch = 0.65241 / elapsed time 194\n",
      "Iterations   86:\tloss of batch = 0.66619 / elapsed time 194\n",
      "Iterations   87:\tloss of batch = 0.68829 / elapsed time 197\n",
      "Iterations   88:\tloss of batch = 0.66540 / elapsed time 198\n",
      "Iterations   89:\tloss of batch = 0.68253 / elapsed time 201\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "start_time = time.time()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Restore model\n",
    "    # saver.restore(sess, record['best_model_dir']+'model.ckpt')\n",
    "    # print('Retrain model: %s' %record['best_model_dir'], flush=True)\n",
    "    best_valid_loss = 0\n",
    "    for it in range(params['n_iterations']):\n",
    "        print('Iterations %4d:\\t' %(it+1) , end='', flush=True)\n",
    "        # Train next batch\n",
    "        next_x1, next_x2, next_y, x1_len, x2_len = train_data_loader.next_batch(batch_size=params['batch_size'], pad_to_length=max_seq_len, return_len=True)\n",
    "        batch_loss, _ = sess.run([loss, train_step], \n",
    "                            feed_dict={context: next_x1, response: next_x2, target: next_y, \n",
    "                            keep_prob: params['keep_prob_train'], context_len: x1_len, response_len:x2_len}) \n",
    "        print('loss of batch = %.5f / elapsed time %.f' % (batch_loss, time.time() - start_time), flush=True)\n",
    "        record['loss_train'].append( batch_loss.tolist() )\n",
    "        if it % 10 == 0:\n",
    "            # Save the model if has smaller loss\n",
    "            current_valid_loss = get_valid_loss_accuracy(sess)\n",
    "            if current_valid_loss >= best_valid_loss:\n",
    "                best_valid_loss = current_valid_loss\n",
    "                if not os.path.exists(record['best_model_dir']):\n",
    "                    os.makedirs(record['best_model_dir'])\n",
    "                save_path = saver.save(sess, record['best_model_dir']+'model.ckpt')\n",
    "                record['best_iter'] = it\n",
    "                print('Best model save in %d iteration' %it, flush=True)\n",
    "        if not os.path.exists(record['newest_model_dir']):\n",
    "            os.makedirs(record['newest_model_dir'])\n",
    "        save_path = saver.save(sess, record['newest_model_dir']+'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump record file as .json\n",
    "if not os.path.exists(save_record_dir):\n",
    "    os.makedirs(save_record_dir)\n",
    "with open(save_record_dir+'%d.json' %params['n_iterations'], 'w') as f:\n",
    "    json.dump(record, f, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
