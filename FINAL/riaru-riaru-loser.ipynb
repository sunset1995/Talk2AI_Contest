{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dual LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/sunset/Talk2AI_Contest/FINAL/dict/dict.txt.big ...\n",
      "Loading model from cache /var/folders/43/l4vp_w_x4wb11mmy_bb1jrkc0000gn/T/jieba.u3da715484192d889a29ab9e17f6253a0.cache\n",
      "Loading model cost 2.295 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('./dict/dict.txt.big')\n",
    "jieba.load_userdict('./dict/edu_dict.txt')\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input_fname = sys.argv[1]\n",
    "input_fname = './train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_datas = pd.read_csv(input_fname)\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x1 = [[[word for word in jieba.cut(s) if word.strip()] for s in q] for q in test_x1]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "test_x2 = [[[word for word in jieba.cut(s) if word.strip()] for s in rs] for rs in test_x2]\n",
    "\n",
    "word2id = {}\n",
    "with open('./dict/id2word', 'r') as f:\n",
    "    for i, word in enumerate(f.readline().split(' ')):\n",
    "        word2id[word] = i\n",
    "\n",
    "def word_lst_2_id_lst(lst, pad_to_len=-1):\n",
    "    pad_word_id = word2id['<pad>']\n",
    "    pad_len = max(len(lst), 0)\n",
    "    id_list = [word2id[lst[i]] if i<len(lst) and lst[i] in word2id else pad_word_id for i in range(pad_len)]\n",
    "    pad_len = pad_to_len - len(id_list)\n",
    "    if pad_len > 0:\n",
    "        id_list.extend([pad_word_id] * pad_len)\n",
    "    return id_list\n",
    "\n",
    "test_id1 = [[word for s in q for word in s] for q in test_x1]\n",
    "test_id1 = np.array([word_lst_2_id_lst(s) for s in test_id1])\n",
    "test_id2 = [[[word for word in r] for r in rs] for rs in test_x2]\n",
    "test_id2 = np.array([[word_lst_2_id_lst(r) for r in rs] for rs in test_id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([109, 7, 15, 937, 17039, 1, 8629])], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id1[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  109,     7,    15,   937, 17039,     1,  8629],\n",
       "       [  109,     7,    15,   937, 17039,     1,  8629],\n",
       "       [  109,     7,    15,   937, 17039,     1,  8629],\n",
       "       [  109,     7,    15,   937, 17039,     1,  8629],\n",
       "       [  109,     7,    15,   937, 17039,     1,  8629],\n",
       "       [  109,     7,    15,   937, 17039,     1,  8629]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat([test_id1[0]], 6, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['embedding_shape'] = (65865, 200)\n",
    "params['rnn_dim'] = 256\n",
    "params['n_layers'] = 2\n",
    "params['batch_size'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Input\n",
    "context = tf.placeholder(dtype=tf.int32, shape=(None, None), name='context')\n",
    "context_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='context_len')\n",
    "response = tf.placeholder(dtype=tf.int32, shape=(None, None), name='response')\n",
    "response_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='response_len')\n",
    "target = tf.placeholder(dtype=tf.int32, shape=(None, ), name='target')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "# Embedding\n",
    "embeddings_W = tf.get_variable('embeddings_W', shape=params['embedding_shape'])\n",
    "context_embedded = tf.nn.embedding_lookup(embeddings_W, context, name=\"embed_context\")\n",
    "response_embedded = tf.nn.embedding_lookup(embeddings_W, response, name=\"embed_response\")\n",
    "\n",
    "if params['n_layers'] == 1:\n",
    "# shared LSTM encoder\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=1.0,\n",
    "                use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(cell, context_embedded, sequence_length=context_len, dtype=tf.float32)\n",
    "    encoding_context = c_states.h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(cell, response_embedded, sequence_length=response_len, dtype=tf.float32)\n",
    "    encoding_response = r_states.h\n",
    "    #mask = tf.expand_dims(tf.one_hot(response_len, depth=tf.shape(response)[1]), 1)\n",
    "    #encoding_response =  tf.squeeze(tf.matmul(mask, r_outputs), 1)  # r_states.h\n",
    "else:\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(num_units=params['rnn_dim'], forget_bias=1.0, use_peepholes=True, state_is_tuple=True, reuse=tf.get_variable_scope().reuse) \n",
    "                for _ in range(params['n_layers'])]\n",
    "    dropcells = [tf.contrib.rnn.DropoutWrapper(cell,input_keep_prob=keep_prob) for cell in cells]\n",
    "    multicell = tf.contrib.rnn.MultiRNNCell(dropcells, state_is_tuple=True)\n",
    "    multicell = tf.contrib.rnn.DropoutWrapper(multicell, output_keep_prob=keep_prob)\n",
    "    c_outputs, c_states = tf.nn.dynamic_rnn(multicell, context_embedded, sequence_length=context_len, dtype=tf.float32)\n",
    "    encoding_context = c_states[-1].h\n",
    "    r_outputs, r_states = tf.nn.dynamic_rnn(multicell, response_embedded, sequence_length=response_len, dtype=tf.float32)\n",
    "    encoding_response = r_states[-1].h\n",
    "\n",
    "# Ïƒ(cMr)\n",
    "M = tf.get_variable('M', shape=[params['rnn_dim'], params['rnn_dim']], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "# \"Predict\" a  response: c * M\n",
    "generated_response = tf.matmul(encoding_context, M)\n",
    "generated_response = tf.expand_dims(generated_response, 2)\n",
    "encoding_response = tf.expand_dims(encoding_response, 2)\n",
    "\n",
    "# Dot product between generated response and actual response\n",
    "logits = tf.matmul(generated_response, encoding_response, True)\n",
    "logits = tf.reshape(logits, [-1])\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities (for prediction, not for loss)\n",
    "probs = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'models/dual_lstm_16/newest/model.ckpt',\n",
    "    'models/dual_lstm_16/best/model.ckpt',\n",
    "    'models/dual_lstm_17/newest/model.ckpt',\n",
    "    'models/dual_lstm_17/best/model.ckpt',\n",
    "]\n",
    "\n",
    "def unitvec(vec):\n",
    "    l = np.linalg.norm(vec)\n",
    "    return vec / l if l != 0 else vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_13/newest/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_13/best/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_15/newest/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_15/best/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_18/newest/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_18/best/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_22/newest/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_22/best/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_24/newest/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/dual_lstm_24/best/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "for mn in model_names:\n",
    "    o_fname = '_'.join(mn.split('/')[1:3])\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    saver.restore(sess, mn)\n",
    "    \n",
    "    # sigmoid(cMr)\n",
    "    all_score = []\n",
    "    for i in range(0, len(test_id1), params['batch_size']):\n",
    "        q = np.repeat(test_id1[i:i+params['batch_size']], 6)\n",
    "        r = test_id2[i:i+params['batch_size']].reshape(-1)\n",
    "        q_l = [len(s) for s in q]\n",
    "        r_l = [len(s) for s in r]\n",
    "        max_l = max(q_l + r_l)\n",
    "        q = np.array([[s[j] if j<len(s) else 0 for j in range(max_l)] for s in q])\n",
    "        r = np.array([[s[j] if j<len(s) else 0 for j in range(max_l)] for s in r])\n",
    "        now_score = sess.run(probs, {\n",
    "                context: q,\n",
    "                response: r,\n",
    "                keep_prob: 1.0,\n",
    "                context_len: q_l,\n",
    "                response_len: r_l})\n",
    "        all_score.extend(now_score)\n",
    "    all_score = np.array(all_score).reshape(-1, 6)\n",
    "    with open('__'+o_fname+'_sigmoid_cMr.txt', 'w') as fo:\n",
    "        fo.write(','.join([str(a) for a in np.argmax(all_score, axis=1)]))\n",
    "    \n",
    "    # cossim(c, Mr)\n",
    "    qq = []\n",
    "    rr = []\n",
    "    for i in range(0, len(test_id1), params['batch_size']):\n",
    "        q = test_id1[i:i+params['batch_size']]\n",
    "        r = test_id2[i:i+params['batch_size']].reshape(-1)\n",
    "        q_l = [len(s) for s in q]\n",
    "        r_l = [len(s) for s in r]\n",
    "        max_l = max(q_l + r_l)\n",
    "        q = np.array([[s[j] if j<len(s) else 0 for j in range(max_l)] for s in q])\n",
    "        r = np.array([[s[j] if j<len(s) else 0 for j in range(max_l)] for s in r])\n",
    "        q_state = sess.run(generated_response, {\n",
    "            context: q,\n",
    "            keep_prob: 1.0,\n",
    "            context_len: q_l,\n",
    "        })\n",
    "        r_state = sess.run(encoding_response, {\n",
    "            response: r,\n",
    "            keep_prob: 1.0,\n",
    "            response_len: r_l,\n",
    "        })\n",
    "        qq.extend(q_state.reshape(-1, params['rnn_dim']))\n",
    "        rr.extend(r_state.reshape(-1, 6, params['rnn_dim']))\n",
    "    qq = np.array(qq)\n",
    "    rr = np.array(rr)\n",
    "\n",
    "    state_cossim = []\n",
    "    for q, rs in zip(qq, rr):\n",
    "        for r in rs:\n",
    "            state_cossim.append(np.dot(unitvec(q), unitvec(r)))\n",
    "    state_cossim = np.array(state_cossim).reshape(-1, 6)\n",
    "    with open('__'+o_fname+'_cossim_c_Mr.txt', 'w') as fo:\n",
    "        fo.write(','.join([str(a) for a in np.argmax(state_cossim, axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
