{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# naive word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/sunset/Talk2AI_Contest/FINAL/dict/dict.txt.big ...\n",
      "Loading model from cache /var/folders/43/l4vp_w_x4wb11mmy_bb1jrkc0000gn/T/jieba.u3da715484192d889a29ab9e17f6253a0.cache\n",
      "Loading model cost 1.802 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Import & Init jieba\n",
    "import jieba\n",
    "jieba.set_dictionary('./dict/dict.txt.big')\n",
    "jieba.load_userdict('./dict/edu_dict.txt')\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input_fname = sys.argv[1]\n",
    "input_fname = './train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_datas = pd.read_csv(input_fname)\n",
    "test_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.dialogue.values]\n",
    "test_x1 = [[[word for word in jieba.cut(s) if word.strip()] for s in q] for q in test_x1]\n",
    "test_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in test_datas.options.values]\n",
    "test_x2 = [[[word for word in jieba.cut(s) if word.strip()] for s in rs] for rs in test_x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'models/word2vec/dual-lstm-12-best',\n",
    "    'models/word2vec/dual-lstm-12-newest',\n",
    "    'models/word2vec/dual-lstm-13-best',\n",
    "    'models/word2vec/dual-lstm-13-newest',\n",
    "    'models/word2vec/dual-lstm-14-best',\n",
    "    'models/word2vec/dual-lstm-14-newest',\n",
    "    'models/word2vec/dual-lstm-15-best',\n",
    "    'models/word2vec/dual-lstm-15-newest',\n",
    "    'models/word2vec/dual-lstm-16-best',\n",
    "    'models/word2vec/dual-lstm-16-newest',\n",
    "    'models/word2vec/dual-lstm-17-best',\n",
    "    'models/word2vec/dual-lstm-17-newest',\n",
    "    'models/word2vec/dual-lstm-18-best',\n",
    "    'models/word2vec/dual-lstm-18-newest',\n",
    "    'models/word2vec/dual-lstm-22-best',\n",
    "    'models/word2vec/dual-lstm-22-newest',\n",
    "    'models/word2vec/dual-lstm-24-best',\n",
    "    'models/word2vec/dual-lstm-24-newest',\n",
    "    'models/word2vec/smn-1-best',\n",
    "    'models/word2vec/smn-1-newest',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Naive - centroid\n",
    "def unitvec(vec):\n",
    "    l = np.linalg.norm(vec)\n",
    "    return vec / l if l != 0 else vec\n",
    "\n",
    "def centroid(sentence):\n",
    "    vecs = [word_vectors.word_vec(word) for word in sentence if word in word_vectors.vocab]\n",
    "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(word_vectors.vector_size)\n",
    "\n",
    "def centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [word for s in a for word in s]\n",
    "        b_sentences = [[word for word in s] for s in b]\n",
    "\n",
    "        a_center = centroid(a_sentence)\n",
    "        b_centers = [centroid(s) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)\n",
    "\n",
    "def attack_naive_centroid(x1, x2):\n",
    "    my_cos_ans = centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive - sentence decay centroid\n",
    "def dis_centroid(ss, beta=0.77):\n",
    "    for s in ss:\n",
    "        assert(type(s) == list)\n",
    "    vecs = [[word_vectors.word_vec(word) for word in s if word in word_vectors.vocab] for s in ss]\n",
    "    vecs = [s for s in vecs if len(s) > 0]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    cens = list(reversed([np.mean(vs, axis=0) for vs in vecs]))\n",
    "    for cen in cens:\n",
    "        assert(np.sum(np.isnan(cen)) == 0)\n",
    "    w_sum = sum(beta**i for i in range(len(cens)))\n",
    "    return np.sum([cens[i] * (beta ** i / w_sum) for i in range(len(cens))], axis=0)\n",
    "\n",
    "def dis_centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [[word for word in s] for s in a]\n",
    "        b_sentences = [[word for word in s] for s in b]\n",
    "\n",
    "        a_center = dis_centroid(a_sentence)\n",
    "        b_centers = [dis_centroid([s]) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)\n",
    "\n",
    "def attack_naive_dis_centroid(x1, x2):\n",
    "    my_cos_ans = dis_centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive - word decay centroid\n",
    "def w_centroid(ss, beta=0.77):\n",
    "    for s in ss:\n",
    "        assert(type(s) == list)\n",
    "    vecs = [[word_vectors.word_vec(word) for word in s if word in word_vectors.vocab] for s in ss]\n",
    "    vecs = list(reversed([s for s in vecs if len(s) > 0]))\n",
    "    w_cen = np.zeros(word_vectors.vector_size)\n",
    "    if len(vecs) == 0:\n",
    "        return w_cen\n",
    "    w = np.array([beta**i for i in range(len(vecs)) for _ in range(len(vecs[i]))]).reshape(-1, 1)\n",
    "    cen = np.array([vec for s in vecs for vec in s])\n",
    "    return np.sum(w * cen, axis=0) / np.sum(w)\n",
    "\n",
    "def w_centroid_score(x1, x2):\n",
    "    cos_score = []\n",
    "    for a, b in zip(x1, x2):\n",
    "        a_sentence = [[word for word in s] for s in a]\n",
    "        b_sentences = [[word for word in s] for s in b]\n",
    "\n",
    "        a_center = w_centroid(a_sentence)\n",
    "        b_centers = [w_centroid([s]) for s in b_sentences]\n",
    "\n",
    "        cos_score.append([np.dot(unitvec(a_center), unitvec(bc)) for bc in b_centers])\n",
    "    return np.array(cos_score).reshape(-1, 6)\n",
    "\n",
    "def attack_naive_w_centroid(x1, x2):\n",
    "    my_cos_ans = w_centroid_score(x1, x2)\n",
    "    return np.argmax(my_cos_ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.667449951171875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tiem = time.time()\n",
    "for mn in model_names:\n",
    "    word_vectors = KeyedVectors.load(mn)\n",
    "    now_ans = attack_naive_w_centroid(test_x1, test_x2)\n",
    "    with open('__naive_'+mn.split('/')[2]+'.txt', 'w') as f:\n",
    "        f.write(','.join([str(a) for a in now_ans]))\n",
    "time.time() - start_tiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
