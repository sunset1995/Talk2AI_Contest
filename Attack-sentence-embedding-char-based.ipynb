{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import util\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Self define module\n",
    "from mini_batch_helper import MiniBatchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading corpus and forming dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fnames = [\n",
    "    'datas/training_data/no_TC_下課花路米.txt',\n",
    "    'datas/training_data/no_TC_誰來晚餐.txt',\n",
    "    'datas/training_data/no_TC_公視藝文大道.txt',\n",
    "    'datas/training_data/no_TC_成語賽恩思.txt',\n",
    "    'datas/training_data/no_TC_我的這一班.txt',\n",
    "    'datas/training_data/no_TC_流言追追追.txt',\n",
    "    'datas/training_data/no_TC_人生劇展.txt',\n",
    "    'datas/training_data/no_TC_聽聽看.txt',\n",
    "]\n",
    "sample_rate_on_training_datas = 0.3\n",
    "valid_cp_num_of_each = 1\n",
    "\n",
    "def word_tok_lst_2_ch_lst(s):\n",
    "    return [ch.strip() for word in s for ch in word if ch.strip() != '']\n",
    "\n",
    "def corpus_extract_sentence(now_corpus):\n",
    "    return [[ch for ch in word_tok_lst_2_ch_lst(s)] for line in now_corpus for s in line.strip().split('\\t')]\n",
    "\n",
    "corpus = []\n",
    "corpus_valid = []\n",
    "for fname in corpus_fnames:\n",
    "    with open(fname, 'r') as f:\n",
    "        now_corpus = np.array([line for line in f])\n",
    "        now_corpus_valid = now_corpus[:valid_cp_num_of_each]\n",
    "        now_corpus = now_corpus[valid_cp_num_of_each:]\n",
    "        if sample_rate_on_training_datas < 1:\n",
    "            sample_num = int(max(len(now_corpus)*sample_rate_on_training_datas, 5))\n",
    "            rnd_idx = np.arange(len(now_corpus))\n",
    "            np.random.shuffle(rnd_idx)\n",
    "            now_corpus = now_corpus[rnd_idx[:sample_num]]\n",
    "        \n",
    "        corpus.append(corpus_extract_sentence(now_corpus))\n",
    "        corpus_valid.append(corpus_extract_sentence(now_corpus_valid))\n",
    "\n",
    "with open('datas/dict/id2ch.txt') as f:\n",
    "    id2ch = f.read().strip().split()\n",
    "ch2id = dict([(ch, i) for i, ch in enumerate(id2ch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_id = [[[ch2id[ch] for ch in s if ch in ch2id] for s in cp] for cp in corpus]\n",
    "corpus_valid_id = [[[ch2id[ch] for ch in s if ch in ch2id] for s in cp] for cp in corpus_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    706461.000000\n",
       "mean          7.663840\n",
       "std           3.539625\n",
       "min           1.000000\n",
       "25%           5.000000\n",
       "50%           8.000000\n",
       "75%          10.000000\n",
       "max          25.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic sentence length\n",
    "Series([len(s) for cp in corpus for s in cp]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pad_word_id: 1894\n",
      "         max_seq_len: 25\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(ch2id)\n",
    "emb_size = 200\n",
    "pad_word_id = ch2id['<eos>']\n",
    "max_seq_len = np.max([len(s) for cp in corpus_id for s in cp])\n",
    "\n",
    "print('%20s: %d' % ('pad_word_id', pad_word_id))\n",
    "print('%20s: %d' % ('max_seq_len', max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datas num: 1412906\n",
      "valid datas num: 16432\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = MiniBatchCorpus(corpus_id)\n",
    "valid_data_loader = MiniBatchCorpus(corpus_valid_id)\n",
    "print('train datas num:', train_data_loader.data_num)\n",
    "print('valid datas num:', valid_data_loader.data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "x1 = tf.placeholder(tf.int32, [None, None])\n",
    "x2 = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.float64, [None])\n",
    "lr = tf.placeholder(tf.float64)\n",
    "\n",
    "# Embedding layer\n",
    "embeddings_W = tf.Variable(tf.truncated_normal([voc_size, emb_size], stddev=0.01, dtype=tf.float64))\n",
    "\n",
    "def sentence_embedding(xs):\n",
    "    xs_mask = 1 - tf.to_double(tf.equal(xs, pad_word_id))\n",
    "    xs_len = tf.reduce_sum(xs_mask, axis=1)\n",
    "    xs_embedded = tf.gather(embeddings_W, xs)\n",
    "    xs_center = tf.reduce_sum(xs_embedded, axis=1) / tf.reshape(tf.to_double(xs_len)+1e-6, [-1, 1])\n",
    "    return xs_center\n",
    "\n",
    "x1_center = sentence_embedding(x1)\n",
    "x2_center = sentence_embedding(x2)\n",
    "W = tf.Variable(tf.truncated_normal([emb_size, emb_size], stddev=0.01, dtype=tf.float64))\n",
    "tf_score = tf.reduce_sum((x2_center * (x1_center @ W)), axis=1)\n",
    "\n",
    "tf_prob = tf.sigmoid(tf_score)\n",
    "tf_correct = tf.reduce_sum(tf.cast(\n",
    "    (tf.equal(y, 1) & tf.greater_equal(tf_prob, 0.5)) | (tf.equal(y, 0) & tf.less(tf_prob, 0.5)),\n",
    "    tf.int32\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = tf.nn.l2_loss(W) * 1e-6\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=tf_score))\n",
    "reg_cost = cost + reg\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "gvs = optimizer.compute_gradients(reg_cost)\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 2), var) for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_valid_loss():\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    valid_batch = 1024\n",
    "    batch_num = valid_data_loader.data_num // valid_batch\n",
    "    for i in range(batch_num):\n",
    "        b_x1, b_x2, b_y = valid_data_loader.next_batch(valid_batch, max_seq_len, pad_word_id)\n",
    "        now_loss, now_correct = sess.run([cost, tf_correct], {x1: b_x1, x2: b_x2, y: b_y})\n",
    "        valid_loss += now_loss / (batch_num * valid_batch)\n",
    "        valid_acc += now_correct / (batch_num * valid_batch)\n",
    "    return valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0\n",
      "train batch loss   0.002655 / valid loss   0.000652 / valid acc   0.581665 / elapsed time 20\n",
      "model saved (best)\n",
      "train batch loss   0.002605 / valid loss   0.000645 / valid acc   0.596802 / elapsed time 40\n",
      "model saved (best)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 256\n",
    "epoch_num = 10\n",
    "log_interval = 500\n",
    "save_interval = 10000\n",
    "\n",
    "last_epoch = None\n",
    "train_batch_loss = 0\n",
    "start_time = time.time()\n",
    "best_validation = None\n",
    "for i_batch in range(epoch_num * train_data_loader.data_num // batch_size):\n",
    "    epoch = i_batch // (train_data_loader.data_num // batch_size)\n",
    "    if last_epoch is None or last_epoch != epoch:\n",
    "        last_epoch = epoch\n",
    "        print('Start epoch %d' % (epoch))\n",
    "    \n",
    "    b_x1, b_x2, b_y = train_data_loader.next_batch(batch_size, max_seq_len, pad_word_id)\n",
    "    _, now_loss = sess.run([train_step, cost], {x1: b_x1, x2: b_x2, y: b_y, lr: learning_rate})\n",
    "    train_batch_loss += now_loss / (log_interval * batch_size)\n",
    "    if (i_batch+1) % log_interval == 0:\n",
    "        valid_loss, valid_acc = eval_valid_loss()\n",
    "        print('train batch loss %10f / valid loss %10f / valid acc %10f / elapsed time %.f' % (\n",
    "            train_batch_loss, valid_loss, valid_acc, time.time()-start_time), flush=True)\n",
    "        train_batch_loss = 0\n",
    "        if best_validation is None or valid_loss < best_validation:\n",
    "            best_validation = valid_loss\n",
    "            print('model saved (best)', flush=True)\n",
    "            saver.save(sess, 'models/Attack-sentence-embedding/best/model')\n",
    "        else:\n",
    "            learning_rate /= 1.01\n",
    "            print('Decay learing rate -> %10f' % (learning_rate))\n",
    "    if save_interval is not None and (i_batch+1) % save_interval == 0:\n",
    "        saver.save(sess, 'models/Attack-sentence-embedding/s_emb', global_step=i_batch+1)\n",
    "        print('model saved (latest)', flush=True)\n",
    "\n",
    "saver.save(sess, 'models/Attack-sentence-embedding/s_emb_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------------------\n",
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, 'models/Attack-sentence-embedding/s_emb_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('datas/sample_test_data.txt')\n",
    "\n",
    "sample_x1 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.dialogue.values]\n",
    "sample_x2 = [[s for s in re.sub('[A-Z]:', '\\t', _).split('\\t') if len(s.strip())] for _ in sample.options.values]\n",
    "sample_y = sample.answer.values\n",
    "\n",
    "def str_2_idlst(s):\n",
    "    idlst = [ch2id[ch] for ch in s if ch in ch2id]\n",
    "    return idlst\n",
    "\n",
    "sample_id1 = []\n",
    "for q in sample_x1:\n",
    "    sample_id1.append([])\n",
    "    for qq in q:\n",
    "        sample_id1[-1].extend(str_2_idlst(qq))\n",
    "sample_id1 = np.array([s + [pad_word_id] * (max_seq_len - len(s)) for s in sample_id1])\n",
    "\n",
    "sample_id2 = []\n",
    "for rs in sample_x2:\n",
    "    sample_id2.append([])\n",
    "    for r in rs:\n",
    "        sample_id2[-1].append(str_2_idlst(r))\n",
    "sample_id2 = np.array([[s + [pad_word_id] * (max_seq_len - len(s)) for s in rs] for rs in sample_id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sess.run(tf_correct, {x1: np.repeat(sample_id1, 6, axis=0), x2: sample_id2.reshape(-1, max_seq_len), y: sample_y})\n",
    "scores = scores.reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.argmax(scores, axis=1) == sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
